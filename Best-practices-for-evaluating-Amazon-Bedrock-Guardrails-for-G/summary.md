# AWS re:Inforce 2025 - Best practices for evaluating Amazon Bedrock Guardrails for Gen AI workloads

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=4E1hX-fBEp0)

## Video Information
- **Author:** Unknown
- **Duration:** 19.9 minutes
- **Word Count:** 3,659 words
- **Publish Date:** None

## Summary
The session, delivered by Corey Smith, a senior security consultant with AWS's Gen AI Innovation Center, focuses on using Amazon Bedrock Guardrails to secure generative AI workloads in production environments. The presentation addresses a common challenge where organizations have Gen AI initiatives but struggle to move them to production due to security uncertainties.

The talk explores Amazon Bedrock's capabilities as a fully managed service for generative AI workloads, introducing Bedrock Guardrails as a native security control for managing prompt-response processes. It details various configurable security policies within guardrails, including denied topics, content filters, word filters, and sensitive information filters, which can be applied to both inputs and outputs.

The presentation concludes with best practices for implementing Bedrock Guardrails, emphasizing the importance of defining use cases, creating comprehensive guardrail strategies early in development, and implementing proper testing frameworks. The speakers stress the necessity of continuous monitoring, evaluation, and iteration of guardrail configurations to ensure optimal security and performance.

## Key Points
- Most organizations have Gen AI initiatives, but fewer have moved to production due to security concerns
- Bedrock Guardrails can evaluate both inputs and outputs, with configurable actions for policy violations
- Guardrails should be implemented as a "day zero" activity, not added at the last minute before production
- Denied topics are recommended as the starting point for implementing guardrails
- Content filters can be tuned for sensitivity levels (low, medium, high)
- Custom word lists and phrase lists can be created for specific use cases
- PII detection supports US, UK, and Canada entities with custom rejection capabilities
- Guardrail enforcement can be managed through IAM policies
- Continuous monitoring through CloudWatch metrics is essential
- Testing automation is crucial for validating guardrail effectiveness

## Technical Details
- Bedrock supports multiple model types: text-to-text, text-to-speech, image-to-image
- Guardrail API allows standalone usage outside of Bedrock models
- Policy violation responses include:
  - Preconfigured block messages
  - Content masking/redaction
  - Unmodified responses for approved content
- CloudWatch metrics track guardrail interventions
- Supports retrieval augmented generation through Bedrock knowledge bases
- Custom reject filters for organization-specific sensitive information
- IAM conditional statements for enforcing specific guardrail versions
- Testing framework requirements for handling hundreds of test prompts
- Integration with multi-agent collaboration and workflows
- Continuous pre-training and fine-tuning capabilities for model customization

## Full Transcript

So I know we're here early. I hope everybody's enjoying the conference. Day two, welcome. Uh, we're gonna kick it off here. Uh, I always appreciate a little bit of fan interaction this early in the morning, so I'm gonna start off with a few questions, some show of hands. Um, how many of you here have ongoing Gen AI initiatives within your business? I see some hands? OK, so about that most everybody. Awesome. Now the crucial question how many of those initiatives have resulted in production deployments? OK, so not as many as the first group, so probably about 5, and that's really what we want to address today. So my name's Corey Smith. I'm a senior security consultant with the Gen AI Innovation Center at AWS, and I have the privilege to work with customers on a daily basis to build, secure, scale, and deploy Gen AI workloads to production. And what I most often hear from customers as a blocker on the path to production. Is uncertainty around security. They just feel like there's a lot of unknowns. So what we wanna do today is highlight the use of bedrock bedrock guard rails to provide a level of certainty and assurance around moving workloads to production. So that's gonna consist of just a really quick overview of bedrock, bedrock guard rails, and then we're gonna talk about how you can configure, deploy, and most crucially test guard rails before moving to production. Um, so before we get going, uh, I'll move to the next slide and cover bedrock, uh, Amazon Bedrock. So if you're not familiar with Amazon Bedrock, it's a fully managed service which allows you to build and deploy generative AI workloads at scale without having to manage any underlying infrastructure. You have access to several models from leading model providers which enables you to choose and test the model that fits best for your use case depending on text to text, text to speech, um, image to image, whatever, whatever the use case is, uh, there's a number of models for you to test and select. Once you've selected a model, you can then customize that model further through continuous pre-training and fine tuning approaches, making it work more specific to your use case, uh, as needed. With retrieval augmented generation through Bedrock knowledge bases, you can incorporate your corpus of knowledge sources across your enterprise to provide context to where results and uh further enhanced prompts to uh provide better accuracy for your users. If you're familiar with AI agents and multi-agent collaboration that allows you to build and deploy and execute multi-step workflows with reasoning built in so intelligent decision making on which path to take, uh, is, is really important there. And of course it's a security conference. All of this is built on a foundation of security, privacy and safety. So for this talk before we introduce Bedrock guardrails, what's important to understand about Amazon Bedrock is that it allows us to prompt a model and receive a response. That's the crucial element for this presentation. Looking at bedrock guard rails, a native security control within the service, this is how we secure that prompt response process. So bedrock guardrails consists of a number of con configurable security policies, and the guardrail simply references the collection of policies that you've configured. And bedrock guard rails can be applied to models within Amazon Bedrock, uh, agents, bedrock workflows, and most interestingly with the release of the Bedrock guardrail apply guardrail API as a standalone API you can now use Bedrock guardrails to secure the prompt response process for models not hosted in Bedrock so you can invoke guardrail evaluations, um, independently without having to use a bedrock model. Um, Most importantly is the use of um. The individual specific policies within guard rails, um, I'll cover the brief flow here so guard rails can be configured to evaluate inputs and outputs or both, and depending on which element violates the security policy, um, depends on the action that's taken and of course this is all preconfigured by you. So if it's the user input that violates the policy, say a user asks about a denied topic, uh, you've got a banking app and they're asking about health care, you wanna return a preconfigured block message which says you're violating the acceptable use of this, uh, chatbot and maybe recommend some pre pre-approved queries that can get them started on their journey to effectively using the chat experience. If it's the foundation model output that violates the security control, you've got one of two options. The first is to return that preconfigured block message saying they're asking about something they shouldn't or they're using language they shouldn't. Maybe it's profanity, maybe it's cursing. There's a number of things that we'll get into in the next few slides that you can configure, but it if it violates, you can return the preconfigured block message. The other option if it's sensitive data discovery or detection sensitive data detection or they're using words or phrases that you've specifically blocked in the chat experience, you can simply mask that provide the answer, uh, by sim but redacting the known elements of PII or any form of regulated data, uh, and that's all pre preconfigured and tuned through the model, uh, through the guard rail for you. If there's no guard rail interventions and what the user asks is approved and the response is approved, then the response simply flows to the user without any form of modification, no redaction, no masking, and no block message so it would work as expected. One of the first places that we tell people to start is with denied topics, so I mentioned the guard rail is simply the collection of security policies. The first security policy we wanna look at is the denied topic, and we often say when when defining your strategy for bedrock guardrails, the first thing you need to do is define your use case because once you've defined your use case you can really quickly define what people shouldn't be asking about. So I gave the example of a banking application in this. We simply don't want it to give any form of investment advice, so we create one denied topic around investment advice using natural language, we specify what the user shouldn't be able to discuss or prompt from the model. And then we provide a few canned examples. uh, if you're familiar with few shot prompting and giving some examples in order to make the model response more performant, very similar concept here by providing a few example phrases, the guardrail actually learns and determines what it should and shouldn't respond to, so you don't have to provide an exhaustive list. You just need a few high quality examples of what shouldn't be allowed from the chatbot, and you can configure a number of denied denied topic policies within one guard rail. Moving on to content filters, so an exhaustive list here hate, insult, sexual violence, prompt attacks, misconduct. There's a number of individual components within the content filter policy that you can enable you can enable one, all, none, and you can also individually tune the sensitivity to low, medium, and high, and what that represents is a confidence rate of the guard rail evaluation. So when the input comes in, if you've got the prompt attack, uh, guard rail or policy configured and there's a high confidence, but you don't have the high set, then you won't block it. So when we get into the testing framework which which Rick will be talking about here in a second, is how we want to test and evaluate guardrails to get the optimal configurations for the sensitivities of these content filters. Uh, looking at word filters, so after we've identified our use case we might want to restrict certain things that people can say. Uh, number one is a managed profanity list by Amazon, so it's a world recognized list of profane words. Um, you also have the ability to create custom word lists and phrase lists so if you don't want customers asking about competitors in your chatbot, you can create. Words and phrases specific to the competitor that you don't want them to ask and similar to what we talked about before, if they do that you can either mask those words or you can return a preconfigured block message saying that they shouldn't be using that language or uh again recommend pre-approved queries to get them started in the chat experience. Sensitive information filters everybody here I assume is familiar with PII and the various forms that PII can take both in healthcare, banking, finance, um. And with this with the sensitive information filters you can redact uh PII entities that are recognized in US, UK, and Canada. It's continuously evolving, uh, but also of note is you can create custom reject filters for detections and indicators specific to you. So if your use case has something that's not on the list of preconfigured PII entities, you can configure that through Reject and uh receive the same response. So before I hand off to Vic to talk about how we can test and iterate, I just wanna cover the best practices for bedrock guard rails. So now that we know what they are, we know what you can do with them, what's the approach that you should take take starting today to secure your production workloads tomorrow? Number one, I mentioned already, define the use case. So if you are doing a banking, uh, chatbot and you wanna expose that to your customers on the website, you need to understand what the inputs and outputs are, how those inputs will be gathered and how the outputs will be served, and from there you can start to create your denied topics, your word and phrase list that you want to restrict and turn on the specific PII detections that you need for your use case. Once we understand the use case and we understand how the input will be gathered and the model responses will be served, uh, we understand that prompt response process. We want to create our guard rail strategy. The guard rail strategy I like to say, is a day zero activity. When you are designing a solution, you should be thinking about your guard rails. Um, Vic will use an analogy to laugh, which I think serves us really well. You don't want to turn on a guard rail when you move to production. You should understand how the guardrail's been built, how it's been tested, how it's been validated, and you want to do this throughout the SDLC so that you know how the guard rail configurations will impact your application. You simply do not want to slap it on, uh, when you go to production. So once we've defined our guard rail strategy, the next step is to configure the the word list, the block list, the denied topics, and test. There are sorry, get a bit sweaty, um, there are a number of configurations in the guard rail that we've just covered and what is optimal to you is gonna be very specific to your use case. So we encourage an iterative approach to designing the security policies within the guard rail, and that requires a testing framework so that you can fire prompt test. Cases at your guard rail and validate that you're getting the expected results when you start to get results that aren't exactly what you anticipated, that's where you want to tune the denied topics, the sensitivities within the content filters and maybe turn on higher thresholds for things like profanity or hate or all these things that you might have within your security policies. Once we have a guardrail that we believe to be tested and fit for purpose for our use case, we want to move on to enforcing that guard rail through policy so I presume everybody's familiar with identity and access management policies and your ability to use conditional statements within them. When you're building the IEM policies for your use case and you have solutions invoking bedrock on AWS, you can use conditional statements to enforce the use of specific guardrail versions, which is why it's important to version guardrails, and you can ensure that highly high quality tested guardrails are being used for your solution, and that's all done through identity and access management conditional statements. So once we've got the guard rail built, we've got it tested, we're enforcing its use across our enterprise or across the solution we want to move into a monitor mode and the nice thing about bedrock guard rails is there's a number of cloud watch metrics that are emitted from the service. Most importantly for me is the guard rail interventions so you can identify when a guard rail is intervened, how it intervened, and why it intervened, and you can also. Use these data points to further tune your guard rails. So if you're getting a number of interventions, it might be an opportunity to go investigate malicious use of your of your app, um, and then bringing this full circle as we're monitoring, we're understanding all these data points, we want to continuously reevaluate our use case. Are we seeing interventions because of prompts. It expect if we are, should our use case allow for those prompts? Do we need to change the denied topics or the examples within them? Do we need to change the word and phrase list? How do we continuously evaluate and iterate our guard rail solution to ensure that we're continually fit for purpose? So this is the overview of how you. Uh, go from A to Z of guard rails. Um, next we're gonna highlight the specific iteration and testing to make sure that your guard rails are fit for purpose, so I'm gonna pass it to Vic. So thanks Corey. So before we dive in into the need for automated testing, I want you to think of a web application rule, web application firewall rule that we have created and have applied that on your web application, but it is not tested. Now you look at the configuration of the rule, you know how it's gonna work, but until and unless you test it with the live traffic or live like traffic, you, you, there is no certainty. How it's going to behave. It is the exact same case with the guard rails as well. You create a guard rail, you apply that on your Gen AI workload, and if it is not tested, it could be too permissive, meaning it could allow a lot of malicious prawns to come in and it might cause business risks. If it is too restrictive, then it might cause business interruption on your GI obligations. So what we recommend is identifying the edge cases for your guard rails and creating the prompts for it and doing a lot of testing. So what we have seen working with our customers is there's a lot of challenge in testing these guardrails with prompts. So you're not testing 10 prompts, you're testing hundreds of prompts against a set of guard rails, so. When we work with our customers, we always recommend creating an automation framework where they can automate the testing of guardrails with hundreds of proms, visualize the results, and analyze them and understand where they can fine-tune their guardrails. So next we are gonna dive deep into the reference architecture that you can use to build your own uh evaluation framework. So this is the evaluation uh reference architecture for guardr testing. It is completely serverless, uses S3 buckets, lambda functions, SPSQs, Amazon Bedrock, and QuickSet dashboard. So what you see here on the left hand side is a promp configuration file. It's gonna contain all the prompts and the guards. That you want to test those prompts against. So in this case we just have 3 prompts and 2 guard rails. In the real case you may have hundreds of prompts and tens of guard rails. So once this file is uploaded to this S3 bucket, this S3 bucket has uh has been configured with the object lambda. So the moment it is uploaded here it triggers this message generator lambda function. So what this lambda function does is it fetches this prompt configuration file, matches, maps each and every prompt to each and every guardrail, and that's what you see here in the generated messages box. So each prompt is mapped to each guardrail. So there are about like 6 messages here, and each message and each message is sent to the Amazon SQSQ. This SQSQ is 5O in nature, which is first in, first out. So when the message one gets to the SQSQ, the lambda function guard will evaluate lambda function picks it up. And then it calls the Apply guardrails API on Amazon Bedrock. So what Apply guardrails API does is it takes the prompt, takes the guard rail, runs the prompt against the guardrail, and provides us with the evaluation results. Those evaluation results are, uh, analyzed by the lambda function here and stored in the SA buckets. Once all of these messages are processed, they are stored in this SU bucket and visualized through Quick site here. So before we take a look at the quickset dashboard and the final results, let's look at the guard rail that we're testing here. So we have a fin ops guard rail. This, uh, this guard rail we have created for a finance chatbot. We have configured all the filters here sexual violence, hate, insults, misconduct, plumber attack, and set the strength to high. We have configured the profanity, uh, here, so this is the. Database managed list of profanity words. If there are any words which match this criteria they get blocked and then we have configured a topic for tax advice. It's a synops chatbot. It shouldn't be advising you on how, how can you claim deductions on your tax. So this is one of the topics that we have, uh, configured as a denied topic, and then there are different PII configts as well, like address, age, email address. IP name so the fin up chatbot really has no use case of processing all of this PII because it is just providing you with advice how you can invest money it doesn't need all of this PII. So next we are gonna look at some prompts that we are gonna run against this guard rail. So on the top we have some healthcare guard rails, then we have some finance guard rails, and then we have some, uh, sorry, we have some healthcare prompts, finance prompts, and then we have some PIR prompts. I want you to focus on PHI 4. So basically this prompt says is this is my social security number, provide me with my laser health records. Now let's see how our guard rails work. So this is the screenshot of the Quick site dashboard. We are visualizing all the results from, uh, the analysis that we did. This is a very wide screenshot. That's why it has been divided into two parts here. So the first column is job name, which is we have named as first one because this is the first iteration of the prompts that we ran through the guard rails sources input because these are the input prompts to the model. So I want you to focus on this prompt ID PHI 4 and you ran this against the two guard rails, Finops guardrail and Healthcare guard rail. So what we can see here is there is none, no action taken by both the guard rails. Now this is expected for a healthcare chatbot because it's a legitimate use case because you wanna have healthcare chatbot answer the questions related to your health records, but it is not a legitimate use case for a Finoffs guard rail. So here we see that it was not blocked. Next, what we do is we find tune the guard rail. Now we have identified that there is a problem with the guard rail. We need to fix it. So we go here in the guardrail configuration and we add a healthcare topic. Basically what it says is do not provide any information related to healthcare disease diagnosis, medical records, and then we have provided some samples so bedrock can learn from it or Bedrock guards can learn from it. And then in the PIA confit we have also configured US Social Security number. With these configurations in let's see how the evaluation takes place now in the second run, what we are seeing is there is no impact on the healthcare guard rail. There's still action as none because we did not update this guard rail, but we did update the Pinop's guard rail here and it has intervened on two accounts, so it has detected the US Social Security number. And then it has also detected the health care topic and the topic type is denied, so it has blocked this prompt on two accounts. So the prompts are evaluated holistically. They are not evaluated just for the action or just for the topic. It matches all the different cri criteria that you have specified within the guardal configuration. So this was a very simple, uh, demonstration of what is possible when it comes to fine tuning your guardrails. Next, I would like to thank every one of you to take your time out and attend our talk. I would request all of you to complete your session survey and your, uh, AWS events app. Thank you.
