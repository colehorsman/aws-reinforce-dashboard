# AWS re:Inforce 2025 - Code, normalize, detect: The detection-as-code revolution (TDR307)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=0xYTdh4YKFw)

## Video Information
- **Author:** AWS Events
- **Duration:** 35.3 minutes
- **Word Count:** 6,008 words
- **Publish Date:** 20250619

## Summary
This AWS re:Inforce 2025 session (TDR307) presents a comprehensive framework for implementing "detection-as-code" methodologies to transform security operations from manual, error-prone processes to scalable, automated systems. Delivered by Julie (Detection Engineer) and Naisha (Product Manager) from Datadog, the presentation addresses the critical challenges facing detection engineering teams: inconsistent quality, siloed development, limited scalability, and lack of centralized version control for security detections.

The session demonstrates how applying software development lifecycle principles to detection engineering enables teams to achieve peer review processes, extensive testing capabilities, seamless deployment pipelines, and improved reusability of detection logic. The speakers emphasize that detection-as-code isn't just about storing queries in GitHubâ€”it's about creating a comprehensive system that includes data normalization through unified data models, automated testing frameworks, and continuous monitoring strategies.

Central to their approach is the adoption of unified data models, specifically the Open Cybersecurity Schema Framework (OCSF), which enables consistent field mapping across different security tools and log sources. This standardization allows detection engineers to write cross-platform queries, accelerate threat hunting activities, and create more effective visualizations for both operational teams and executive reporting. The presentation includes practical implementation details from Datadog's internal deployment, showcasing how GitHub Actions, Terraform, and API-driven approaches can automate the entire detection lifecycle from development through production monitoring.

## Key Points
- Detection-as-code transforms manual security operations into scalable, automated systems using software development lifecycle principles
- Traditional manual detection deployment doesn't scale beyond small teams and lacks centralized version control and audit history
- Unified data models (especially OCSF) enable consistent field mapping across different security tools and log sources
- GitHub-based workflows with pull requests enable thorough peer review and knowledge sharing across detection engineering teams
- Automated testing includes unit testing with synthetic logs, dynamic testing in staging environments, and impact analysis on historical data
- Attack simulation using tools like Stratus Red Team and Atomic Red Team validates detection effectiveness against real attack patterns
- Open source Sigma rules provide baseline detection coverage, allowing teams to focus on environment-specific threats
- Data normalization reduces investigation time by standardizing field names across different log sources (actor, username, identity fields)
- Continuous monitoring and detection review processes prevent stale rules and ensure ongoing effectiveness
- Multi-org deployment capabilities allow selective rule deployment across production, staging, corporate, and government instances

## Technical Details
- **GitHub Integration**: Pull request workflows with automated linting, validation, and deployment jobs triggered by code changes
- **Terraform Infrastructure**: All rules, suppressions, and automation workflows managed as Terraform resources for consistency and repeatability
- **CI/CD Pipeline**: GitLab/GitHub Actions automating build process from staging to production with human error reduction
- **Multi-Instance Deployment**: Selective deployment across prod, staging, corporate, and government environments using Terraform
- **State Management**: DynamoDB for rule state locking and S3 buckets for Terraform state file storage
- **API-Driven Deployment**: Programmatic rule creation, validation, and deployment through cloud SIM APIs
- **OCSF Implementation**: Open Cybersecurity Schema Framework for unified data model with AWS-focused field mapping
- **Testing Framework**: Four-tier testing approach including unit testing, dynamic testing, impact analysis, and attack simulation
- **Historical Analysis**: Backtesting capabilities running new detections against 6-10 months of stored data
- **Data Pipeline Integration**: OCSF integration within logging pipelines for real-time field standardization and enrichment
- **Metadata Management**: In-code metadata including references, playbooks, modification dates, testing status, and deployment stage
- **Version Control**: Centralized GitHub repositories with audit history and change tracking for all security detections
- **Detection Monitoring**: Automated flagging of detections older than 6 months for review and potential decommissioning
- **Open Source Tooling**: Integration with Wazuh SIM, Airflow workflows, GitHub Issues for case management, and Encoder.io for query language conversion
- **Cross-Platform Queries**: Standardized detection logic deployable across multiple SIMs and security products from centralized repositories

## Full Transcript

Thank you all for coming. Um, we're gonna be talking a lot about detection as code. Um, it's something I think a lot about, um, I've been doing detection engineering for the last 5 years, and then Naisha also dives really deep into it because she makes sure that we're giving customers and our product exactly what they need to do things like this. So kind of give you a little bit of background about me, um, been doing detection engineering in response for the last 5 years. I ended up joining Data Dog a year ago, um, to focus more on the security research side. So what a lot of that looks like is threat hunting, understanding like current threats, attack paths, and then how we can build better detections. Yeah, and I'm Naha. um, I actually started, I guess I've spent the majority of my career in security, but, uh, in the past 4 years I've been building, uh, the security products at Data Docs, so working with personas like Julie and making sure that they have what they need from a product perspective and build those for them. Yeah, it's a team effort. Um, so I'm gonna go over a lot of the principles of detectionist code like how we think about it, and then Emission is gonna dive into how we've implemented it internally. So I think it's like a buzzword right? you hear detection is code it's like important but then people don't really go into how they implement it, why it's important so we're gonna talk a little bit about that um I've worked in organizations where you know you're still deploying detections very manually. Um, and that's great when you're in a small team, but it doesn't scale and so when you're thinking about how do we enable a detection engineering team who has limited resources to ship quality detections and then also be able to manage them in a very reasonable way. So the way I think about it is we have this PR we give engineers the ability to do very thorough code reviews in a very like seamless way um then we can deploy very extensive testing and then we can actually deploy this to our SIM. And also I would say this is great if you're using multiple SIMs or multiple security products where you're deploying detections because you're storing detections in a central place and then you're able to say well I want to send this to the Aquarian Snowflake. I wanna do this for my actual SIM and it gives you a lot of control over how you actually implement your detections. So some people might be familiar with this um this is kind of my diagram based on a lot of research that has come out from other practitioners about how they apply the software development life cycle to detection engineering um I highly recommend this great article by Haider at Snowflake. He used to run the detection team and the way that you think about detection development is actually very similar to software engineering. And I think the more parallels we draw between software engineers and people that do DevOps SRE can really help us uh increase both our posture and the scalability of our detections. So I won't go through every step here I think some of them are pretty um standard if you're familiar with the SCLC but some very like important aspects of it is I think defining requirements and how you approach design so there's a lot of steps you get to before you actually write the actual query. And for our team we focus a lot on what is the best approach. Can we prove that this is a path that an attacker would take? Have we seen it in the wild? How do we actually want to build the query so like what thresholds do we want to define? Do we group a bunch of anomalies? There's all these design um aspects and sometimes the outcome of design is that you're gonna deploy multiple detections for the same attack paths so you can best evaluate them. Um, and so all of this goes into development. We have the testing and deployment we'll talk about and then really thinking of this as a cycle. So once you deploy a detection, it isn't static. You need to review it like code you might need to refractor it. You might need to update it if log structure structures change, um, if you attack surface change and it's no longer relevant, maybe the alerts are blowing up because of a new process that IT has implemented or the infrastructure team. So these are like ways to think about detection engineering and continually improving them outside of just your sock or your analysts are upset about the noise, right? So there's things you can do to make sure that both the detections are working properly and being reviewed continuously as well as they're not blowing up for your analysts. And so some of the main challenges I hear from other detection teams that I know is they don't have the centralized version control, audit history, understanding of where their detection was in a point of time. And really being able to track the growth and the changes of those detections, which you get when you're centralizing your code and something like GitHub. You really have inconsistent quality because it's harder for each of those detections to be reviewed so you might have one person on your team that like approaches the detection in one way, another person that approaches it another way, and you don't get to share um that knowledge and like understanding of how you wanna approach detections as a team. Um, and that leads into the siloed development and then the hard to manage at scale again how I mentioned like a lot of teams have multiple solutions they're using and understanding what is deployed to what, how you can easily search your coverage, um, and like what is being deployed. So I put a lot of memes in this talk, um, but thinking about detection as code. There's some aspects of manual engineering that you do a detection that you're not able to track fully so if you're deploying a detection, you're not doing continuous testing you're not monitoring, you actually don't know if the detection's working as you expect it to and then an incident happens and you're like, but I wrote a detection about this and then you realize oh again the audit log changed or the threshold wasn't working um as expected. Or something in the query language changed there's all these things that you need to think about continuously to have effective detections and the worst thing you want is an incident happens and then they come back and you're like yeah the detection didn't work. So kind of the parallel to some of those manual engineering issues um you get the benefits of that peer review um being able to do extensive testing however you wanna set up testing in your environment uh there's a variety of ways that you can do this depending on the maturity of your team. You get those like benefits of deployment just like software engineers do. And then really the reusability has been a big thing for me understanding a detection engineer that I work with built out this really crazy complex query that is amazing and like I don't have to go through just starting from scratch when I'm also looking for a similar attack path but if that was deployed in the SIM it would be really hard to find and pull and understand um so just being able to search GitHub and get everything I need and really speeds up detection engineering uh time. Um, so this is a screenshot from a Sigma rule, which is the most common open source, uh, detection language and then also repository. But one thing I really liked um for deploying detection as code is you can actually inlay all this metadata into the code that you're storing in your repository so things like references um even playbooks, uh, when it was modified, when it was updated the last time it was tested, um, what is the deployment looks like right now? is it in production is it in staging? Uh, you get a lot of really good context that like it's up to your team to add and you have the power to include this, um, in a way that if you were navigating a semi I you wouldn't really have the ability to build your own context. So again with the open source rules, you know, Sigma is the biggest plug everyone uses Sigma, um, or at least looked at it. So the one thing I wanna call out with open source repositories is a lot of these detection as code queries that are available open source Sigma, etc. elastic you're able to understand the coverage, the basic detection coverage that a lot of other companies are deploying. And then you get to shift your time as a detection engineer into things that are really critical and unique to your environment um and that's where you get like the really interesting problems you can tell leadership that you have visibility into these things and really prioritize your time in a way that tailors it to your organization. Um, and then one thing I just wanted to mention is there's a site, I think it's a couple of years old, um, it's called Encoder.O if people aren't familiar with it, um, but it allows you to convert detection queries to other languages, very popular like some languages, etc. So then if you're approaching, you see this open source repo and you're like, wow, this detection's amazing, um, this tool might be able to help you convert it into a language that you're actually using so I highly recommend checking it out if you're thinking about adopting some of these open source rules. OK, I'll hand it off to Naisha. Alright, um, so yeah, now that Julie has sort of laid out all the foundation and how you could potentially, you know, benefit from, uh, detection as code, let's dive a little bit deeper on how we actually do that internally at scale at Data Dog. So how we write these rules, how we deploy them, how we test them, and all that fun stuff. Um, so our development, uh, like, uh, pipeline really just starts like any other software engineering, uh, teams would, which is GitHub, right? So detection engineers go in, they write these rules, uh, they write it in code every change that goes, uh, into these rules there again, uh, goes through the pull request process for peer review so all these. PRs get, you know, reviewed um and then they get merged so the moment that you cut a PR out uh or we have GitHub, um, you know, jobs that kick off right away so these include lining, so checking the syntax of the rule, uh, we look at validation, so making sure the rule can actually pass through the API for Cloude and finally deploy it wherever we want. Uh, so at Datao we also have multiple instances so we wanna be selective and make sure that we deploy it at the right spot and everything again from rules to suppressions to automation work flows, they are all managed as terraform resources that really helps us with consistency, uh, repeat repeatability that Julie just mentioned, and then they get finally deployed in prod into Qatam. Um, so let's take a little bit, uh, a deeper look at how we actually go through the entire process. So every engineer starts in the UI. This really helps, uh, it's easy to write those queries, those really long complex queries, uh, we write them in the UI itself and it. With a simple button we're able to sort of export it as terraform files um once they're uh we take those terraform files we look at a lot of standardizations in our formats so things like tags are very important for our teams to maintain minor tactics techniques to then, uh, I guess look at the, you know, detection coverage that we have overall throughout those rules that we deploy. And then we also look at um our query so these queries really help us understand um you know what's the actual search that we're running and what the logs that are going to match to this rule, and they're all again wrapped in this terraform resource we also look at the message which is really our. Detection playbook so we look at um you know our detection strategy why we wrote this rule um how our responders can actually take this context and run certain, you know, actions from there um and all the blind spots to look out for really just the why behind the rule that was written. And then again, uh, all of our templates our PR templates really help us to understand what uh what the changes were made, um, why, how it was tested, uh, what are some of the jobs that we ran internally, externally manually, all that fun stuff, and then get that um cut out and reviewed by someone on the team. So our GitHub, uh, Gitlab, uh, pipeline is a series of jobs that we trigger. So once the PR is approved, our CICD pipeline takes over, uh, from sing staging to production, it really just handles that end to end build, uh, process. So this automation really just helps again with human error and ultimately helping us, you know, deploy rules faster. Um, GitHub will uh Gitlab will here trigger, uh, a series of jobs like I mentioned in RCI. So this would again include lining jobs. It really just helps you look at the structure of the rule, check for any syntax, right? We also have validation that helps us, you know, make sure that all the relevant fields that are required for the rule to get deployed. And then, uh, ultimately the deployment really helps us figure out which instance we need to deploy this rule to and I'll go a little bit deeper into that as well. Um, we manage all of our, um, all of the, uh, rules that are deployed into, so Dynamodibi it really just helps us to lock the state of that rule, and then, um, S3 bucket is where we hold all of our, um, state files. Um, so here's where I wanted to talk a little bit about our multi-org deployment. So at Datado internally we have multiple different instances that we manage for our SEM. So this includes our prod, our staging, corporate, even Gov. Right, so, uh, really Terraform just helps us, you know, to make sure that we're able to select the different types of rules and make sure we're able to consistently either deploy it everywhere or selectively to one of these instances. Um, yeah, and ultimately this is where you know it's all backed by our same API. We are pragmatically able to deploy, create, validate all of these rules, making the detection truly, you know, code driven through the API, the cloud and API itself. And yeah, I'll pass it to Julie to talk a little bit about normalization. Yeah, so at Data Dog this is a problem that we saw in that there's a different need for data normalization for um logs that are coming in about infrastructure resources applications that our SREs are may be dealing with and there's an entirely different approach to data normalization for security engineers it's really important that we have consistent fields that. That we can write detections again we can speed up incident investigations and really being able to better understand the data and for us the facets and the fields we're building on the software engineering side for people that use Data Dog for observability just really didn't work from the security perspective so we wanted to pitch and build this unified data model. And so like as I mentioned the unified data model has a lot of key benefits and I think one of my favorite out of this is also just the data reporting and visualization that's possible so when you're building these graphs you're understanding your coverage what alerts are firing you can actually dig in into the individual fields and understand alert noise actors that are. Taking multiple actions that are triggering multiple alerts you can build these better visualizations and graphs to both speed up your security analysts and take those visualizations and use them for leadership. And so I'm sure people have seen this one's pretty popular uh but there's this idea that everyone wants to create a unified data model structure that security engineers use so there's so many standards I'll go into them. But then every vendor keeps creating a new standard or maybe like open source um and a lot of ones that are open source are influenced by vendors so you just keep seeing these standards being created and you have to like understand what is the best for you to adopt and then also what meshes the best with the security tooling you're using. So say these are probably the 4 biggest ones I've seen people using. Elastic has been around a long time. It's a very common one, especially if you're using Sigma rules. Um, obviously if you're using Spunk, you're gonna be using the common information model. A lot of spunk partners will also adopt this or have this as an available option. Uh, AWS has been doing the open cybersecurity scheme of framework which has like more of a observability SRE logging focus especially for AWS logs it's highly influenced for people that have AWS environments and then uh Chronicle also came out with a unified data model, um, as they were running into the same issues that some other vendors were running into. And so we spent a long time trying to figure out what the best unified data model is and we didn't want to create a new one either open source or like Datao uh so we we evaluated internally and because we're so heavy reliant on AWS and we focus a lot on cloud trail, um, a lot of cloud environments and also being able to map fields from one cloud environment to another, uh, we ended up choosing OCSF. So some of the key things about OCSF is you, while it's influenced by AWS, you can take the current log fields and you can still map it to standardized classes and fields. So you still have a lot of the flexibility of like a traditional unified data model. Uh, you can then integrate OCSF into your logging pipeline. So as your logs flow in to your environment, you're actually transitioning it into OCSF in a standardized way. Um, a lot of people think about logging pipelines. Um, some people just pipe logs directly into their SIM, whereas other people maintain logging pipelines where they can do enrichments in line, um, they can drop certain fields, add certain fields, etc. So if you are doing it this way, you can actually adopt OCSF in the pipeline or another unified data model might come with your security solutions. And then another thing about OCSF um or other unified data models is just being able to test the schema, understanding if this is actually gonna work against your logs, uh, what fields are gonna be populated, etc. so you have as a detection engineer full visibility into what your logs are gonna look like and how you can write detections across sources. So this is like one example we did where we were looking at GitHubb logs and then we're mapping it to those OCSF fields so that way if we're looking at GitHubb logs versus Gitlab logs we or even like across unrelated sources you can then change. Detections together and have quicker investigations when you know actor that username is always gonna be present and you don't have to say I'm looking at actor here and username here um and identity here so you get like that um faster investigation time but also the simplified experience for detection engineers. So one small step in data pipeline engineering is one giant leap in general. So manipulating data pipelines, working with log ingestion is one of the hardest problems for security engineers, both from getting logs, understanding when logs drop, storing logs. the expenses of storing those logs for a long time understanding when you keep them in hot storage, when you keep them in cold storage, etc. so really having a centralized strategy and this unified data model helps in a small way to better your overall data engineering pipeline. Alright, um, so on to our favorite topic, probably not, but testing detections, it's painful sometimes it leads to, you know, uh, something was missed and we all have been in a position where I didn't check a syntax or I wrote the field wrong and then all of a sudden I have 1000 alerts being generated in my SIM. It's never fun and the clean up is not. Right, so just wanted to talk a little bit about testing detections. It's something that uh even I as a product manager we have been working with a number of customers they always say, hey, what can your tool do to help us with that? It's a combination of, you know what the tool can offer you, but also, you know, the practices that you can embed in your detection engineering, uh, process. So let's go a little bit uh deeper into that uh just wanted to start a little bit with you know the key benefits really again you wanna be able to validate um you wanna be able to look for accuracy at the end of the day we're all trying to detect that one thing and you wanna make sure your detection allows you to do that. Uh, ultimately reduce the number of false positive, that's a given every detection that we write, we wanna be able to keep fine tuning it and make sure that it again, uh, reduces the number of alerts I need to triage for it. And then ultimately also uh continuous development is another piece here based on feedback, based on the number of alerts, the metrics that we got from that detection, you wanna be able to go back and then test it again and go through that entire process and do it at scale. So I'm gonna go into a couple methods, uh, again from a tool perspective also from a practice perspective, uh, that's helpful uh in testing so we look at four angles here, um, unit testing, dynamic testing impact analysis, and finally attack stimulation. Um, so starting with, um, unit testing, this is really just your first line of defense. You wanna make sure that your logic, uh, can run on synthetic logs. Sometimes I'm writing detections for stuff that has never occurred that has never occurred in my environment. I don't. Know what the log looks like I wanna be able to take a sample log and run it against that detection that I just wrote, right? So to make sure again the query works uh as intended, uh, so think of it as like a sanity check before anything goes even further. Um, then we look at, uh, dynamic testing, so this, uh, again by no means this is something that we follow internally very well, but this is again these are practices that we wanna be able to get to, um, there's this step that helps you sort of, you know, look at the logic inside our SIM, make sure it goes through the entire CIC. um, the, uh, I showed you how we do that internally at Data dog, and then we wanna make sure uh the syntax is fine, uh, functionally the logic really works and it confirms that both, uh, those internal and external factors will make sure that the detection will work as intended. Um, going into, uh, impact analysis, so this is something that I've, uh, even from a tool perspective I've been working with a lot of customers on users come in and they say, hey, I have all this data, all this telemetry that lives within Datao. How do I use that to improve my detection, right? You want to be able to take that detection that you wrote today and run it on past 10 months of data that's been sitting in your cold storage, hot storage, or whatever that means, right? Uh, you wanna be able to run it against that and see how many, um, you know, alerts would have triggered. Did I miss anything? Do I need to, you know, escalate those results that I got into an actual alert in my environment and get that triage, right? So understanding the impact, uh, before really moving the detection from testing into prod is super, super important and, uh, even with Cloudim you're able to run a simple historical job that says hey I just wrote this rule today run it on past 6 months of data that lives within Data Do. So yeah, incredibly useful for tuning and making sure that you didn't miss anything in the past and finally attack simulation so uh here at Data Dog we have a dedicated team of security researchers they have developed these open source tools so Strauss Red Team, Atomic Red Team. They what they really help you do is assimilate an attack in your environment. Environment, uh, so what that means a series of logs, uh, series of events that occur in a specific pattern that would simulate what an attacker would actually do and then you can take your detection and check if did a trigger for those actions for those locks that came into my environment. And this really helped me again to check um you know if the rule would have worked tomorrow if the attack was actually real um there's a lot of other open source tools you can also work with your red team on these type of practices but yeah um and yeah um again it's just means of us trying to be as proactive as possible uh before taking any detection into prod. Um, yeah, and I'll give it to Julia to talk a little bit about strategy. Yeah, so I think monitoring is the hard part. Um, we're very constrained by resources as a typical security engineering team, and once you deploy detection to production, you're not necessarily thinking about how do we monitor it unless of course you get a ton of false positives and like alert fatigue from your sock. The detection kind of just sits there and so building a process whether it's manual with like Jira or it's like automated by flagging uh different like past pull requests and versioning. It's really important to make sure that those detections are staying accurate for your environment and they're useful for the purposes you need them to be. So for monitoring you can do detection reviews I think this is a more manual process but it's really important depending on your organization so maybe you have the person that is on call or like sometimes they're called goalies or like the assistance person. For other teams asks or like ad hoc projects maybe they're doing that position for a week and then if a detection has been in production for longer than 6 months they get a ticket or like they get a ping and then they can re-review it that's like part of their like operational job. Um, again, not perfect, but different organizations have different resourcing so sometimes people just do like a bug bash, like little hackathon where they spend time the whole team looking at past detections, but having some sort of process is really important because otherwise they get stale. Uh, also considering detection decommissioning, this is like a discussion to have both internally on the detection team understanding what detections are still useful for you, but then also engaging the sock or whatever security analyst team you have because if they're getting pummeled with the uh alert, um, they're either going to come up to you to tune it or you have to make the valuable decision if the alert is even worth keeping in the first place. Uh, and then last, I just wanna like talk about improvements. So when you have one detection engineer create a query for detection, maybe they did it through a GitHub repo through a pull request other people reviewed it, or maybe that detection even existed before anyone on the team, like current team was there. So understanding from your perspective where you come in you understand the current technology, the current way the company is working um how can you make that detection better how can you make the playbook better um is the logic still relevant? You can use your expertise to improve maybe some um foundational work that was done before but has not been maintained, etc. um, really making sure that those key security events don't fall through the cracks. So some key takeaways to all of our yapping is why would you adopt detection as code and a unified data model. So these are like the two key aspects to actually doing detections at scale as your organization grows bigger or your team is small, these are the best ways we found to make sure that you are empowering your detection engineers and making sure they're both writing quality detections able to focus on important things and not spending all their time on like my details. So normalizing the data allows us for example to write a detection that might be focused on an AWS behavior but that same behavior occurs in GCP and we can write across telemetry detection we can also do more efficient threat hunting and detection analysis so that normalization piece has been the biggest game changer for us. And then as I showed in the beginning there's the standard detection development life cycle that is mimicking the software development life cycle, but really taking that understanding what it means for your team and how you're gonna approach each of those facets or you're gonna prioritize them in your switch detectionist code. And then as the Misha talked about, the testing mechanisms are really critical and you're empowered to do that when you're doing detection of code. So being able to one of my favorite is like so you make a pull request and then for example an impact analysis runs on our past like two months of data so then you know if you push this detection, what is the noise level that your analysts are gonna feel um what are they gonna be seeing and you can really make a strategic decision as a team does this need more tuning? Is this good enough for our environment? Do we keep it in testing longer? Um, and so you really get that visibility and how to make the highest quality detections, especially if you aren't the one triaging them. And so just like overall we're gonna talk about some next steps that you can take to adopt this in your environment, but if you get anything out of this talk, I just want you to know that these are two critical aspects to just being able to have a detection engineering team that has both a good quality of life and can scale very quickly. So if you want to implement this in your environment, um, there's a couple of different ways you can go about it. Um, Nisha's gonna talk about how you would do it at Dataog and then I'm gonna talk about some open source alternatives. Alright, um, so yeah, let's connect all that we learned all the dots, uh, and see how that really comes together, uh, in data cloudsim. So with cloudsim you can build, you can test and scale your detections, uh, using the automation and you know, really the detection as code strategy that. We just talked about today so we can link the logic you can run unit test to uh with mock data you can backtest with the real data that's already stored in Data Doc then you can go in and convert those rules into terraform uh for standardization, deploy them with via terraform. And uh or just the API uh into your live environment and then ultimately track everything through through uh version control and all of this is fully automated um you know all the system is designed for you with uh each of the APIs available in the tool and yeah hopefully that really helps with, you know managing those detections at scale and I'll pass it to. Julie, to talk about the open source. Yeah, so going through each of the steps that we kind of talked about the critical infrastructure that's needed to implement these things, uh, you can really approach it in a lot of different ways, but I just wanted to give some examples of how you could do it on like a really low budget or using open source tools so you don't have to worry about like how you're gonna maintain it and things like that. So, um, an example is Wazoo SIM. Um, I haven't personally used it, but I've heard good things. It's open source, um, there's plenty of ones out there where you can understand like your self hosting, uh, you have complete control over your SIM. And then using something like GitHub repository, if you're using GitHub repositories, it's very easy to adopt GitHub actions so you can kind of use that entire pipeline and then maybe from a detection for example because you have a deployed as code you have metadata that associates it with an automation. So if that detection triggers, um, you automatically have a work flow system that will take actions or recommend things to analysts so having this detection as code approach both links you into the detection engineering process to improve it, but also like the end experience for the analyst. And I've used Airflow in the past, really like it, um, and it also helps if engineering teams within the company are using it and then you're able to like mind meld and like share code but also just leveraging whatever work flow and automation tools your company's using internally um and also sometimes that means that you don't have to pay for a license which is nice. And then you can do case management so I mentioned GitHub issues here. I've seen people use it in the past um it has doesn't have the most features but if you're already using the GitHub ecosystem you can focus on just that entire life cycle um for example, a lot of people use Jira and they already have licenses the company's already using it, etc. so just whatever case management tool you can leverage it makes the easiest, uh, process for your detection engineers. Uh, that's all we have to talk about, um, regarding detection as code and a unified data model, um, but we're happy to chat if anything comes up that you wanna talk about or if you're wondering how you can, um, deploy detection as code in your environment. Cool, thank you.
