# AWS re:Inforce 2025 - Routing the right data to Amazon Security Lake (TDR222)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=69SnyaPk9ek)

## Video Information
- **Author:** AWS Events
- **Duration:** 19.2 minutes
- **Word Count:** 3,100 words
- **Publish Date:** 20250618
- **Video ID:** 69SnyaPk9ek

## Summary
This AWS re:Inforce 2025 session (TDR222) presents Splunk's comprehensive data routing strategy for Amazon Security Lake and S3, addressing the critical challenge of balancing security data visibility with storage costs. Delivered by Colin Gibbons from Splunk's product management team, the presentation tackles the reality that customers primarily complain about Splunk's cost rather than its technology, particularly when handling high-volume, low-fidelity security data such as VPC flow logs, WAF logs, and firewall data.

The session introduces a sophisticated data pipeline architecture using Splunk's Edge Processor and Ingest Processor tools, which enable intelligent routing of security data based on value and use case requirements. The core strategy involves storing high-volume, low-signal data in cost-effective alternatives like AWS Security Lake or S3 while maintaining full search and analysis capabilities through Federated Analytics. This approach allows organizations to significantly reduce Splunk storage costs while preserving investigative capabilities for compliance, troubleshooting, and security operations.

A key technical advancement presented is the evolution from SPL (Splunk Processing Language) to SPL 2, transforming from a query language to a full scripting language that enables sophisticated data transformation, masking, and enrichment without requiring manual configuration of props and transforms files. The presentation demonstrates how organizations can implement tiered data strategies with real-time detections running against frequently accessed data in Splunk while maintaining access to historical data through federated search capabilities in Security Lake and S3.

## Key Points
- Customer cost concerns drive need for intelligent data routing rather than storing all security data in expensive Splunk indexes
- High-volume, low-fidelity data (VPC flows, WAF logs, firewall data) can be cost-effectively stored in AWS Security Lake or S3
- Federated Analytics provides dual capabilities: federated search for ad-hoc investigations and Data Lake Index for frequent detections
- Edge Processor and Ingest Processor offer on-premises and cloud-managed data pipeline solutions respectively
- SPL 2 evolution enables sophisticated scripting, transformation, and enrichment without manual configuration file editing
- Data masking capabilities support PII/PHI compliance requirements for healthcare and regulated industries
- Cisco Live customer feedback drives firewall and network device data routing use cases
- OCSF (Open Cybersecurity Schema Framework) pipeline support enables schema standardization across security tools
- Federated search uses Amazon Athena with data scan unit pricing for cost-effective historical analysis
- Enterprise Security detections can run against Data Lake Index data without expensive Athena queries

## Technical Details
- **Edge Processor Architecture**: Linux RPM-based tool with cloud management plane for distributed deployment and centralized control
- **Ingest Processor**: Cloud-managed hosted version eliminating on-premises infrastructure requirements
- **SPL 2 Capabilities**: Full scripting language supporting data transformation, masking, enrichment, and routing logic
- **Federated Analytics Components**: Federated search for S3 data and Data Lake Index for Security Lake with 30-day rolling windows
- **Data Routing Options**: HTTP Event Collector (HEC), Heavy Forwarders, and Syslog input methods with flexible output destinations
- **Cost Optimization Strategy**: Data Scan Unit (DSU) pricing for Athena-based federated search vs. SVC pricing for Data Lake Index
- **Security Lake Integration**: SQS subscriber-based data ingestion for VPC Flow Logs, CloudTrail, CloudWatch, and third-party sources
- **Data Transformation Pipeline**: Common Information Model mapping, field masking, filtering, and enrichment capabilities
- **Compliance Features**: PII/PHI data masking with role-based access controls for healthcare and regulated industry requirements
- **OCSF Support**: Schema-on-write pipeline for Open Cybersecurity Schema Framework standardization and vendor interoperability
- **Management Interface**: Cloud-based UI for pipeline configuration, monitoring, and SPL 2 query development
- **Integration Ecosystem**: Support for Cisco firewalls, Palo Alto, CrowdStrike, and other security vendor data sources
- **Storage Tiering**: Intelligent routing between Splunk indexes, S3 buckets, and Security Lake based on data value and access patterns
- **Query Performance**: Transparent federated search capabilities maintaining user experience while accessing external data sources

## Full Transcript

Good evening everybody. Thank you for coming to my session. My name is Colin Gibbons. I'm on the product management team at Splunk. I'm assuming if you're sitting here, you're a customer, and so one of the things that I'm going to be talking about is routing data to Amazon's security, and it could be Amazon. It could also be an Amazon S3 bucket. It doesn't necessarily have to be security late even though that is what the the the thing is. Typically when we do these sessions, anything that I talk about that's kind of forward thinking, basically road maps are always subject to change, but there's nothing here that I'm going to be really talking about. So if you're a customer. As you know, there are multiple ways of getting data into spunk. We have what we refer to as add-ons, and add-on is something that can be installed on your spun instance which then basically either listens, pulls, or routes data from another product into your splunk instance. The that we about a year or so ago, we announced an our AWS uh TA um. We keep making updates to the TA and so that is available right now to get data get data and to spawn. We also announced last year a new capability that we call Federated analytics, and Federated analytics is really about two parts to it. It's about a federated search part to be able to search data in Amazon security lake and or an S3 bucket, but it also gives you the ability to pull data from Security Lake back into Slonk to be able to run detections against that data. We know that from a security data growth point of view, and if you're a splu customer, customers never tend to complain about splunk from a technology point of view. Customers complain about splunk from a cost point of view. And so we know that not all data needs to be in splu. There's a lot of data that is high volume, low fidelity, low signal data, VPC flow logs, WAF. Firewall data, all that type of data tends to be very high volume data that you might not need all that data and spunk, but you might want to have that data as part of an investigation or as part of doing troubleshooting or or doing or for debunks. Last week I was at Cisco Live, which is part of Cisco now, and went on Cisco Live, I gave a lot of sessions around Cisco firewalls, and I know again, this is like one of the things that customers complain about the most. It's like, hey man, my firewalls generate a lot of data, a lot of noise. I don't want to send all that data into splunk. Is there a lower cost storage option for me to store that data? And I mean, the short answer is yes. I mean through our integrations and our partnerships with Amazon, we've launched integrations with things like Amazon Security like, which I'm talking about NRS3. So even if you're not wanting to store all that data and spawn, having access to that data is very important. If you're not familiar with its processor, its processor is a data pipeline management tool. That you can download from book and we have two different offerings. We have a free version of each processor which basically limits the amount of data that we will route and then we have a premier edition that gives customers the ability to route however much data they want through through each processor. Each processor is more or less. A Linux RPM that you can install on your hardware whether or not it's on-prem or hosted in the cloud, but it has a cloud management stack that basically controls Edge processor. So the cloud, the cloud management component is where you go to basically manage all of your edge edge processor nodes. So like I said, it is a It's basically a routing mechanism, a data pipeline management tool. Now its processor is based on SPL 2. If you're familiar with Spark and you're familiar with Sun query language, you're familiar with SPL. SPL is a query language. The difference between SPL and SPL 2 is SPL 2 is now more of a scripting language. It's not just a query language. Anything you could do from a query point of view, you can still. With SPO2, but now you can do other additional things like scripting, things that you used to have to manually do by modifying props and transforms in a heavyweight forwarder. Now you can do through SPO2, meaning I can go in there. I can do things like enrichment. I can basically do things like masking data and stuff like that. So you know, before, like I said, we had basically different ways of getting data into splong. You you're probably familiar with heavyweight forwarders. Heavyweight forwarders, of course, is one way of getting data into sprung. It's more, more or less a cut down version of Sluk that you can install TAs on and and forward the data into your into spunk index. With ingest actions, ingest actions now basically like I said before, you used to have to go out there and and manually configure the transform and configuration files. Now we have basically a UI around it and then like I said, now it's processed, it gives you the ability to manage that data. So again, when you're setting up your its processor on your Linux system, wherever it might be cloud or on Prem, then you can forward data to its processor just like you can't afford data to splo either through Heck, the HTTP event collector, through heavyweight forwarders, or through CISO. Once the data comes into its processor. Through the cloud UI, you can come in here and you can start doing things like transforming, transforming the data, basically map mapping the data to the common information model or mapping or, or mapping the data to whatever uh data like models you want to. Adding filtering mask. If I want to filter certain data out, I can filter certain data out. If there's certain data that I want to send into spook, but I want to put a mask on it, meaning I don't want everybody to see that data. I can mask the data and then I can come here and I can route the data to a spunk index or I can route the data to to an to an S3 bucket. Last year we announced Federated search for S3. So if you're a Sport customer and you're routing your data to an S3 bucket, you can use federated search for S3 to search that data just like if you were in the console. Now, as I mentioned, when you have these different itch processor nodes that are sitting wherever you're hosting them and you had the cloud plane. No data leaves your your your on site to come up to the to the cloud. Everything maintain continues to maintain persistent on-prem, but you had a the management plan is is is is in the cloud. Makes it easier. For you to go through and manage all these various different edge processor nodes, we also what we have called ingest processor. ingest processor is a cloud managed version of ingest processor that is hosted by us. So now again when we talk about again last week when I was at Cisco Live, some customers, some customers said, Well, I don't want to send all my firewall data into Splunk because of the cost, but I would like to send the alerts from firewall threat defense and splut or I. I'd like to send this data great through its processor now. I don't have to send that data into my spunk instance through the add-on. I'm able to through ingest processor come here and say, OK, I want to send my firewall data, Palo Alto, whatever, whatever firewall you're doing. Then I want to come over here. I want to filter out certain data. And I can then say, I want to filter, filter that data. I want to enrich that data, and then I want to route that data. So, all my connection events, all my connection accepted events, all that stuff, all that kind of noisy data, I can say take that data. And I'm gonna route that data to a an estuary bucket um and or in the future Amazon security like that way I still have access to that data if I still need access to that data, but my high fidelity data. The alerts that are being generated by CrowdStrike, by Palo Alto, by any of those types of products. I want to take that data and I'm gonna route that data over to the S3 bucket or to to to Amazon security light. So this was like Last week when I when I was at Cisco Live was like one of the big things that we heard from a lot of our work, Cisco customers like when it comes to like router data, SD-WAN data, all that data from all those networking devices, they still want to have access to that data. They still want to have visibility for. Troubleshooting for security reasons, whatever your use case might be, but bottom line is I can basically determine through now what we call the data manager, it's process ingest processor, where to where to route that data accordingly. Now, once I had that data and I routed that data into my S3 bucket or are coming soon, Amazon security lake, uh, like I mentioned, uh, last year we announced a new capability called Federated Index, and Federated Index or or Federated Analytics allows you to do a few things like I was talking about earlier, whether the data stored in S3. If it's stored in S3, then I can use Federated search to search that data in S3. So I don't have to persist that data permanently in Sun, but from Spunk, I can still search that data just like if that data uh was in SPU transparently. It uses Athena. So if you're if you're here, you're very familiar with what Athena is, and then it, it is, it is priced and based on data scan units. So how much data, how much data scan. How much data you're scanning during that during that search, so that is how that is how that is priced. If you are using Amazon Security lake, then you have two choices. You can use federated search to search the data and security lake, and or you can use what we call data lake index. Data lake index basically works by listening on SQS subscribers and as data is coming into security lake. Then basically you can set up these subscribers like VPC Flowlog, Cloud Trail, Cloud Watch, whatever you might third party so that as that data is coming in, we listen on that subscriber and then that we route that data, we route that data into Sun into a temporary index and that temporary index is after 30 days, it starts rolling out that data. So why both? Think about Federated search for more like an ad hoc threat hunting use case. There's something I want to go search for. I get a cloud watch event telling me that, hey, I've got this um container, this workload, whatever. Detect some suspicious traffic. Now I want to go and I want to search all the traffic surrounding that particular container, that workload. And all that VPC flow log information is stored in S3 or security light, but then I can use federated search to go in there and run a query against that workload or container looking at all the VPC flow log graph data or whatever one use case. Now let's say before I was using the the technical add-on Splunk to ingest the data from cloud cloud trail into spook, but now I don't want to ingest the data from cloud trail into spook anymore. I want to store it in security lake, whatever, but I still want to run. I, I, I use enterprise security. And I still don't want frequent detections. Well, you're not going to use Athena and eat up DSUs to basically run these frequent frequent detections. It's gonna basically, it's slow and it's gonna basically come at a cost. So Data Lake index allows me to, like I said, pull that data into a rolling window. I can still run my frequent enterprise security detections against that data just like it's in Spark, and then that and then that data will start, will start, will start aging out. And so that's the, um, so that's really kind of like two major use cases right now. But yeah, I would highly recommend like if you're if you're a split customer and you are looking at basically how can I reduce the data that I'm sending into spli high volume, low fidelity. Look at ingest processor. Look at, look at each processor as basically a way to set it up in your environment, route the data in there. Go set up, set up your props and transforms, and then basically determine again what data you want to send to SAN and what data you you you want to send, you know, to, to these, to these um other data lakes and. You know, from a Management point of view, kind of like what I said. When you come in here and you add in your security like integration, you basically come in here, you fill out the information and or your S3 bucket and then right here from the UI you you will see up here basically the ingest processor and this is again the SPL 2 query language that I'm then able to come in here, create a pipeline and. start writing my writing my, my, my filter and what I want to do again, whether or not I want to filter out data, whether or not I want a mask. So last week I was talking to a healthcare customer that when they're routing certain data. That contains personal identifiable information, PII data, or patient healthcare information. They want to be able to mask things like patient names or they want to be able to mask things like that where they don't want everybody, everybody who's logging into spunk, even though if I take data and I route data into spunk, as you know, I could take that data and I could say, hey, from this data source right here, I want to route this data into this index and if I route. That data into that index, then I can go in there and I can R back it. I can basically put whatever controls I want to on top of that data to limit. But even then I thought if I want to get more granular and just completely mask a patient a patient name or other or other potential PII data, then you can come in here and you can basically just create. A mask right down here. I know it's not easy to see, but I could just basically say, hey, anything coming from this field, I want to mask it. But if you're part of this user group or you're this user, then you can read it. So that means if I'm part of that spunk user group, then I would be able to go in there and basically have I have I have access to that data. I know the UI is not pretty, but hey, is writing SPL queries and so which you know we have we do have a new he's laughing and so we do have a new AI assistant that we that we recently announced that basically will help customers write write SPL queries, and we are, we are planning on making more improvements to the UI to make it to make it pretty, but um. I know being in this industry for as long as I have, and when you have uh expert users, they're just like, hey, you know what, I don't want some drag and drop capability, some pretty UI I wouldn't be able to go in there and, and, and write the write the raw query myself. You know, it's like being a python developer having some fancy UI just let me write, let me, let me write it myself. So. Thank you for joining my session. We are in the process of releasing an OCSF pipeline. So what that means is that as data is coming into into spunk, we will be able to normalize that data in OCSF and route that data into S and or route it to S3 and our route it to uh to security. As you know, all data that's stored in OCSF in security light is is stored in in OCSF. We are, we are trying to get, we're trying to start supporting OCSF more as a schema and. SIM is not going away. The common information model is not going to go away. Remember, the common information model is schema on read. OCSF is schema on right. So basically the bottom line is when you think about every time data's coming in and you're mapping that data to to the common information model, basically it's as it's getting the data is getting read, it's basically doing that mapping where what we're trying to get to is supporting a schema on read and as AWS has adopted OCSF, there are a lot of a lot of other vendors out there that have been adopting OCSF. We want to start using it as, as, as our schema, but again, the power of spunk is that you can send anything you want to in spunk, right? I mean, it's like a garbage disposal. You throw anything in it, you throw the switch, and you know, eats it up. So I mean basically bottom line is you can still throw anything you want into it and. We are focused on coming out with more content, more detections, and more dashboards and everything else that is supportive of of OCSF. So great, thank you very much. I told them I was going to be done on time, so I got to walk off stage.
