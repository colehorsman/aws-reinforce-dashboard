# AWS re:Inforce 2025 - Getting the most out of your data for security outcomes (TDR223)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=B5ZmoibcKw8)

## Video Information
- **Author:** AWS Events
- **Duration:** 20.0 minutes
- **Word Count:** 3,373 words
- **Publish Date:** 20250619

## Summary
This AWS re:Inforce 2025 session (TDR223) presents Splunk's (now Cisco) approach to maximizing security data value through integration with AWS services, particularly Amazon Security Lake. Delivered by Colin Gibbons, a Product Manager at Splunk/Cisco, the presentation addresses the common challenge of security data costs versus comprehensive threat detection capabilities.

The session focuses on the reality that security products generate massive amounts of data, making it expensive to store everything in Splunk due to ingest costs for on-premises deployments or SVC costs for cloud deployments. However, security teams still need access to this data for investigations, threat hunting, and compliance requirements. Gibbons explains how Splunk's integration with Amazon Security Lake enables customers to maintain cost efficiency while preserving security capabilities through federated search and data lake indexing approaches.

A key technical advancement presented is Splunk's Federated Analytics capability, introduced in November, which allows customers to search data stored in Amazon Security Lake as if it were natively stored in Splunk. The solution provides two distinct approaches: Data Lake Index for frequent detections and real-time use cases, and Federated Search for ad-hoc threat hunting and investigations. The presentation also introduces upcoming features like S3 Replay, which enables temporary data restoration from S3 buckets back into Splunk indexes for specific investigations or audit requirements.

## Key Points
- Security products generate massive data volumes, making comprehensive Splunk storage expensive but necessary for threat detection
- Amazon Security Lake stores data in normalized OCSF format with Apache Parquet columnar storage for efficient querying
- Federated Analytics enables searching Security Lake data directly from Splunk without full data ingestion
- Data Lake Index pulls Security Lake data into temporary 30-day Splunk indexes for frequent detections and real-time use cases
- Federated Search provides ad-hoc querying capabilities against Security Lake data using Athena with built-in cost guardrails
- Network and security teams often have conflicting priorities regarding data storage costs versus visibility requirements
- Use case latency requirements determine whether to use federated search versus direct data ingestion into Splunk
- OCSF normalization requires updating existing SPL queries and detections written for raw AWS data formats
- S3 Replay feature (October 2025 launch) enables temporary restoration of S3-stored data back into Splunk for investigations
- Edge Processor/Data Manager provides data pipeline routing capabilities between Splunk, S3, and Security Lake

## Technical Details
- **Data Architecture**: Amazon Security Lake stores data in Apache Parquet format with OCSF normalization versus raw CSV/XML/JSON in S3
- **Federated Index Components**: Data Lake Index for temporary 30-day storage and Federated Search for direct querying
- **Pricing Models**: Data Lake Index priced by SVCs (lower cost than standard ingestion), Federated Search uses DSU pack pricing
- **Query Performance**: TSA file format in Data Lake Index provides faster queries and better compression than standard indexing
- **Cost Guardrails**: Automated query analysis prevents expensive Athena queries from executing in Federated Search mode
- **Data Pipeline**: Edge Processor/Data Manager supports routing, filtering, and transformation before storage destination selection
- **Schema Mapping**: OCSF normalization requires updating existing SPL detections written for raw AWS service data
- **Detection Library**: 40+ out-of-the-box detections available for CloudTrail data in OCSF format
- **Preview Features**: S3 Replay launching October 2025 for Splunk Cloud customers only initially
- **Subscriber Configuration**: Security Lake subscriber setup for CloudTrail, VPC Flow Logs, CloudWatch with OCSF event class filtering
- **Temporary Indexing**: Data Lake Index creates time-bound indexes with automatic 30-day aging policies
- **Integration Methods**: Standard add-ons, Universal Forwarders, Heavy Forwarders, Syslog, and HEC for data ingestion
- **Production Deployment**: S3 Replay preview available on production stacks or dedicated test environments
- **Data Restoration**: S3 Replay enables audit compliance by temporarily restoring archived data for reporting and dashboard generation

## Full Transcript

Thank you very much. Uh, my name is Colin Gibbons. I'm a product manager, um, at Spook, uh, now Cisco. I'm assuming if you're sitting here, you're Sun customer, yes, yes, yes, yes, yes, perfect. OK, so yeah, so one of the things that there's I. Well, multiple multiple responsibilities at Splunk, but over the past few years, the main thing that I've been working on is the integration between Spunk, uh, and Amazon services. This could be either from the add-on that he's shaking his head. Everybody's probably familiar with add-ons if you're a Sun customer, so you know that we have add-ons that are able to pull data from third party products into Spunk. And then the other integration that I've been working on is kind of what I'm gonna be talking about. So on top of working on all the AWS integrations a year ago when Cisco acquired us. They're like, oh my God, you did such a great job doing all the integration work with AWS. Now we want you to go and figure out how to integrate all of Cisco products into Splunk. So I've been spending the last year basically working across Cisco on trying to build integrations with all those. Products there's nothing I'm really gonna be talking about that should be under NDA or for or roadmap items, but in case there is basically just take note that um anything that I say that's a road map items is always subject to change your customers, you know that. Anytime a vendor tells you that something's gonna come out, it's probably not. So yes, the nature of software. So one of the things that you know we know is that and again I work in the cybersecurity group and we know that security products generate a lot of a lot of data and we know that spunk can be fairly expensive. Yes, yes, and so we know that a lot of times customers don't want to store all that data in spook because of the cost, whether it's ingest costs for on-prem or SVC calls for cloud, but the key thing is. That for security use cases or even network use use cases you still would have access to that data when when you need it for like investigations threat hunting um or whatever and so you know when we've been working with AWS over the course of the past few years on you know swim lanes of what is what you know what is spunk used for, what is the estuary bucket used for or or security like, you know. You use spunk for a reason. You can ingest any data you want to in spun. You can query the data at, you know, if the data is indexed, if the data is mapped to the common information model and and accelerated it, you get timely responses to that data. If I'm using enterprise security and I'm running content and detections against that data for threat detection for security use cases, that's basically the the the key thing with um. Was getting being able to run searches run detections against that data in real time when you start talking about data lakes, you know, like S3 buckets which you're probably all storing data in S3 and our security la, you know that that comes at, you know, you're not really, it's a it's a data lake. Data storage you're not really running, you know, real time detections and against, you know, against that data. So when we started partnering with AWS, you know, on this, that was kind of like what we started thinking about. We're like, OK, well, customers might be storing their VPC flow logsAF logs, and all this information. And security like they're not storing it in Splunk, but they still wanna be able to run detections against the data they still wanna be able to search that data for threat hunting forensic use cases and so that's really where, where Sun comes into play, um. Depending on whether or not you are an on-prem spunk on-prem customer or spun cloud customer, as you know, for spunk cloud, a lot of our customers store data for 90 days. On Prem, some customers build out infrastructures to store data for a lot longer period of time for compliance use cases. Last week when I was at Cisco. Live a lot of customers I was talking to in the financial and regulated industries were storing a massive amount of Cisco firewall router switch data, you know, in their on-prem on-prem spunk, but they were trying to store high fidelity, low volume data and spunk cloud. Well, the good thing about Amazon security lake is. You're able to basically use security like or an estuary bucket. I mean, it's basically estuary under the covers to basically store data for longer periods of time for compliance. But now if you need to go back and you need to use that data for any sort of like forensic use cases or searching, that's kind of where the integration comes into play. I'm sure everybody here knows. Like, so I'm not really going to spend basically any time on this slide of what data you can store in security lake, you know, AWS services information as well as third party log information. So the key thing here, the only thing I'll talk about this is no matter whether or not you you switch from sending your data from an estuary bucket into security lake. The key thing is, no matter what data sources you store in security like there's two big differences S3 bucket, you know, as you know, you can store anything in an S3 bucket. SV, you know, CSV falls, XML JSON, whatever you want to store the data is not schematized. The data is not normalized, you know, it's whatever and security like the key thing is, is that the data is stored in ark. And the data is normalized to OCSF. What does this matter? I now if I'm going in and I'm using Athena or I'm doing a federated search against the data, now if I say, hey, go look at VPC flow log source IP. Well, because the data schematized to OCSF and it's stored in Parque, it knows where to go look. It knows like, oh, here's the column for uh source IP versus if the data stored in S3, I got to dig through CSV files, XML files and all that. So we were looking at customers were coming to us and saying, hey, you know, we got all this data stored there and again we wanna get more visibility into that data that we're not sending into Splunk and or you know I'll say it even though I'm I'm I work for Splunk we're trying to reduce cost, you know, we don't wanna send all that data and splunk we wanna reduce cost and it was kind of interesting one of the things that. Came up last week when I was at Cisco Live with a couple customers. One customer specifically won't name the name, but on one side of the table I had the the Netox team. On the other side of the table I had the security ops team and security was who was paying for Slunk, but the network team was sending all their log data. You know where this is going. And and and the security teams like, uh, yeah, you're driving up my calls you're driving up my calls with all this router switch data, you know, and stuff like that. So you know we start talking about, well, you know, there's, you know, hey, if send it to S3, send it to security like if you know if you can, you know, and then we can basically come in here and still gain gain visibility. So I mean we've had the TA for a while. We're always constantly making updates to the TA and look, I do have some customers and this is what I always say. What is the use case? How, uh how, what, what, what latency are you willing to deal with the data? To sponsor that use case, some of my customers, they're pulling the cloud trail data into Spark where they're running frequent detections against that data. They, they, they don't want to deal with any latencies because if if there's any changes in their AWS environment or there there's outages or anything that they need to do troubleshooting, they need to be more proactive and they they and they need to know about that immediately. Not store the data in estuary or security lake. Then let me use federated search to basically figure out whether or not there's a latency is an outage or things like that. So what I would say is when you kind of when you're wrapping your mind around should I use federated search, should I use uh uh uh ingested data and, and, uh, spun just kind of think through the use case last November. We announced federated Federated analytics which basically. Allows you if you're a security late customer, uh, and I'll talk about S3 so I mean if you're an S3 customer we also offer federated search uh for S3. This gives you the ability from within SPAN to be able to search data in S3 just like if the data was uh was in uh was in SPAN for security like we, we introduced this component called Federated index. uh, uh, there's two pieces to it. There is what we call federated search. And data like data like index. So when you think about the regular way of data coming into splunk through uh through add-ons or through its processor or through anything like that, you know, the, the, the, the, the data gets stored in a in a regular. S3 bucket, I'm sorry and spunk index now we do have methods to basically then take that data and move it off to uh to an S3 bucket if you want to migrate that data now as data is coming in now into uh Amazon security lake. You, uh, the, the what we call the data lake index piece will listen to subscribers. You set up subscribers and security la VPC flow log, cloud trail, cloud watch whatever data you want is coming into security lake, you can set up these subscribers through data lake index. What this will do is this listens to that or that subscriber. Um, cloud trail, OCSF, uh, event class, OCSF category, you can go out there and you can define. So now as that cloud trail data is coming into security like and we're listening on that subscriber, we will pull that data. Back into spork and and store that data in a temporary index that is basically time time bound to 30 days and after 30 days that that data will start aging out um how is it priced? And it's priced just like it's it's priced by SVC so it's priced based on your current if you're cloud based customer it's, it's, it's, it's priced based on on on on on SVCs and so so but it is actually a it comes out to be a lower cost than a than if you're ingesting the. Data through through the normal channel because we're indexing we're we're indexing the data we're we're uh the data like index is pulling the data directly into uh into the index as as as as opposed to going through the search head and then also it is stored in TSA files which is basically this there's a summary index and so the data is much it's faster to query. It's uh it's a lot better, it's a lot better compressed. It is stored in OCSF and so the data in that temporary index is all normalized and schematized to uh to OCSF. Now from within SON. You're able to query that data. You're able to basically search that data just like that data is in. I mean technically it is in Spock. We are pulling it back into a summary index and it looks just like any other normal index. That's, that's uh capability number 1. Capability number 2 is what we call federated search. Federated search. Allows you to search the data in security light just like the data is in Sun. Now this is based on Athena, so behind the the behind the scenes we are using Athena to initiate those searches, and that is based on the same type of Athena costs that you normally would incur by basically a data scan unit, a DSU costs, but we do. Put guard rails behind it so that some analysts could not come in there and basically write some crazy query to basically drive up your DSU calls. We will look at that query. We will not let that query execute at that query if that query is going to basically, you know, drive up, you know, a cost hit and. And so, uh, if you're looking at federated search, then basically you can buy what we call DSU packs from Spark. So you can buy like, hey, I want to buy 5 DSUs, I want to buy 10 DSUs, whatever. You can basically buy, you know, uh buy these packs. The data like index piece, like I said, is more based on SVC pricing so that you can basically contact your Spork sales rep and basically use the uh the uh that that index. Piece and and it's not going to impact uh that you're not gonna have to pay for anything else. So for example some of my customers were rolling off certain logs out of spunk into security like well so they're not ingesting it in splunk so they're not paying for the SVC and spunk, so they're just rolling it off into the security lake and pulling it back in and again there's still a SVC hit but the hit is the hit is not is not as bad. The use cases here is twofold. So when, when we think about federated search and we think about the ability of searching the data and security like or searching the data and and uh and and uh uh uh or pulling the data threat hunting. So when we think about security threat hunting use cases, hey, I wanna go search for this IOC. I wanna go search for this observable uh I wanna go search for this information and security like that is kind of more of the ad hoc, you know, threat investigation use case when you are wanting to pull data back. In Splunk where you're wanting to run a bunch of frequent detections using enterprise security or even if you're just scheduling searches against the data that's more of a threat detection. So that's where I would say, hey, if it's going to be frequent detections that you're running, that's the that's the data index piece. If it's ad hoc querying and you just want to query the data that's more of the, uh, that's more of the kind of federated search uh search use case now. Um, as I mentioned, the, the, um, the data that comes back into into spunk through through the data like index is mapped and normalized to OCSF. So that means any of your out of the box detections that you've got today that's written based on the add-on of data that you're pulling in from like cloud trail cloudwatch. If you have detections in SPL queries written, that's gonna be based on the AWS raw data that you're pulling into book. So any of your content that you've got that you is written for uh OCS for the raw data or for SIM will just have to be updated to basically work with the new uh the the the the new OCSF schema, um. If you're an enterprise security customer or even if you're not, you can go to our detection, uh, our threat detection uh site and we have a bunch of out of the box we've got up to 40 detections now that are written against the uh the cloud trail data now. Um, another component that we, um, we, we've been working on that you might be familiar with might not it's called edge processor or data manager. Data Manager is a tool. It's a data pipeline management tool that allows you to route data. Um, into Sluk or into a uh into S3 and security lake is on the road map for this. So basically bottom line is you can use a just like if you're familiar with, um, you know how you're sending data and spunk today through heavyweight forwarders or through ciklog or through Heck or anything like that universal Forers. You can send all that data into ingest processor or edge processor, but the key thing is now through ingest processor and edge processor you're you're able to do routing, filtering, transforms and all that stuff through edge processor, um. So, so that way you can then say, hey, this particular data, I wanna route it to uh spunk this data I wanna route it to an estuary bucket or or or to another uh to another data source. And so, um. One other thing that we've got coming out that I, I'm actually actively looking for preview customers, so this is something that we're gonna be launching in in the October time frame but I'm looking for customers. So if anybody's interested, uh, come talk to me. We have a new feature coming out that we call S3 replay. So what Sary replay will do is that will, if you've got data stored in splunk, so we have Federated search for S3 that allows you to search data in in in Sary but S3 replay allows you to replay data from your S3 bucket back into. Spunk index, so I mean you could create a spunk index that's time bound and you could say, hey, I want to go. I'm doing this investigation. I want to go grab the data out of my S3 bucket to pull it back into this temporary index where I can run a bunch of detections against it and or search it or like for example. A couple customers I have spoken to uh wanted to use it for audit reasons. Uh, they, they, they an auditor comes in and says, Hey, prove to us that you have the data stored for whatever this particular workload or infrastructure or whatever, and it's stored in S3. Now they can replay that information back in. Then basically you can run your dashboards and reports and queries against that so that is something that we are looking at launching like I said in the uh in the October time frame um so if anybody we're starting preview and in the next week, anybody who's interested in being preview would. to sign you up and and all that stuff and I think that I am almost out of time so I've got 50 seconds left but what I'll do is again if you're interested in estuary replay, I'll just walk off to the side, give me your information, and then I will, I, I will contact you preview. Works very simple. We can either deploy it on your production stack. All we gotta do is basically turn it on on your in your production stack. This is, so let me caveat, this is for sport cloud customers only in the very for for the first preview then we will look at supporting what we call BYOL or CMP customers at a later time. So basically cloud customers only at first. Uh, or we can basically spin up a new stack for you, a test stack for you to test out the feature if you don't want us to deploy it, uh, on your production stack. And with that said, I'm at 0 seconds.
