# AWS re:Inforce 2025 - Modernizing privacy compliance workflows in AWS (COM221)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=lQd09XcHl4E)

## Video Information
- **Author:** AWS Events
- **Duration:** 15.6 minutes
- **Word Count:** 2,759 words
- **Publish Date:** 20250620
- **Video ID:** lQd09XcHl4E

## Summary
This session from AWS re:Inforce 2025 was delivered by Guillermo Fisher, an AWS Data Hero and fractional CTO/CISO, focusing on modernizing privacy compliance workflows in AWS. The presentation addresses the growing challenges organizations face with Data Subject Requests (DSRs), highlighting that over 25% of internet users filed DSRs last year, with a 30% year-over-year increase expected.

The speaker discusses the significant costs and inefficiencies of manual DSR processing, noting that companies handling around a million identities spend nearly $900,000 annually on manual processing, with each request costing approximately $1,500. He introduces the RVVR model (Receive, Verify, Validate, Respond) as a framework for handling DSRs and demonstrates how manual processes can be transformed into cloud-native solutions using AWS services.

The presentation concludes with a practical blueprint for modernizing DSR workflows, emphasizing the benefits of cloud-native solutions which can reduce costs to under $10 per request and processing time from weeks to minutes. Key recommendations include starting small with one request type, using familiar AWS tools, implementing early observability, and treating the solution as a first-class application.

## Key Points
- DSRs are increasing by 30% year-over-year, with over 25% of internet users filing requests
- Manual DSR processing costs approximately $1,500 per request and can take 7-14 days
- The RVVR model (Receive, Verify, Validate, Respond) provides a framework for DSR handling
- Cloud-native solutions can reduce processing costs to under $10 per request
- Processing time can be reduced from days/weeks to minutes or seconds
- Clear handoffs between humans and automation are crucial for success
- Avoid over-engineering and implement observability from day one
- Start with one request type (like access or delete requests) and expand gradually
- Use familiar AWS tools and services rather than introducing new technologies
- Treat the DSR automation solution as a first-class application

## Technical Details
- AWS Services utilized:
  - API Gateway for request intake
  - Lambda functions for processing
  - Step Functions for orchestration
  - DynamoDB for status tracking
  - EventBridge for scheduled checks
  - SNS for notifications
  - S3 for secure storage
  - SES for secure delivery
- Implementation features:
  - Pre-signed URLs for time-boxed access
  - Restrictive IAM policies for access control
  - Built-in retries and timeouts through Step Functions
  - CloudTrail for audit logging
  - Integration with notification systems (Slack, PagerDuty)
- Architecture components:
  - Event-driven orchestration
  - Centralized logging and monitoring
  - Automated verification processes
  - Standardized components for different request types
  - Unit testing capabilities for individual functions

## Full Transcript

All right. If you are tired of running manual queries to fulfill your data privacy request, you are in the right place. Uh, my name is Guillermo Fisher. I am an AWS data hero, uh, and a fractional CTO and CISO, and I help companies solve engineering productivity, security and compliance problems, and today we're gonna talk about, uh, modernizing privacy privacy compliance work flows in AWS. So, uh, just to talk through you through what we're gonna be going through today, we're gonna first talk about trends and data subject requests or DSRs, uh, why manual compliance is a suboptimal long term solution, uh, gonna talk about what a cloud native compliance solution looks like, uh, walk through some lessons learned the lessons that I've learned in the field, and then I'm gonna leave you all with a practical blueprint that you can use to get started as soon as you get out of this conference. Last year a little over 25% of all Internet users uh filed the DSR so and that makes a lot of sense when you think about, you know, GDPR and CCPA and other states rolling out, you know, similar privacy laws, so you know they're everywhere, uh, and they're increasing by about 30% year over year and we're gonna continue to expect that kind of an increase, uh, and. Expensive as well. So if you consider, uh, uh, companies that deal with around a million identities, uh, spend a little bit less than $900,000 a year actually manually processing those data subject requests and on average it costs about $1500 to manually process a DSR. So if you to bring that to light a little bit to life a little bit. Uh, I was working at a company. I was brought on to be the head of infrastructure, and one of the things I was responsible for doing was, was defining this DSR process. Uh, within my 1st 90 days, the first security, uh, sorry, privacy request came in. There was no process, no work flow. It might have been a form somewhere, uh, so between meetings with legal, meetings with engineering and security, uh, getting on Zoom, emails, etc. etc. it took. Meetings with 6 very senior people, uh, 40 hours of work, and we barely, uh, file the, uh, fulfill the request in time, so pretty expensive, pretty time consuming and pretty stressful. Thank Doing all this manually might be OK for a smaller organization where you aren't getting that many requests, but it doesn't scale very well. Uh, intake can come in through or these requests can come in through, uh, forms, through emails that get sent to, you know, different people within the organization, uh, through ticketing systems, uh, for something like Zendesk or through Jira or whatever it is that you're using. Uh, the handoffs that you are, uh, executing while you're trying to fulfill these requests can be undocumented. Things might slip through the cracks pretty easily. SLA deadlines if they are tracked at all, they might be missed or they're not communicated to people that need to be aware of how much time they might have to actually fulfill the request and again legal security engineering are spending a lot of time working on this stuff instead of doing some more you know potentially innovative work or doing the kind of things that that you want them to be doing. I started to round these conversations with this really simple model of RVVR, uh, that's receive, verify, validate and respond. So in the receive step you're obviously intaking the request, you're logging it, making sure that the clock starts on that request, uh, the verify you're actually making sure that the requester is who they say they are. Uh, with the validate step you're making sure that this request is applicable to you and your business and that is an actual legal request and then on the, the response side you're either, you know, sending the information to the person if it's an access request, deleting the information if it's a delete request, etc. etc. so this is pretty simple enough to talk about with, you know, somebody who is not technical but great enough to capture all of the essential pieces that an auditor would be looking for when you're communicating about the process to them. Here's an example of an actual manual process that was in place at a place where I worked, so the Requests came in through Zendesk. There was a ticketing system in in Zendex, so people submitted uh customer support requests there, and then the data was distributed across RDS and we had a data warehouse as well. So as we were verifying the request, we had to ask the user to to provide some information and then check throughout these two different data stores to make sure that that information was there. And then there was a lot of communication to validate the request. So in this particular instance legal was outsourced and so there was a lot of Zoom meetings and and Slack conversations internally to make sure that this request was in within the scope of what we needed to handle and then once again going through different data stores we had again RD. for our transactional stuff warehouse was in Redshift and then we had to deliver the the access request in uh Dropbox. So we used, uh, uh, we packaged up CSVs, put them into Dropbox, and then, uh, popped the link into Zendesk and then to that into that ticket so that the requester could have access to that data. So this worked, this definitely got the job done, but again it took a long time. Uh, on average it took around 7 days for us to fulfill these requests and if somebody was on PTO who was a necessary part of of the work flow, it could take upwards of 2 weeks. So not only is this expensive because doing it manually expensive because it takes up a lot of time, but getting it wrong can be incredibly expensive. In March of this year, the CPPA fined an automaker for not handling this. Stuff very well they had poor consent work flows, uh, bad third party data sharing practices, and that isn't a reflection of their intention, right? It wasn't that they were trying to get this stuff wrong it just they didn't do a good job of managing this process and so this fine is real, this just happened this year, and this is something that you need to be aware of so that you know how serious it is to get this stuff wrong. So here's an example of a a workflow that is cloud native. So, uh, we've got a bunch of different services here and we're gonna dive into each of these different pieces in a bit, but, uh, on the receive side we've got API gateway, Amazon API gateway kind of taking in the requests, uh, we've got, uh, verification happening through. Uh, the different, uh, through, through lambda functions that are managed and orchestrated through, uh, uh, step functions we've got Validate also happening, uh, through, uh, lambda functions that are called through through step functions and then the respond step, uh, we're, we're compiling data into a structured format. We are delivering it to the user through SES, um, and storing the data in S3, uh, so all of these different RVVVR steps are being managed through a much more automated, much cleaner, much faster, uh, cloud, uh, native solution. First step we're gonna take a look at is orchestration and logic. Why step functions? Because step functions support out of the box retries, time outs. They also support any manual, uh, human intervention if, if that's needed, if something goes wrong, uh, and then lambda functions are pretty ubiquitous in general, but you can unit test or or or uh. Yeah, you can test each of these different pieces, um, individually, you can swap out different pieces for different kinds of requests, and it's, it's pretty easy for for folks to to upgrade, refactor any of these if if needed. Notifications and tracking. So this is a very important, uh, piece of the whole solution. You wanna make sure that you are keeping track of the status of each of these, uh, steps in the process, um, in some kind of a solution. Dynamo Dynamo DB is a great solution for this. You can track, uh. You know, when the the uh the request comes in, how different pieces of the process are being managed, how quickly they're being managed, and then you can use a solution like EventBridge to run frequent checks to make sure that you're staying on top of it. So if you have, for instance, less than 45 days left and for some reason the the uh request has. been fulfilled you can actually schedule a check and then have that check trigger a notification using SNS Amazon simple notification service to hit up uh Slack or pager duty or send an email to let you know that there's something wrong and that you are potentially in danger of not being able to fulfill this request in time. Again, storage and and uh secure delivery, so using S3 is great for storing this stuff, uh, use, you know, restrictive IM policies to make sure the right people have access to the data. Uh, and then use SES to deliver a pre-signed a URL so that the requester gets access to it. You can time box access to that. So you know, if for instance if 24 or 48 hours pass, um, and they still haven't clicked on the link, then the link is no good anymore. They'll have to reach back out to you, but. SES and and simple uh sorry, and S3 are are great candidates for for storage and delivery. Let's talk about why this is better in terms of a few different dimensions. So, uh, cost, of course, first and foremost, uh, we already discussed that it's about $1500 to manually process a DSR with this solution, with this kind of a solution. Less than $10 per request and that factors in the dev time up front that factors in time to, you know, refactor things uh and maintain the solution down the line on average about $10 a request, which is uh a steep decrease definitely worth it to to investigate and see if you can. Produce your own cloud native solution. Time to fulfill goes from days and weeks, like I said before, it could be 7 days, 2 weeks, potentially months, depending on the complexity of your data, uh, 2 minutes and hours or seconds even if you, you know, the data allows for that. Your risk here, uh, in a manual workflow, like I said, things can potentially drop, uh, get dropped, slip through the cracks, so pretty high risk with uh things more controlled and, and logging in place and uh auditing in place. uh, it's a much more controlled for the cloud native solution. And then auditability we we already talked about that having everything centralized in Dynamo DB is a is a great benefit so that when it comes to auditing audit time you already have stuff that you can present, uh, in, in, uh, Dynamo DB and potentially in Codtrail as well so that access to different things is also, uh, tracked. No Where I've seen teams go wrong in trying to implement these cognitive solutions is in 3 different areas in particular and just to go outside of the scope of this talk, I think these 3 can be applied to any project where you're doing any kind of automation. Uh, so first and foremost, unclear handoffs between humans and automation. If you have to get involved as a human being and, and make a change or support something, there needs to be a pretty clear process that lays out what needs to happen as a result of your interaction, what happens next? Do you have to talk to somebody? Do you have to push a button that needs to be laid out so that the process isn't uh unnecessarily uh uh haltered. Don't over engineer the thing could you potentially build an LLM and uh have it fine tuned to your data and your schema? Sure, you could, but it could take much longer to do that. You don't wanna tie, for instance, the use of new new technologies to a project like this because you could potentially fail and. Not be able to use those technologies anymore if your company sees the the the lack of success for that thing and ties it to for instance lambda functions if you haven't used them before or um you know, terraform if you haven't used that before you wanna don't try to boil the ocean, don't try to over engineer just do what needs to be done for that project. And then of course you want to build observability into it from day one, as early as possible. Do not treat it as an afterthought, what what you can measure, you can manage, and so you wanna make sure that you're measuring as much as possible, as early as possible. To get started you wanna go with one of your uh request types you wanna map that existing process, you know, lay it all out, make sure you understand what's what's involved. You wanna identify any pain points and risks that exist within that specific work flow. You want to choose one again, uh, uh, access requests are a great place to start. Delete requests are also pretty simple and a good place to start. And then you wanna define what success looks like so how long will it take us on average to to do this manually? What does it look like if we do this, uh, using a cloud native solution, uh, if, if that response time is, you know, days, then make sure you have a clear understanding of how many days, uh, or how many hours or how many seconds, uh, whatever it is, make sure that's clear and communicate it out to folks, make sure you have an understanding of what cost savings you wanna have and what the SLA is that you wanna have in place. Right, automate one core workflow, uh, use event driven orchestration if possible use services that you're familiar with again, you don't wanna try to to to use this as an excuse to use some other service that you haven't used before. You wanna be successful here, you wanna get this done quickly, so use something that you know and you wanna make it testable, um, so that you can test different parts of it to make sure it works and again be observable as early as possible. And once you've done, once you're done automating all that, uh, you wanna start adding alerts, of course, and more metrics, more dashboards, etc. you want to start to build in, build out more, uh, DSR types. So if you started with access, move on to delete and you know, opt out, whatever the different uh request types are that you need to support and then you wanna make sure that as different people get involved and, uh, you start to build out for different uh request types that you're standardizing different components of this solution. Uh, and then you wanna also review it and treat it like a first class uh application. Don't treat this like some, uh, one-off that exists somewhere else. This is a part, now a part of your production, uh, workload, and you wanna treat it as such. Start small, automate 11 request type well. Use AWS native tools that you already know very well, uh, build login and metrics early and then expand with confidence, and that's, that's basically how you're gonna get started and, uh, that's all I have for you all. So, um, that's my QR code. It's safe. This is me on LinkedIn, so just use that if you want, uh, to get in touch with me. I'll be over here to answer questions if you want, and please do fill out the uh. The uh survey at the, uh, for this session and that's it thank you very much.
