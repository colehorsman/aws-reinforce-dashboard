# AWS re:Inforce 2025 - Establishing a data perimeter on AWS, featuring Block, Inc. (IAM305)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=9NMOBobLpY0)

## Video Information
- **Author:** AWS Events
- **Duration:** 51.3 minutes
- **Word Count:** 8,436 words
- **Publish Date:** 20250620
- **Video ID:** 9NMOBobLpY0

## Summary
This AWS re:Inforce 2025 session, delivered by AWS Security Specialist Ashraf Mue Deni and Block's Cloud Security Engineer Megaindal, focuses on establishing and implementing data perimeter controls in AWS environments. The session aims to help organizations prevent unintended data access and disclosure through comprehensive security measures.

The presentation explains how data perimeter controls extend beyond traditional network-level security to include permission guardrails that restrict who can access data and from where. The speakers emphasize that data perimeter is not a specific AWS service but rather a framework utilizing various authorization policies such as Service Control Policies (SCPs), Resource Control Policies (RCPs), and VPC endpoint policies to create comprehensive security boundaries.

The session outlines three main types of data perimeter controls: identity perimeter controls (preventing untrusted identity access), network perimeter controls (preventing access from unexpected networks), and resource perimeter controls (preventing access to untrusted resources). The speakers provide practical examples and implementation guidance, including specific policy configurations and real-world applications at Block Inc.

## Key Points
- Data perimeter extends beyond traditional network controls to include permission guardrails
- Three core components: identity perimeter, network perimeter, and resource perimeter controls
- Data perimeter controls are coarse-grained and complement principle of least privilege
- Controls focus on trusted identities, expected networks, and trusted resources
- Implementation uses various policy types (SCPs, RCPs, VPC endpoint policies)
- Boundaries must be clearly defined before implementing controls
- Controls can prevent both unauthorized access and unintended data disclosure
- Policies should be tested and adapted to specific organizational contexts
- Framework is not a single AWS service but a combination of various security controls

## Technical Details
- Uses Service Control Policies (SCPs) for organization-wide access control
- Resource Control Policies (RCPs) for resource-level access management
- VPC endpoint policies for network access control
- AWS Principal Organization ID conditions for identity verification
- AWS Resource Organization ID for resource ownership verification
- Condition keys like AWS:PrincipalIsAWSService for service principal validation
- Implementation through deny-based policy statements
- Integration with existing AWS services like CloudTrail and AWS Glue
- Utilizes AWS Organizations for policy enforcement
- VPC endpoints for private AWS service access

## Full Transcript

OK, welcome everyone to our session establishing a data perimeter. During this session we are going to learn about data perimeter controls and how you can use them to mitigate security risks such as unintended access to your data and data disclosure. My name is Ashraf Mue Deni, and I'm a security specialist within AWS. I will deliver this session alongside Megaindal, cloud security engineer at Block. So we will first start by defining what the data parameter is and then we'll give you some examples of data parameter controls. Just after that, Mega will provide insights on how block implemented their data parameter controls. So let's start first by defining what the data perimeter is. So previously on your corporate on premises data centers, you would typically enforce your boundaries and security perimeters using network segmentation controls and for example, firewalls. And while you implement or while you build your application on the AWS cloud or you migrate them to the AWS cloud, you would typically keep using such types of network level controls. So for example, you would use security group to restrict inbound and outbound traffic at the level of your elastic network interface. You would use AWSWOFF to inspect your HTTPS and web traffic or AWS network for your role to restrict east and west traffic at the level of your organization. If your security policies has requirements that restrict you to access AWS service endpoints privately, you will also deploy VPC endpoints. So for example, in the diagram that you have on the screen, your Amazon EC2 instance can then access your Amazon S3 bucket through your VPC endpoint without the need to cross the internet. And while these while these network level controls are fundamental, a comprehensive data perimeter strategy extends beyond this infrastructure layer controls to focus on establishing permission guard rails that help you restrict who can access your data and from where. So if We go back to the objective of preventing unintended access to your data, you would typically like to help ensure that, for example, the data residing on your Amazon S3 bucket cannot be accessed by an external identity that you don't trust and don't belong to your organization. Similarly, you would want also to make sure that one of your identity cannot disclose data to an external bucket that is not owned neither by your organization and at its core, building a data perimeter is actually defining these boundaries between the things that belong to you and the things that do not belong to you. From a technical point of view, a data perimeter is a set of permission guardrails that you can use to help ensure that only your trusted identities can access your trusted resources from your expected networks. So the key takeaway is that data perimeter controls are permission guardrails. So it means that first of all data perimeter is not an AWS service per se. It's rather a framework that uses different types of authorization policies such as service control policies, resource control policies, or VPC endpoint policies that you would use to meet your security objectives. The other thing is that permission guards fits within your overall access management strategy. So for example, in the security field there is this well known principle which is a principle of this privilege where you would implement fine grain permissions. So you would typically start by setting your permission, then verifying if the permissions are right size or not, and when required, you would refine them. So for example you would say this specific identity can only access this specific resource with only a specific set of actions. On the other hand you have data perimeter controls which are coarse grained by nature. They apply to a large set of accounts and what would typically you would typically say that for example your resources can only be accessed by your trusted identities of course provided that you define what is a trusted identity and we are going to review that in the upcoming minutes together. So what would happen is that actually both principles, the principle of this privilege and data perimeter concepts are complementary. You would typically have your central team enforcing coarse grain controls that will act as always on security environments which are data perimeter controls, and while you have your developers iterating on closing the gap with the principle of this privilege and refining the permission accordingly. So let's now review how data perimeter controls will affect your data access path. So you have 3 types of data perimeter controls. You have identity perimeter controls that will prevent calls by untrusted identities. You have network perimeter controls that prevent calls from unexpected networks, and finally resource perimeter control that help you prevent calls to untrusted resources. Then if we need to review the different dimensions, API calls can either be performed by trusted identities or untrusted identities from expected networks or unexpected networks, and finally they can target either a trusted resource or an untrusted resource. So let's start first by focusing on API calls against trusted resources. Your primary goal there is to help ensure to prevent unintended access to your data, so. In such a situation, the first API call should not be denied, as you may have guessed. It's performed by one of your trusted identities from your expected networks. The second API call is performed by untrusted identities and should be denied by your identity perimeter controls. The 3rd API call here in this situation is performed from unexpected networks and should be denied by your network perimeter controls. And finally, if for the last API call performed by untrusted identity, it should as well be denied first by your identity perimeter controls. So now if we switch the target and we focus on API codes that are made against. Untrusted resources where your primary goal is to help ensure to prevent unintended data disclosure. The first API goal is performed against an untrusted resource and we should expect your resource perimeter controls to help help you prevent this APA go from happening. The second API call is performed by untrusted identities. So here your identity perimeter control should help you prevent this API call. However, one point to note here is that actually you are going to implement identity perimeter controls at the level of your network, helping you ensure that even your networks can only be used by your trusted identities. And finally for the 3rd API code, it's exactly the same as the previous example. It would be denied by our network perimeter controls. So as we have seen together. Your each type of data perimeter controls will help you implement permissions. Identity perimeter controls focus on the who of the access path and help you ensure that your resources can only be accessed by your trusted identities. Network perimeter controls focus on the where of the access path and help you ensure that your resources are only used from expected networks. And finally resource perimeter controls focus on the what of the access path and help you ensure that your identity, your resources, your identity, are only accessed, um, can only access your trusted resources. So the combination of trusted identities, expected networks, and trusted resources is what defines your boundaries. As soon as you define your boundaries, you can then design your data perimeter controls and enforce them. So let's review together how you can build your data perimeter controls. So to build your data perimeter controls, you can use different types of authorization policies such as service control policies or SCPs that will help you enforce access control on your identities. Resource control policies or RCPs that help you enforce access control on your resources and finally VPC endpoint policies that help you enforce access controls on your networks. Together during this session we are going to review 3 examples of data parameter controls. You have more examples on the data parameters on AWS web page. You have a QR code on the screen that is linking to this page. Please feel free to bookmark it. It's one of the key resources for this session. You will be able to find white papers, more examples of controls, um, GitHub repositories, tools, and customer success stories that will showcase how you can build and implement data parameter controls. The other point to note is that we are going to review together some examples of policies. Those are only samples that we are going to, of course, simplify for the sake of this presentation. You will still need to test them, adapt them to your context. However, they will provide you with a strong foundation to kick off your data perimeter life cycle. So let's start first with the first example, which is the objective is to prevent unintended access to your data. So from a security objective point of view. Your objective is to help ensure that your resources can only be accessed by your trusted identities. So we are talking about an identity perimeter control. So first, before designing our data perimeter controls, we need to define what we mean by trusted identities. So for the sake of this example, let's say that a trusted identity, it's either a principle that belongs to your organization or an AWS service principle that is acting on your behalf. So if we review that, you know, just with a quick diagram to set a bit the stage, it means that if your corporate credentials are trying to access one of your S3 buckets, this API code should not be denied by your data parameter controls. However, if you have non-corporate credentials trying to access one of your S3 buckets, this API code should be denied by. Identity perimeter controls and you would typically enforce this permission boundary using RCPs or control policies because the target resource belongs to your organization and is governed by resource control policies. On the other hand, non-corporate credentials that are outside of your organization are not governed by your service control policies. And finally, if you have a service principle such as cloudtrail. Amazon AWS.com trying to access one of your SG buckets, so typically cloud. Amazon AWS.com is owned by AWS Cloud Trail and is used by Cloudil Trail. To send logs to your bucket, so we can also restrict this access path to help ensure that this service principle is only acting on behalf of a trial that belongs to your organization, and we are going to review that with an example in 1 2nd. So let's review a sample of SCP that will help you meet your security objective. So first of all, we are going to start with a with an effect equals to deny. Why? Because we want to enforce a preventive control. Then data perimeters are core coarse grained by nature. So this would apply to any principle, any resource, and for the sake of this example, any S3 action. We have two types of principles that are part of our boundaries either principles that belong to your organization, so those are non-service, non-service principles, and actually AWS service principles. So to denote that and to reflect that in our statement, we are going to use the condition key. AWS principle is AWS service, which is said to be true if the APA code is performed by an AWS service principle. The first statement will focus on APA calls that actually are performed by non-service principals and as you may have guessed, we need a second one which should focus on service principle and here the value would be set to true. Then you can use the condition key AWS principal org ID which contains the ID of the organization to which the requester belongs to, and we can compare that using the operator string not equals if exists to actually your corporate organization ID, which means that if we stay like that, the first statement helps you ensure that if an APA call is performed by a non. Service principle and by a principle that does not belong to your organization, this API call would be denied. We need then to focus also on the second use case and API calls that are performed by service principles. So for this situation we are going to use the condition key AWS source or ID which contains the ID of the organization to which the initial resources that are that use the AWS principle belongs to. So for example, if we take again, you know, AWS cloud trail, it will be the organization ID of the trail trying to send logs, you know, to one of your buckets. So with with both these statements we can help ensure that only your trusted identities as we have defined them, you know, when we set our boundaries can actually access your resources. Let's now review the second example which aims to prevent unintended data disclosure. So in this example, only your identities can access your trusted resources. So it's a resource perimeter control and we need first to start by defining what we mean by trusted resources so we need to set our boundaries for the sake of this example let's say that a trusted resource, it's either a resource that belongs to your organization or a resource that is owned by an AWS service so. Knowing that we have these boundaries, let's review some data access path. It means that if your corporate credentials are trying to access one of your S3 buckets, this APA goal should not be denied by your resource perimeter controls. However, if your corporate credentials are trying to access an SG bucket that is not owned by your organization and that you do not trust, you should expect your resource perimeter controls to help you prevent this API call from happening. To enforce this permission, you can use SCPs or service control policies. Why? Because service control policies apply to your identities, and here the IPA goal is performed by one of your corporate credentials. The target resource is not owned by your organization and is not governed neither by your resource control policies. Similarly, if your corporate credentials are trying to access an S3 bucket that is owned by an AWS service, you should expect your perimeter controls to not deny this API code. So for example, AWS Glue Studio is storing some artifacts in S3 buckets that are owned by Glu Studio and documented by Glu Studio. So in this in such situation, if you rely on Glu Studio, you shouldn't expect your resource perimeter controls to deny this API code. So let's give you an example of an SCP to meet the security objective. So we are taking exactly the same structure as the previous example with an effect set to deny any action, any resource. Then we can use the global condition key AWS resource org ID which contains the ID of the organization of the target resource and you can compare that to your corporate organization ID. Using the operator string not equals if it exists, it means that if we stay as such, this statement will help you ensure that if one of your identities to which of course this SCP applies to, it's trying you know to access a resource here, for example, a bucket or a necessary object that does not belong to your organization, this API call would be denied. Finally, we need also to manage a situation where the resource is actually owned by an AWS service. So if we take back the example of AWS Glu Studio, AWS Glu Studio is documenting the list of S3 buckets that you need to grant access to. They also document the list of account ID to which each one of these buckets belongs to. And you can use then the global condition TAWS resource account which contains the account ID of the target resource and exactly like previously compare that to the account ID provided by the Glu Studio documentation. You have other types of examples to manage such situations in our Git repositories and data parameters on AWS web page that you can check out after the session. So let's review our last example on how you can prevent use of corporate credentials outside corporate environments. So here in this situation you want to help ensure that your identities can only be used from your expected networks. So we are talking about a network perimeter control. Exactly like our previous examples, we need first to start by setting our boundaries. So for the sake of this example, let's say that your expected networks is either corporate on premises networks or corporate AWS networks. So if we review again some examples of data access pass to illustrate our boundaries, it means that if one of your cloud engineers is trying to access one of your Amazon S3 buckets from your corporate on premises network, this API code should not be denied. Similarly, if corporate credentials are used from one of your VPC, this API code should not be denied neither. However, if your corporate credentials are used from an unexpected location, this API code should be denied by your network perimeter controls. So we are going to review together how you can enforce these boundaries using an SCP. So similarly to the previous example, since you want to control the access that is managed by one of your identities, you can use SCPs to meet your security objective. Again, we are going to use exactly the same structure as previously with an effect to deny any action, any resource. So we are using the wildcard, for example, for the resource policy field. Then our boundaries we have API codes that are performed from corporate on premises networks or corporate AWS networks and we need also to reflect that in our policy. So the first condition key that we can use for this is the global condition key AWS source IP which contains the IP of the requesters in in situations where actually the target, the request has been performed against the public service endpoints. And you can compare that to your corporate CID ranges using the operators, not IP address if it exists. So this first part is managing, you know, the access path that is done through AWS public service endpoints. We also need to manage API calls that are performed through VPC endpoints. So to do that we can use the global condition key AWS source VPC which contains the VPC ID on which the VPC endpoint has been deployed, and you can then. Make sure that you compare that to your corporate VPC ID. Finally, there is another type of access pad that is usually existing is that we have situations where an AWS service will actually forward your session to another service. So for example, if you try to access an S3 object that has been encrypted using an AWS KMS key, um, you will actually Amazon S3 will forward your credentials to AWS KMS to perform, you know, cryptographic operations. And while doing that from a network point of view, when Amazon S3 is forwarding your session to AWS KMS, it's from a network point of view it's doing it from the network owned by Amazon S3. We are calling this Access path forward access session, and we need also to make sure that we are not preventing this access path which is expected. So to do that we can use the global condition key AWS via AWS service, which is set to true in situations where forward access session is actually used. And finally, we also wanted to show you an example on how you can toggle on off your controls when required or if you want also to make sure that your controls can only be applied where it matters, you can use the the global condition key AWS principle tag which contains, you know, the tag of your principles. Here for example, we are using the tag key DP include network, so DP stands for data perimeter and the value true. It means that if a principal has actually the tag key DP include network set to the value true, it means that this statement would apply. So you can help ensure that your data perimeter controls are only enforced for principles where it matters. So for example, me, we share an example in a few minutes where block implemented their data parameters only on rules that are sensitive. Another common example is, you know, you can use network perimeter controls for all the roles that are used by your workforce more generally, or even rules that are used from one of your VPCs, for example. So as we have seen together, data parameter controls can be a part of your network of our security strategy and security framework. So the key benefits uh that usually customers are sharing with us, one of the first benefits that they are sharing with us is that data perimeter controls help them meet their compliance requirements. At scale, so here typically they would mention frameworks or compliance frameworks that are related, you know, to data governance movement of data within and outside of the organization or data protection uh controls. The other key benefit is that data parameter help you enforce all ways on security invariants at scale, as we have seen data parameter controls are coarse grain by nature and can be applied across a large set of accounts during the example that we have reviewed together. We have seen that data perimeter controls can help you prevent unintended access to your data, but also unintended data disclosure, so they help you meet your data loss prevention objective. During the last example that we have reviewed, data parameter controls can also help you, uh, we saw that the parameter controls can also help you prevent misuse of your corporate credentials. And finally, One other example that that customers share with us is that data perimeter controls can help you segment access between environments with varying data access requirements. So for example, you can use data parameter controls to segment between production and non-production environments. In our first examples we have for example said that. A trusted identity or even a trusted resource is a resource that belongs, you know, to our organization we could also have said that actually a trusted resource or a trusted identity for a production account, for example, is a resource or an identity that belongs, you know, to the same organizational units. So that's something that uh some customers also are are doing and using with data parameter controls. Data perimeter controls are also and help you meet your business objectives, so they are a business enabler. One of the first key objectives that is shared by our customers is that data parameter controls help them accelerate service adoption and the usage of the latest features. So some organizations have a process, a governance process where they would review the security and compliance objectives of services or features before enabling them. And by having this always insecurity environment control which are data parameter controls in place, they can actually accelerate this process and make sure that they can provide new services, new features, new features faster to their customers and developers. The other key benefit that is shared is that data parameter controls helps provide you know protected environment to your developers where they can actually, you know, iterate, potentially fail if required, but still they would have this always on security variants in place that would make sure that your boundaries are enforced. And finally Data parameter controls also help you host all types of data classification types. So for example, some customers and media will provide an example with block in a few minutes are using data perimeter controls first on their most sensitive data. But some other customers are also using data parameter controls to help ensure and to, you know, broader the scope of the types of data that they can actually host within the AWS cloud. So as we have seen together during the first part of this session, we have defined what a data parameter is. We reviewed some examples of data parameter controls and we took a step back to also assess what are the benefits, the overall benefits of data parameter controls. So you may wonder how to kick off your data perimeter journey. So to do that, you can start first by examining. Your security objectives, so exactly like any security initiative, you need first to start by defining what are the key security risks that you want to address. Should you start with misuse of your corporate credentials or data disclosure or unintended access to your data, then you can set your boundaries exactly like we did actually during the examples that we reviewed together. So you need to define for your context and for your organization what a trusted identity is, a trusted resource and an expected network. You can then design your data perimeter controls. So for that you have a great accelerator which is the data perimeter policy example Git repositories. You have the link on the screen to the GitHub repository. On this GitHub you will find samples of policies that you can use. Of course you need to test them and adapt them to your context, but they will provide you with a strong foundation. Plus just last week we released service specific guidance which actually go into the depth of each service that we are covering for now on how you can implement data parameter control for this specific services. Then, since we are talking about uh security controls before enforcing them, you need to anticipate potential impacts. So to do that you can use AWS IMes Analyzer external access Analyzer, which will provide you with a view on. Which types of external principles can access your resources. You can also analyze your account activity, for example, using AWS Cloud Trail and use Amazon Athena queries, you know, to query your logs. We have samples also of Amazon Athena queries on our Git repositories so you can check them out later after this session. Then you can finally implement your data perimeter controls. um, so to do that you need of course to follow your DevOps, best practices and to make sure that you put the required level of automation to put in place your controls. And finally you can monitor your data parameter controls. This will help you ensure first of all that they are working as expected, but also they will help you ensure that this will help you ensure that your controls are continuously aligned with your security. And business requirements while helping you ensure that you have continuous improvement as well. So without further ado, I will let the mic to Mega who will provide you actually insights on how Blo implemented these data parameter controls at scale with success within the organization. Thank you for your attention. Thank you, Ashraf. Uh, before I start, can you guys hear me? Is the mic OK? Perfect. Awesome. So, hi, I'm Mega Jindal. I am a cloud security engineer at Block, and today I'll be walking you through a journey of implementing data perimeters. Let me first start by introducing Block for those of you who don't know. Block's mission is to empower people to participate in the global economy, whether that's a small business owner accepting payments, whether it's an artist distributing its music, or maybe just someone managing finances from their phone. What makes us unique is how we operate. We are like a company of companies, a federation of teams like Square, Cash App, Tidal, Afterpay. Each team has its own mission, culture and way of building, which allows innovation to thrive. These decentralized models help teams to build what's best for their users while still staying connected by a shared purpose, but that brings unique challenges for us as security. We need to design controls which are scalable and able to adopt across all these different operating models, these tech stacks, these workflows without slowing down innovation. So our job essentially is to accelerate business, not block it, no pun intended there, but we want to still keep security as the focus. So when we started evolving, we realized that to truly support this decentralized model, we wanted to build a secure cloud foundation. Our goal was to unlock the agility and scale of public cloud. We envisioned a model where developers could spin up infrastructure in minutes, and they don't have to wait weeks. Our principles guiding this journey were very clear. We wanted rapid deployment and experimentation, so teams could iterate very quickly. We wanted developer autonomy to self-service, so they are not having bottlenecks or dependencies, and we wanted flexible on demand scaling, so infrastructure could match the pace of our growth. Now to turn this cloud vision to life, uh, what we did was we translated those high level principles into architectural decisions. What we call services internally at block, we refer to them as apps, and they're deployed across multiple cloud providers, pretty much showing the diversity of our teams. Um, what we do is we adopt the one account per app per environment model. Uh, Essentially what it does is every app gets its own AWS account and for each environment like dev, staging, production, everyone has its own AWS account. These accounts are then connected via regional shared VPCs to enforce isolation. What does this architecture do? It helps us strict workload isolation. It minimizes blast radius in case of issues and also it helps us in boundaries in case of ownership and governance. So this model helps us to scale quickly while also maintaining fine-grained access controls. Now let's talk about how our platform matured and then what we enabled through it. So as we were maturing, we realized we wanted to have the velocity and control to coexist. What we enabled was we enabled faster iterations through self-service. We enabled quickly deploying resources, we enabled first AWS catalog and unrestricted innovation. The impact of that was we were able to onboard teams and developers very quickly. We were having accelerated experimentation and delivery through self-service, and most importantly, we got developer confidence. But that also brought operational complexity and unmanaged risks. Now once we uh had the secure foundations in place, we wanted to move on to the next phase. How do we build the right things and ship them fast? The intent here was not just moving quickly but also building with intent. We wanted to be secure by default. How do we do that? We maintain. while embedding security and compliance into the developer workflow. We provide platform APIs and paved paths to accelerate delivery and improve developer experience. We shift left by having pre-deployment validations, automated checks to enable sustainable scaling. What this does is it provides a platform secure, and it keeps it locked down. Um, uh, when we look about trusting its scale, it's critical for us to define what the access boundaries are. What should we allow and what should we disallow? At block, uh, this slide will illustrate what the trusted boundaries for us are. We allow AWS services and partner systems to communicate with us through secure and authenticated parts. What we disallow is unrestricted cross-region interactions because that could cause data leakage, it could cause privilege escalations. We also don't allow direct external uh access because uh unless it comes from very secured and monitored gateways, we disallow anonymous or unauthenticated calls because we only want to grant uh access after the identity is verified. The environment this way is locked down to support uh collaboration between teams and partners, but also it blocks any unauthenticated or risky patterns. So I just talked about what data perimeter is conceptually. Let's look at how it takes shape within blocks infrastructure. Trusted identities for us are block principles, our partner resources, and our service principles, AW service principles acting on our behalf. Expected networks would be for us like desired network boundaries, our shared regional VPCs, our VPN networks, our secure partner networks. Trusted resources are resources that lie inside blocks controlled AWS accounts, our partner resources, and some AWS resources. Together these three layers form the foundation of our data perimeter, ensuring trusted identities over expected networks can interact with trusted resources. Uh, now before we dive into specifics, it's important to level set on how we think about access. At block access isn't just about who you are. It's also about where and how you're trying to access. We intentionally designed these access patterns to create secure authenticated parts so that data only flows from where it's supposed to. These access patterns apply equally to human users as well as applications. Let's, for example, let's look at a human user. An employee if it wants to get access to AWS service, it needs to go to via a trogdoor proxy, which is an access proxy. It performs some device at the station, some authorization checks, and then gives access to the service. This type of access is there for uh device aware. It's time bound and it's auditable. An application on the other hand, would go through a shared BPC to get access to an AWS service. What that ensures is a trusted identity and expected network. By separating and optimizing these two different access patterns, we're able to control, um, access is also about uh where you're accessing from and who is trying to access, um. Uh, when you try to secure our access, we want to think about uh that access needs are not one size fits all. It differs significantly between a human user and an application. A human user would be typically like a developer, an operator, or someone who's just trying to have interactive access. This type of access needs to be time bound. It needs to go through proper identity verification and has to have comprehensive auditing. How we maintain that is through having centralized authentication. We have device attestation and authorization to verify trusted endpoints, and we have compliance monitoring and logging for full visibility and accountability. And application access on the other hand, is mostly automated. It is like third party interactions or API integrations. It doesn't need to be interactive, but we need to have it scoped, ephemeral, and secured. Now how we do that is through direct secure connection. We do network segmentation to tightly control these access parts and also we do resource-based policies to define these boundaries. Now what that does is it makes sure that um. Our access, what we are coming from, it's not coming from external accounts which we don't allow. OK, so, uh, let's dive into how we secure these both different types of access. Now securing an application access in a centralized environment requires multiple layer of controls working together. At block we leverage three key mechanisms to ensure secure communication between apps and services. Uh, first is VPC endpoint policies. Uh, these policies control what services and what actions are allowed within the VPC. It also ensures that the traffic between apps and services follow expected network and they follow least privileged path. Next is service control policies. They are a great way for us to centrally apply at the organization level and control what access the service account can have even though whatever permission is given to the service account doesn't matter. So it helps us enforce compliance across all accounts hosting apps. Now next comes resource-based policy. These are applied directly to the resources like S3, Lambda, SQSQs. What it does is it gives fine-grained permission to which principles those resources are allowed to access from. What this shows is that we have a defense in depth model which minimizes risk and supports safe automation. Now this is an example of a VPC endpoint policy and what it's doing is it's going to be enforcing two things. It's going to be enforcing identity perimeter and a resource perimeter. Um, how we do that is, uh, we use principal or ID, which pretty much specifies that block principles only can access this endpoint. So if a request is coming from outside network, even though it's within the VPC, access would be denied unless the identity belongs to block AWS org. Next thing we are doing is resource or ID. What we want to make sure is that if a trusted entity is trying to access a resource, that resource should also belong to blocks org. Uh, now what it does is, um, there are obviously cases where we have to allow different type of access also, but this ensures that our model is locked down. Uh, let's take a look at an action, uh, at a request in action. Uh, let's say we have an AI agent which is deployed on ICO instance and it's trying to access a bucket. Uh, it could be trying to write some output data to that bucket, or it could be trying to get some data for some analysis from the bucket, but the IO instance has an IM role attached to it. The IM role decides what resources and what actions it can perform. Those permissions can also be narrowed down by service control policy if we have something enforced. Now the request routes through our network goes through a VPC endpoint. The VPC endpoint has a VP. Endpoint policy configured which decides whether the communication between the app and the resource stays within the network and is following least privilege. OK, that passed. Now we go to the resource. The resource has resource-based permissions which decides whether that principle and that action is allowed on the resource or not. So for a request to actually succeed, it needs to pass through all these three different layers and needs to be authorized to all of them. So this is how we ensure that only legitimate requests reach our data. Now we've talked about how we secure application access. Let's dive a little bit into human access. Now securing human access is critical because it represents a primary vector if left unmanaged. So like any other credentials, AW's credentials, whether they are short lived or whether they are long lived, they need to be protected against threats like phishing or unintended disclosure. The challenge here is once the credentials are compromised, they could be used from any network. What that means is the attacker doesn't need to be inside our network to actually cause any damage. So what it shows us is that authentication alone is not sufficient. We need to tie in network pedimeter protections along with identity to control where that can actually be used from. So what we do is we have divisive authentication, we have VPN networks, which is our expected networks, and we have conditional access. All these three combined together help us decide whether this credential, whether that person is who he is, whether that resource should be allowed to access from the location where that person is trying to, and how they're accessing that resource. Uh, this is an example of a service control policy which we enforce to allow this network perimeter. So here it shows one of our sensitive role admin, which is access is only allowed for that role if the request originates from our expected VPN network. That means if the credentials are valid, even if the credentials are valid, if the network doesn't originate from our expected network, access would be denied. This actually protects against in like credential misuse from uncontrolled environments. This also helps us with AI level attacks because what happens if somebody tricks the AI agent to tell a sensitive information, we still have network controls on it. So even only if the credential is coming from within the VPN network, the AI agent would be able to give the correct information. Now we've talked a lot about how we define these network perimeters. Let's talk about how we enforce these different types of parameters. Now when we were looking to enforce these perimeter controls, we faced a critical challenge. We wanted to ensure security, but we also wanted to balance it with operational safety. Applying these controls across all the accounts at the odd level could have caused some widespread outage. Like if a policy was misconfigured and we applied it at the OU level, it could have caused some essential deployments were blocked or some access parts were blocked, which we didn't want. We also thought about applying it at the account level, but without proper coordination that could have caused like some cross environment interactions to break silently without us knowing. So that could have been another critical issue. So what did we do? We created a new OU called the Secure OU inside our AWS org, and we applied all the perimeter controls on it by default. This OU became our landing zone for perimeter enforcement. All the guard rails, all the baseline protections were applied at the OU level to begin with. Then we started moving accounts with proper validation into the new OU, so we looked at. Dependencies, we looked at whether the runtime traffic was being affected. What is the access path for this. If everything seemed fine, then we moved the accounts into the new OU. What this model gave us was, it was a safe and testable rollout path for us, and we were able to scale security without causing any disruption. This model also became a blueprint for how we run any security campaigns. We want to start secure and we want to stay secure. So once we established the secure OU, we had to uh think about migrating the accounts. At that time we had around 2,000+ AWS accounts. Now we have around 4000 or 6000 more than that. But we wanted to migrate those accounts into the new OU, so we had to prioritize them somehow. How did we do that? We took a roll out strategy based on sensitivity and complexity. Let's look at the complexity first. We started with a low complexity apps first. These were new services, uh, apps which had only staging environments, uh, pretty much isolated. So we knew that the blast radius was small. We first moved these accounts into the new OU so we could validate our guard rails and also our tooling. Once that was done, we moved on to a moderate complexity. These were like they had some moderate production usage, but they did require some coordination with the teams. Finally came our high complex apps. These were our critical production workloads, legacy workloads, dependencies, stuff like that. These required end to end validation, required all the access part simulations before moving them, and often it required infrastructure changes by the teams. This model also helped teams to adapt. So instead of reactive enforcement, we kind of went to a model where teams are proactively participating with us to move their accounts. Now let's look at blogs, data sensitivity model framework to understand how we think about protecting information. We classify our accounts into our data into 4 levels of sensitivity. It helps us to apply tailored security controls at each level. Uh, a DSL one is like minimal uh sensitivity. It's like public non-confidential data where we have only baseline protections. DSL 2 is a low sensitivity internal data, which is not confidential, but we still want to protect it from accidental exposure. The SL 3 is our moderate sensitivity. It includes records like personal identifiable information and other sensitive things which need to be compliant with privacy standards. The SL-4 is a highly secure and regulated data. It's payment information, regulated personal. These have the strongest security monitoring and guardrails attached to it. So what we did was we picked apps based on a DSL 4 because we knew that applying data pedimeter to these accounts would give us the most security benefits because these are crown jewels. We want to protect them. So what we did was instead of guessing and we wanted to ensure a precision framework, we took a data driven approach. We analyzed cloud trail logs at scale. We looked for what roles were being accessed. We looked for what resources and actions were being accessed, and then we implemented all the controls on a DSL 4. So what we did was we applied stricter guard rails first there, then we monitored them in real time, the access patterns, how it's affecting and everything, and then we made iterative policy adjustments based on real-time data. That way we are able to create a model which is able to scale without causing disruption and also which is grounded in real behavioral data. Um, now we all know security initiatives don't really succeed just by enforcement. They need to be adopted. They need to be understood, and they need to be supported by the teams they actually impact. So to make our model sustainable, we invested in communication. We provided updates on what is changing, why it's changing. We created FAQs, we created internal dogs so teams could understand what these perimeter controls are, and we provided them consistent updates at each and every phase. We also invested in visibility. We created dashboards to visualize these enforcement status across all accounts. We did cloud trail driven simulations. We were also able to apply guardrails in lower environments first so teams could actually see impact before committing their high sensitive production workloads. We also work directly with engineering teams. We created dedicated Slack channels where teams could ask, probably we could help teams with troubleshooting or real-time questions. Now what that did was instead of seeing security as a blocker, we were able to enable security in the platform. Now even after putting strong guard rails in place, there could be cases which require temporary exceptions or narrow exceptions. This also helps us in a way because we don't need to anticipate all the needs. We can move quickly knowing that we have an escape hatch. So what we did was instead of having manual workflows for exceptions, we created a self-service exception mechanism. Now what that gives us is it gives us accountability. Every exception request is logged and it's tied to a business purpose. It gives us transparency. Engineers know what policy is blocking them and why it's blocking them. And it also gave control, so we were able to speed up deployments because it gave control to developers within a safe parameters. Now what this system can do is you can request scoped exceptions for specific resources, specific services. It also has a way to review for including review workflows for high risk changes. So where are we today? We've deployed uh restrictive VPC control policies to control what identities can talk to our resources like S3 Dynamo DB. We're also monitoring cloud trail to validate our guardrails to find out real world access patterns, what is happening so we can keep refining those guard rails based on the access patterns. Um, we've also created dashboards to, uh, track and alert so we can help teams to identify issues quickly and resolve them. We're also adding administrative controls, whether it's a console access or whether it's a CLI access. So we are going to protect administrative role with identity and network protections. It's going to be rolled out in a phased approach like I talked about. What this does is it enables us to move quickly, provide visibility, close gaps, and mature our controls in a way how teams are doing. So we are in alignment with the teams. So if you are just starting your data perimeter journey, or if you are scaling your data parameters, here are some key takeaways from our journey at blog, which can help you move faster and quicker. One thing is adopt a phased approach. Uh, start small, validate, and then scale. You can classify accounts based on complexity or sensitivity. Uh, next thing is define your data perimeter. Know what is, uh, trusted resources, what is expected network, what is trusted identity in your case. Plan for exceptions from day one. So build structured workflows where developers can request exceptions and stay within the guardrails. Consider resource control policies. So don't rely on SCPs alone, and complement it with BPC network policies, complement it with resource-based permission policy. We are also exploring RCPs now so we can provide these standard controls instead of resources, applying policies individually. Anticipate quotas. When we started scaling these, we hit our limits. We hit SEP limits to plan for scale. So one of the things which we came across was SEP limits. How we did that was we created an exception OU specifically, and usually we apply SEPs at the OU level, but in case of exceptions we applied at the account level. That way we could apply catered exceptions just for the account. Also engage your enterprise support. So talk to AWS partners, help them get ahead of blockers, validate your design patterns, and navigate your corner cases with them. So yeah, Now we know AI is exploding, developers are moving very fast, and we need to put controls in place. It's like a car which is already on the freeway and you're trying to lock the doors down. So what data perimeter will do, it will give you control back. It will help you. Take data from places it should not go, even if you have broad IM policies or if you could trick an AI agent to give you sensitive data, it will help you protect that. So yeah, our data perimeter journey is just getting started. We are excited to see what's ahead and pretty much that's it. Thank you. Uh, we do have a survey at the end, so appreciate if you guys can fill it, and we'll be here to answer any questions if you guys have. Thank you.
