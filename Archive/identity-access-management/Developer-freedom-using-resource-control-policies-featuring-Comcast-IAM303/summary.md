# AWS re:Inforce 2025 - Developer freedom using resource control policies, featuring Comcast (IAM303)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=RKoDpi8YO9g)

## Video Information
- **Author:** AWS Events
- **Duration:** 48.7 minutes
- **Word Count:** 8,634 words
- **Publish Date:** 20250619
- **Video ID:** RKoDpi8YO9g

## Summary
This session demonstrated how Resource Control Policies (RCPs) enable developer freedom while maintaining security at scale. Comcast shared their practical implementation experience across thousands of AWS accounts, showing how RCPs create orthogonal security controls that prevent unauthorized external access regardless of individual developer configurations. The session covered the technical foundations of cross-account IAM authorization and provided real-world deployment strategies for large organizations.

## Key Points
- **Data Perimeter Strategy**: RCPs enforce that only trusted identities access trusted resources through trusted networks
- **Orthogonal Controls**: RCPs work independently of individual bucket/resource policies, preventing configuration mistakes
- **Comcast Scale**: Implementation across thousands of accounts, 7500+ users, multiple regions and business lines
- **Phased Deployment**: Start with internal test accounts, sandbox OU, new accounts, then production in waves
- **Self-Service Process**: Automated portal for developers to request external access with approval workflows
- **IAM Authorization**: Two-round authorization process for cross-account access (caller account + resource account)
- **Confused Deputy Protection**: Ensures AWS services only act on behalf of organization accounts using source conditions
- **Service Coverage**: Currently supports S3, KMS, Secrets Manager, Systems Manager, and EBS with expansion planned

## Technical Details
- **RCP Policy Structure**: Default allow with explicit denies for external access, similar to SCP pattern
- **Condition Keys**: aws:PrincipalOrgID, aws:SourceOrgID for organization boundaries, OU path conditions for internal controls
- **Testing Considerations**: RCPs apply to external principals requiring separate test organizations or third-party validation
- **Resource vs Principal Policies**: RCPs attached to resources, orders of magnitude more resources than principals
- **IAM Access Analyzer Integration**: Identifies existing external access, archive rules for known exceptions
- **CloudTrail Analysis**: Management events included by default, data events require cost consideration
- **Deployment Limits**: 1000 policies per organization, 5 RCPs per entity, 5000 character limit
- **AWS Service Compatibility**: Not all services support confused deputy condition keys, requiring security vs operational trade-offs

## Full Transcript

We talk about how we actually deploy these things at Comcast, but I'm getting ahead of myself. Let's build a workload. OK. No matter what you're doing in AWS, it's extremely likely that there's some data in S3 that is part of what you're building. So we have an S3 bucket. Now, as you know, if you've even spent a few minutes building on AWS that a resource exists inside an account. In AWS now account doesn't necessarily isn't necessarily in a one on one relationship with customer. Rather account is you can think of it as a tenant you as a customer might have multiple, maybe many AWS accounts, but here's one that you do have account 111 and your S3 bucket is in that account. Well, of course, data isn't very interesting until you're doing something with it. So let's keep building our workload. Um, let's say there's, we'll keep it really simple. Let's run, you know, we're, we're just running an application of some kind on an EC2 instance or a lambda function or ECS or EKS or whatever you want, but it's this, this, uh, your application is reading and reading data from this S3 bucket. This is, this is your uh workload and actually, you can do a lot of things with uh simple workload shaped this way. All right. But we're here to talk about, this is an IAM talk. So, what, what do you have to do with IAM for this workload? Well, OK. Um, You know, because if you've ever tried this, you see that you need to do this. Your EC2 instance or lambda function or what have you has an IAM identity associated with it with the credentials sort of seamlessly delivered to the compute. This is called an IM principle. And before this EC2 instance can read the data from S3, it's gonna need some permissions to do that. Um, so over here, uh, you can see that we've written, uh, we've written an IAM policy. Uh, this is a good, well-configured IAM policy for what we're doing here, assuming that the instance needs to read the whole bucket. So we've given the instance permission to read the bucket and everything works. This is great. OK, but in the, in, in the real world, things often get a little bit more complicated. There's a lot of things you could do with that application, but let's make it a little bit more interesting. Most every customer of AWS, once they get past, you know, a little bit of, you know, experimentation by an individual is gonna have more than one AWS account. Some large customers have thousands of AWS accounts. And again, these accounts are, they're useful security boundary, they're a useful blast radius boundary, a useful kind of a useful kind of unit to contain the resources associated with one workload. So as soon as you start working in an organization that has multiple, uh, that has multiple AWS accounts, you're gonna find that pretty soon you have a workload that's gonna look like this. For whatever reason, the computer is in one account and the data's in another account. Now, there's all kinds of legitimate reasons why you might need to do that. Maybe that S3 bucket is actually data shared across multiple accounts, right? Maybe it's some shared configuration, something like that. So you're gonna have the compute over in one account, reading the data from the other account. Once again, we have that, uh, very good, very well written IAM policy on our EC2 instance says it can read, says it can read that uh S3 bucket. But of course, if you think about that one a little intuitively, right, that shouldn't work. Right. The S3 bucket is now owned by a different account, which is a security boundary in AWS It's a boundary of ownership, and nobody in that account said anything about whether this access should be allowed. This isn't sufficient. What you have here isn't sufficient to make this workload work. OK. Well, again, if you have, you know, if you have kind of some 300 level knowledge of IEM like I assume many of you do, you know exactly the solution to this problem. We're gonna write a bucket policy and basically an IM policy that's attached to the S3 bucket. And this IAM policy, if you look at it, you see that it is saying that account one. Can read the objects in this bucket. OK, so now you've got a count one saying this can happen, you've got a count 2 saying this can happen. And um, those of you who know I am know that this this uh workload is gonna work now. And you know what we have here, what we have here is a well-configured. AWS uh workload well-configured policies in both places. This is a good application that you might write. OK, but let's think a little bit more about that bucket policy that allowed access from outside the account. We had a nice least privileged policy there. Who wrote it? Well, who wrote it? Somebody wrote it, right? A cloud operator of some kind. Now, of course, you might have CICD pipelines, infrastructure is code, you might have automation, that this goes through, so there may be a much more complex and guard railed set up on this, but ultimately, somebody put that policy there. So I have this operator up here. This operator, his job is to write these bucket policies in account two. He, you know, has control over the buckets in account two. And, you know, just to make it real clear, this cloud operator, if I look at the IAM role that he gets to work in, he has this put bucket policy. He's allowed to assign policies to S3 buckets. OK, so what does he write in there? Well, hopefully. He writes that policy we saw in the previous slide. That's a good policy to write if you're trying to share it with account one. But the truth is, with the put bucket policy permission, He could, he could write anything in there. It's simply a document and he gets to write the contents there. So, you know, you might start wondering, how do we make sure that this cloud operator in account 2 who's responsible for this bucket is writing a good least privileged bucket policy on this bucket. How do we know that he's on top of what he's doing? This problem gets a little bit more interesting when we have lots of accounts and lots of buckets and other sorts of resources that can be shared across account and lots of different cloud operators that are respectively empowered to write these policies and you know, among, once you have a lot of people doing a thing, you know that there's gonna be a pretty wide range of skill set here. There's gonna be people who are pros at writing IM policy and people who are just learning how. OK. So what do we do about this? Of course, it's important to us to make sure that all of these bucket policies are reasonably well configured for whatever it is they're doing. But even more important than that the most basic need here is something that we at AWS for the last 6 years or so have been calling a data perimeter. This idea that at scale, Across all of these accounts and if you have many accounts in AWS or probably in AWS organization that's an organization of accounts across all of these accounts, no matter what the respective skill sets of all of these cloud operators are, we are keeping outsiders out of the data. You want it to be that no matter what these operators are doing in these buckets, whether it's maliciously or more likely accidentally just due to not having the right skills, you make sure that the buckets are not accessible from the outside. And that is exactly what we are here to talk about today. We are going to solve this problem here today with a feature called resource control policies. So, I'm gonna start by kind of explaining how resource control policies fit into this story that we started. We'll finish the story and talk about how, how you get there with resource control policies. Um, because this is a deep dive talk in IAM, I'm gonna tell you I work on IM. I love IAM. It is my favorite service in AWS, um, and I'm gonna tell you a little bit about just how it works, like what is actually going on in IAM. You don't really need to understand that to be effective, but it can be very useful in understanding why this and that does and doesn't work. I'm gonna give you, uh, I'm gonna give you a few sort of cheat sheets for the common patterns that we see being effective for customers and then the best part of this talk. I'm gonna have, uh, David Hockey, uh, who's here from Comcast NBC Universal talking about all of the practical considerations as they roll out resource control policies over a very large AWS environment. All of the things, all of the practical considerations to think about the details here are fascinating, um. So that's, that's actually the part that you came for and it's gonna be fantastic. David is, uh, we've worked with David on the IM team, we've worked with David for many, many years, um, and we've gotten a lot of deep insights into how customers really do think about IAM. So you're in for a treat. This talk is by builders for builders. OK, let's go back to our story, which I kind of ruined already because I told you that resource control policies is like, is the, uh, is the endgame of this story. But let's go back to the story. Lots of accounts, lots of cloud operators. They're writing bucket policies, but I don't actually have, I don't actually know what they're writing. So before resource control policies and, you know, and it continues to be the case that there are some solutions to this problem. Um, one solution is this. Well, Hey, we have all of these cloud operators with a diverse set of cloud skill sets. We're not sure we trust each and every one of them to write these policies correctly, so we're not going to. We're gonna take away that permission from all of them. We're gonna make them go through a single arbiter of good taste in IAM policy, someone who really does know their IAM, and this is gonna work and you know how this, uh, you know how this, uh, this operator up at the top really, uh, really knows her stuff and I am or she's wearing her hair up. Yeah, OK. Uh, so, yeah, so she's, you know, they go, they all go through this single reviewer. Um, if this sounds to you like a bottleneck, if it sounds to you like not quite developer freedom, yeah, you're right. But hey, it'll be better than having all of these people write it individually, maybe. OK, what are some other options that we had and continue to have? Well, we have in the specific case of S3, we have S3 block public access. This is a feature that we've had on S3 since 2018 and since 2023, it's actually on by default, so you probably don't even need to think about it anymore. But this is a great automated reasoning, you know, mathematically provable, uh, technology that we have on S3 buckets to make it actually impossible to write an inadvertently public policy on the S3 buckets. So we have that on all of our buckets, you know, then the, the worst kinds of mistakes can't really be made anymore. And that's good, except, um, and, you know, it's out there, it's on by default. You're probably using it whether or not you're thinking about it. Um. And that helps me from some of those use cases of the data perimeter, but you know, what if one of these cloud operators somewhere is writing the policy allowing access to an account, but maybe like misspells the account, fat fingers, it gets it wrong, maybe even maliciously puts a different account in there. Well, that's not a public policy, so block public, block public access not gonna help you there even though it's, you know, so useful in general and it's just on by default. OK, what else could we do? Well, we've been talking about the preventive realm. Uh, let's talk about the detective realm, right? Any good any well-configured, uh, AWS environment from a security perspective is gonna have some detective controls on it. Uh, we have a great feature called AWS config. Uh, AWS config is a great way to get a map of all of your, uh, resources in AWS and it also has this feature called rules. So these are change triggered rules. Uh, we have some that we offer out of the box. You can implement your own anything you want to do on lambda. Functions they can alarm, they can remediate really anything you can do in a lambda function you can do from these rules. So we could write some rules that looks for any changes to S3 bucket policies and, uh, scans them and just make sure that they don't allow in any accounts or organizations that we weren't. Um, expecting and that works, uh, that works quite well too, of course we've had many customers observed to us over the years and this makes sense that preventive controls are always better than detective controls because the preventive control prevents the misconfiguration from being made in the first place. So that brings us to what we launched. It was about 6 months ago, a feature called Resource control policies. So this is a feature of AWS organizations, and this is, this is a a type of IEM policy that you can apply to your whole AWS organization that attaches to all of the resources in it. So this can create a protective boundary around all of your resources and if you want to declare that outsiders can't get in, you can do exactly that in a preventive control and resource control policies, and I'll show you how that works. So they can be applied to the whole organization to organization units, to accounts. That's gonna figure into David's part of the talk. He's gonna talk about how he makes use of that. If these sound similar to a cer uh feature we've had in AWS organizations for quite some time called service control policies, you're right, it's basically service control policies was basically this feature but applied to all of the principles, all of the identities from the organization. This one applies to all of the resources, so it's a very efficient way to keep outsiders out of all of them. So that is what we're here to talk about. Resource control policies. Resource control policies are straightforward, they're scalable, and they're preventive controls that are, you know, that are going to be a linchpin of your data perimeter strategy. Uh, as of today, resource control policies are supported by these 5 AWS services. You can see S3 is, uh, S3 is right up here, but also a few other resources in AWS that our customers have told us is a priority. What all of these resources have in, uh, in common is they're all. Uh, resource types in AWS that are by design shareable across accounts. They all have a lot of use cases that make it very compelling for them to be used across accounts like a key in KMS, you know, in some cases you want to allow another specific account to decrypt using the key. So customers told us that these are the resources that they tend to share and these are, and therefore these are the resources that they most want resource control policies applied to. Of course, we're working on getting coverage for, uh, you know, for a much wider set of AWS services, but those were the priorities. OK, so that's why you might use resource control policies is to create that scalable straightforward perimeter. I'm gonna tell you a little bit about how just in the IAM world, how, how this exactly works. So let's go back to our workload in 2 accounts. And if you remember, we said that we had to write 2 policies here. We had to write one policy on the principal, on the caller that I am roll in the EC2 instance in account one, and then another one on the S3 bucket in account 2. I sort of mentioned offhand that like, hey, it's intuitive that you should need both of these, uh, policies and maybe it was intuitive to you. Um, but I'm gonna be a little bit of pedantic about like exactly why that was needed. OK. So in AWS account, I, I sort of roughly defined it as a, as a unit, an ownership construct for resources, a workload. I've described it in all of the following ways. From an IAM perspective, It's an issuing authority for policies. It is, uh, a thing that owns identities. It's a thing that owns resources and it's a thing that can that is authorized to issue policies for the things in its account. So we have two issuers involved in this request. There's a caller from account one and there's an S3 bucket in account 2. So we actually have 2 issuers here and if we have 2 issuers. That means that each of the issuers is gonna have to say, OK. Before the request is allowed to, uh, before the request is allowed to proceed. And what that means, technically from an authorization perspective is we're gonna do authorization in two separate rounds. First, we're gonna authorize our request against, uh, Against uh policies issued by count one. If we get stuck here, we don't continue, then we're also gonna uh do a separate round with different policies from a account two. We gotta pass through both of those. OK, so we're gonna start with count one. Right. So this one, this, uh, the principal policy, uh, by count one, the bucket policy by count two. Let's start with the count one. OK. Now, if you remember, we wrote a very nice, well-configured IAM policy. We attached it to the principal for that EC2 instance, the IAM role for the EC2 instance over an account one. Um, that means that policy was issued by account one. So in our first round, and I'm ignoring if, if you're an IM pro, you know that there's things like VPC endpoint policies and other things in play here. Don't think about those right now. We're just gonna focus on, uh, the principle and the resource. So over here in it, what, what policies is account one issued? Well, it's issued this policy and probably a lot of other ones that are irrelevant to our request, but we're gonna focus on this one. And uh our authorization system is going to match the request that's being made and all the properties of all of the things in the request against this policy and it's gonna succeed, right, because, uh, uh, the bucket being requested is indeed some the object being requested is indeed something in example bucket, so that's gonna succeed. In fact, because we're here to talk about organization policies, let's make this site a little bit interesting. Let's say that also issued by Account one or technically by the organization that owns account one also issued in this account is a service control policy. And I've, I'm showing you a service control policy here. And, uh, you know, if you're an IAM person and you haven't seen one of these before, you're getting, you're getting a little bit itchy from that first statement. Allow everything to everything. Um, but you should know that a service control policy is not actually a kind of policy that can grant access to anything. It's more a gate that you need to get through. So that first statement says unless it's denied in the rest of this policy, you're going to continue on to the next round, but that statement that might make you itchy doesn't actually grant anybody access to do anything. But that second statement is a pretty common statement for service control policies. Um, I sort of parsing this one out to you it says we're not gonna do anything in S3. Unless the bucket belongs to our AWS organization. That kind of makes sense, right? I don't want them exfiltrating data to an S3 bucket that's not under my control. So I write this policy. OK, so we passed the service control policy because we didn't run afoul of that, uh, deny statement because that other account too actually is in my same organization. And then we went on to the principal policy which said, yes, they actually, you know, they are allowed to do get object. So account one has said this request passes, allow. But we're not done, right? Um, we actually need to go on to account 2, right? Cause there's 2 issuers here. In account 2, it's a very similar story. Account 2 owns the bucket and therefore has issued this bucket policy. But again, let's make this more interesting. Let's add a resource control policy now that we have this feature. And you'll notice that it's very much shaped to the service shaped like the service control policy we looked at a second ago. Um, you can see that that first statement. Yeah, we're allowing everything and that basically means that we're, you know, anything that isn't explicitly denied here is gonna, this is a gate. And then that second one, well, let's take a look at the 2nd 1. The 2nd 1 has two conditions on it. I'm going to explain those two conditions in just a minute, but the first of these conditions says unless the principal is from my organization, unless it's someone from within my perimeter, I'm just gonna say no. I just don't want, I don't, I don't know who's making a request to my bucket, but I don't want them to succeed. Right, so that's the important gate here. That means that, uh, that means that no matter what's in that bucket policy, we have prevented outsiders from getting into my data. If you're wondering what that principle is AWS service is doing at the bottom, don't worry, I'm gonna explain that in just a second, but let me explain to you what just happened. We have an orthogonal control here. This is very, very significant for giving your developers freedom because if you remember, uh, At the top, we might have had, we might have had some org level administrator who really does know what they're doing in IAM. They wrote this resource control policy and that's in place regardless of what that individual cloud operator at the bottom is written. Perhaps that cloud operator has written a really good well scoped policy allowing specifically account one, and that would be great if he did that. But again, at scale, he may also have written this policy. And because we have this orthogonal control. Now, I don't know what this policy does because it's literally a scribble, um. Regardless of what he did, the resource control policy at the top is still in place. Outsiders aren't getting in. Even if he wrote a policy allowing account 666 that is not in my organization. If 666 isn't in my organization, they're not going to be able to get access because of this other orthogonal control. That's the power of this. And what that means is we can have lots of cloud operators and lots of S3 buckets. Hey, S3 recently, uh, drastically raised the number of buckets you can have in an account, so we're now talking millions of buckets perhaps. And no matter what is written in the respective policies of all of these buckets, I know that the policy up top, uh, is in effect here. Gonna give you a little bit of practical guidance here on writing resource control policies. These are patterns that we've seen our customers, uh, use effectively to just achieve some very sort of common goals here. So we're gonna revisit this this uh resource control policy for S3, uh, that we wrote before, and I said, OK, don't allow uh requesters who are outside my org. That aren't AWS services. That's what that second one means. OK. I'm gonna explain this a little bit more. Um, this statement here is denying access from any AWS account owned by someone in the world who's not in my org, and we're excluding AWS so why do we exclude AWS? Well, the policy statement you just, you just saw here, it's good for these kinds of use cases where uh you have just a regular AWS account talking to a resource is another regular AWS account, and a lot of access in AWS is in fact shaped that way any workload you build is going to look like that. But if you've used AWS, you know, there's another pattern of access to your ay bucket that's out there, and that's the AWS services. That are trying to get access to your bucket. A great example is Cloudril, which you're all running because we're here at a security conference. Cloudtrail writes its events into your S3 bucket. You've noticed that you have an S3 bucket with a policy on it allowing cloudtrail. Amazon AWS.com to write its events into your bucket. You want Cloudtrail to access your bucket. Now, of course, Cloudril is not in your organization. They're an AWS service principal entity. Um, the bucket policy on your S3 bucket for cloud trail looks about like this. This isn't the full thing with all of the, uh, with everything in there. I've got a little QR code if you want to see the page of the documentation where we show you the real one. But the important point here, OK, so the caller is cloud trailed at amazon AWS.com, and there's the second thing at the bottom that you're probably noticing called source account. This is an indicator that Cloudtrail is acting on behalf of my account, on account 22, so that's what I expect. I do not expect in this account for Cloudtrail to be operating on behalf of some other account and writing data to my bucket. So I say that specifically in the policy, and that's in the docs example. You can see it. Um, this is if you, if you know the jargon here, uh, the security jargon here, it's called confused deputy. This is a confused deputy protection, Cloud trail on behalf of me. OK, now, of course, I've got a whole organization full of accounts, all of them with cloudtrail turn on, all of them with the cloud trail bucket, and I need to make sure that Cloudtrail is respectively writing my organization's data to each of these buckets. It's not, again, I want confused deputy protection at scale. And so there's actually a second statement that you would normally typically put on a resource control policy statement. This one says, um, Yeah, so this was the one that we looked at before, not an AWS service, they better be for my org. This is the other statement that's usually there with it. If they are an AWS service, that might be OK too, but they better be acting on behalf of, you know this source org ID. They better be acting on behalf of an account in my org. So it's conffused deputy protection scale. Um, so that's how, uh, that's how this works. All right, so that is what resource control policies are. That's how they work. Those are some examples, successful resource control policies that we see customers being successful with out there. And now I want to, uh, bring up David, and he's gonna tell you how this actually plays out in, uh, in practice at Comcast. Thank you so much, David. Thanks, Becky. Uh, so as Becky mentioned, my name is David Hockey, and I lead governance and product for AWS at Comcast within our cable group. Today I'm gonna give you an overview of our organization, tell you a little bit about how we operate in AWS and our approach to AWS, uh, our approach to governance. Um, as part of that, I'm gonna lead into how we're integrating resource control policies into our journey and tell you a little bit more about what's next. So for those of you who don't know, uh, Comcast based here in Philadelphia. Is a media entertainment and telecommunications company. That operates uh uh building customer experiences globally. Within AWS we have thousands of accounts and more than 7500 users, and we operate across tens of regions. Within this we consume dozens of AWS services across multiple lines of business. So an organization that's so complex and varied in terms of use cases, how do we think about governance? So we developed a set of guiding principles that really inform the way we think about everything we do. And that's driven by uh developer experience. The scalability of the solution. Operational stability of the solution and of course all that has to be balanced with security. Choice and agility are key to our culture, but this of course needs to be balanced with security. To achieve that, we implement a core set of controls, both preventative and detective. I'm gonna talk about each about each of these guiding principles a little bit more. So from a developer experience perspective, we think about these things like the ability to deploy and scale quickly. Really being flexible in terms of allowing teams to leverage the services and tools that work best for their use case. In terms of seamless, yeah, in terms of integration, we wanna make sure that we respect the AWS native SDKs and tooling wherever possible, and we always prefer self-service access over tickets. On the ground floor of this um is the DevOps Foundation. We want to make sure that anything we're implementing emphasizes automation, infrastructure is code, and CICD. So how does the rest of does the rest of that play in? We take a layered approach to security. On the on the bottom we have organizational defaults. You can think of these as things like VPC configuration, IMDSV2, uh, usage, allowed on these, and tag standards. On top of that are preventative controls. These tend to be more uh core grain things like service control policies and resource control policies. And then above that we have a set of detective controls. This consists of a series of 1st and 3rd party services as well as custom tooling. And then finally we have that fine grain configuration. You can think of this as things like I am principal and resource based policies and security groups. All of this forms a shared responsibility model in the same way AWS has a shared responsibility model with its customers being responsible for the security of the cloud, customers are a responsibility for security in the cloud, and we share that with our users. It's a partnership between our cloud security and application teams. So from an operational stability perspective, we look at these organizational defaults and preventative controls, the things that can break potentially break things when deployed, and everything we do in this space needs to be scalable to the number of accounts and engineers that we're supporting. These have the largest blast radius and therefore require the largest amount of change control. Above that, we think about detective controls and fine grain configuration as part of a least privileged journey. Detective controls provide a framework for an iterative approach to least privilege and monitor for things that don't yet have preventative controls. So the natural question you might have is, OK, so with all the different places you can implement controls, how do you decide on the approach and the way we think about this or the way I think about this is, is sort of on two dimensions. Uh, on the bottom we have sort of the adoption of the service or feature that you're thinking about making a change to and on the on the Y axis we think about sort of the uh increasing complexity or risk that's associated with making that change. So, um, In the base case or in the perhaps the simplest case, you have a new feature or service. This is where you have the easiest opportunity to act. You can imagine writing a policy, a very simple policy like this, uh, you know, deny a given service for all principles and all resources. This is pretty straightforward. What happens as adoption grows? So when you have something like a low use or niche service, uh, this typically requires modest amount of communication and mitigation, uh, overhead to apply changes in this case. Uh, you might start with detective controls or you might take something like a cap and grow approach. So similarly, we're writing now a slightly different policy where we're saying, you know, deny access to a given service, uh, for all principles and all resources except those in let's say account one and account 3. So in this case we're capping usage on the current set of accounts and we're going to grow usage in perhaps a preferred alternative solution. Where things get more complicated is once you hit a critical mass. So this happens most often when the AWS introduces new controls like new preventative controls like resource control policies that we'll be talking about today. In this case, we always start with detective controls, and there's significant trade-offs, planning and timelines associated with making changes at this point becomes much less cut and dry. Now you can see that you know this deny statement for a given service, it's no longer obvious what principles or resources or sets of accounts we're going to allow when there's when adoption is so uh proliferated across the organization. So how does this apply to RCPs? Well, so for context, there's a lot of ways you can uh use RCPs. Our initial approach is really focused on the data perimeter use case that Becky talked about earlier, really thinking about how data enters and leaves our organization. When we when we looked at the set of resources that are supported by resource control policies, we also know that we use all of these services at significant scale. So alongside this significant new capability, um, that augments detective controls, it, it has a huge potential benefit but also huge potential impact. There's also some limitations we'll have to be aware of. As I mentioned, there's obviously a very large glass radius. Um, you're limited to 1000 policies today per organization, so this becomes relevant as you think about potentially writing policies per account. And like service control policies, you're limited to 5 RCPs uh per entity. An entity is an AWS organization, organizational unit, or account, and you're limited to about 5000 characters. This is really about as complex as it could get. So what do we do in this scenario? Well, we look at the set of capabilities in our governance tool kit today I'll primarily be focused on these. And uh really let's look at how we're gonna uh design and deploy a set of policies. So to help you understand how we're gonna do this, it's important for you to understand what our organizational AWS organizational structure looks like today. Today we have sort of two key organizational units, a sandbox organizational unit and a standard organizational unit. Sandbox organizational unit houses accounts for learning and development. Um, they don't contain any production data, uh, they're time bound and budget bound. On the other side we have our what we call a standard organizational unit. This is where all other accounts live. You can think non-prod prod accounts, QA, what have you. We apply the same controls or similar controls to all accounts in this area and the reason we do that is it really allows our development teams to have confidence that something that they build in a lower environment will successfully promote as they go towards production. This also gives them some more flexibility in terms of uh what and how they do things or what they store in perhaps lower environments knowing that they have the same expectations of uh security controls applied in those environments. The other important thing to note is that this doesn't reflect any sort of business hierarchy but really policy boundaries such as regional restrictions, and I'm showing that here as a control organization, two different control organizational units. So using this organizational structure actually really helps us when we think about how are we gonna deploy complex changes when we're thinking about new deployments, we look at a couple of things and we consider things like the risk of change. Are we adding or removing access as well as the location of the change? Is this a change within an account or uh an organization wide change? And based on this, we'll typically do some sort of phase deployment. Generally we'll start with internal test accounts. These are accounts that are managed by our team that live within our standard organizational unit where we'll apply those controls perhaps first within the account or on top of the account depending on the type of change being made. And then once we're confident in the in the way that policy looks and we're ready to deploy it further, we'll take that first to our sandbox organizational unit. This again uh emphasizes that pillar of uh operational stability. We know that changes we make here are unlikely to have any significant uh organizational impact as they don't contain any production workloads. This is also the time when we start communications and documentation with our development community. Well then again, uh, uh, focus on new accounts. Again, operational stability, no running resources, very unlikely to break anything. Then we look at things like accounts not potentially not using that service or feature that we're impacting again lowest risk, and then as we get into the higher risk, uh, categories, we'll proceed typically in waves looking at things like production versus non-production as well as impacted resource count. So now that you know how we're gonna deploy these policies, what, what is the actual policy that we're gonna deploy? So there's some policy considerations that you need to be aware of, um, especially if you're coming from a world like we are, uh, deploying service control policies for the last several years, uh, we're generally comfortable with the policy language and syntax and deployment approach, uh, but there's some new differences that we need to be aware of that sort of change the way you think about writing policy. So SEP is applied to to principles. This could be a user in your organization in AWS role, um, or another AWS service versus the number of resources in your organization. So for us we imagine that we have orders of magnitude more resources than we have principles, and this really comes into play when you're thinking about designing for allowances. So if you're thinking about enumerating resources that you're gonna allow through this data perimeter. You need to be cognizant of the fact that the number of resources you might need to exempt is likely far more than you've ever had to write in some sort of principle policy like an SEP. Additionally, RCP is applied to both principles inside and outside of your organization. Which creates a new testing burden. Um, previously with SEPs you could be really confident that a policy you applied to a principal in your account in your organization, uh, would exhibit the behavior as any other resource or any other principle in your organization. With RCPs you have this new dimension where you no longer have visibility potentially into the user or role that it's calling into your organization. So there's a few ways to test this. You can either create uh an additional AWS organization such as test organization or leverage it, uh, an organization from a trusted third party. Or you can apply inorganizational controls. So in addition to those uh resource organization ID and um source organization ID condition keys that Becky talked about earlier, we also have condition keys called uh resource OU path and source OU path which allow you to do things apply similar data perimeter controls but across organizational units uh within your environment. And lastly, going back to that confused deputy case that Becky talked about earlier, where an AWS services operating on your behalf, it turns out that you also have to make a compatibility versus security decision. Today not all AWS services support those uh confused deputy condition keys. So when you're rolling out RCPs, you really have to think about, uh, do I want to take an operationally safe approach and allow all existing services to work, uh, seamlessly with my organization, or do you take a more security centric approach and say, hey, only services that support these confused deputy keys will be allowed to continue to operate in my organization. So we have a policy that we're gonna deploy and we know what some of the limitations are, but how do we actually determine the impact of applying these new preventative controls? So for us we turn to an AWS service called IAM Access Analyzer. You probably heard about it a little bit in the keynote this morning. Access Analyzer is integrated with AWS organizations and it's a regional service. Uh, the goal of Access Analyzer, one of the capabilities of Access Analyzer is to tell you about all the, uh, access that is external to a given AWS account or AWS organization. So what that allows you to do is really understand, OK, who currently has access to resources outside of my account. And to clean up that data you can use another capability of access Analyzer called archive rules. This allows you to define well known, uh, exceptions or exclusions I should say for well-known federated trusts, things like your single sign on providers, uh, third party SAS products you might uh deploy pervasively in your organization. And additionally, just as a note, uh, we also defined an archive rule for all IAM resources in everything but one region today Access Analyzer creates findings in every region for for IAM. And then we aggregate and analyze the remaining findings using the service APIs as well as custom tooling. There are some limitations to be aware of though of Axis Analyzer. So currently it actually doesn't support the confused deputy use case as it doesn't include service principles like uh the service that would be used to deliver VPC flow logs to an S3 bucket that you maintain in resource policies. So if you're planning on deploying safely confused deputy controls, um, you need to leverage other tools, potentially a third party CSPM or AWS config to evaluate those resource policies and understand impact. It's also important to note that Axis Analyzer is a static analysis. So while it identifies all of that existing external access, it doesn't actually tell you if it's still in use or if it's required. And it supports today only a single zone of trust, and what that means is that um it only supports looking at a single AWS account or a single organization at a given time. So if you if you have another AWS organization you expect to allow in your data perimeter, you're gonna have to sort of manually pull or programmatically pull the list of accounts from that AWS organization and integrate it into your custom tooling, uh, in order to, you know, sort of filter out those accounts. Um, Access Analyzer doesn't cover all services but does generally cover all services that you can impact today with RCPs, which makes it a really great tool to use. Another tool you can use to assess impact is AWS Cloud trail. Cloud Trail also integrated with AWS organizations and also regional service, supports centralized event delivery for all activity going on in your uh in your AWS accounts, and Cloudtrail is really your opportunity to say like how do I determine what's really gonna break. So the nice thing is that Cloudtrail by default and and for free logs uh management events and so for STSss Manager and KMS, uh, basically all management events, control plan events and data events are actually included in this by default. That being said, there's other resource types that are classified as data events like S3 object level APIs and SQS message level level APIs that come at an additional cost. There's a cost benefit trade off uh for data events and most organizations likely only enable them for the highest value or risk resources so Cloudtrail may not be able to tell you truly if a resource is in use, uh, without potentially, uh, you know, significant spend and significant data volume to analyze. So with that, what did things look like prior to RCPs? So prior to RCPs, we can imagine that scenario that Becky showed earlier of the of an authorized developer looking to provide access to an S3 bucket to a third party account. Um, and they write that bucket policy. So today we can use things like or prior to RCPs we could use things like detective controls to notify that user and confirm hey is this access really required? And at that point, you know, they can potentially go change that policy as required. But in the new world where we've deployed some sort of restrictive resource control policy on top of the account 2 when that authorized developer adds a bucket policy that's 3, that access is no longer allowed. So what do they do? So in order to support this in a scalable way we implemented a self-service process where where developers can go and declare their uh intended external access uh through a portal it goes through some sort of additional approval logic and then once that request is approved, uh, we then have automation that'll go and deploy. Uh, this updated, uh, RCP on top of the count 2, and you can see now that the combination of these three policies have allowed access into the organization. Additionally, this gives us a great data set to understand uh when uh or a great place to uh remove access as well when it's no longer required, uh, and we can simply remove access from the resource control policy using that same set of information that we had before. So where are we in this? We've created that external access provisioning process that I just talked about and we've developed a sort of a baseline external access RCP for our organization. We've identified or we're identifying the existing required external access and we're utilizing a cap and growth strategy where basically we're gonna take all uh sort of existing required external access and we're gonna put that into account specific RCPs, um. And then basically any new access after these deployments happen we'll have to go through that self-service provisioning process and we're in the initial rollout of this now. What's next? So we're gonna continue deployments. Uh, we need to fully automate that allow listing process today there's still sort of a manual step between a request being approved and then that RCP getting updated. We're planning to fully automate that end to end, so that, uh, development teams have sort of fully self-service experience. We know that we have to iterate on this approach and be cognizant of the limits, as I mentioned before, you know, we already have more than 1000 accounts, so we have to really be cognizant of the fact that there are these limits and we have to be aware of where and how external access is required. And then we're starting to look at as AWS makes new service available with RCBs, how would we safely roll those out on top. So with that, I'm gonna turn it back to Becky to wrap things up and talk about additional resources available to you so you can do this in your organization. Wow, thank you so much, David. I those I hopefully you're all taking notes because this is what, you know, this is what builders, particularly builders at large enterprise organizations need to know in order to be able to effectively, uh, roll out these, uh, roll out these, you know, wide impact, uh, kind of changes to establish this perimeter. So now is the time when you should take out your phones because I got some QR codes for you that are gonna save you a lot of time. All right, so here we have this is our, uh, this is our white paper on building a data perimeter at AWS. If this sounds like something you wanna do, and it probably is to, you know, keep your data inside your organization, keep outsiders out of your data and other considerations that I didn't talk about today like network-based, uh, data perimeter rules start and you don't know where to start. Start with this white paper kind of explains the concepts and then gives you a, uh, further kind of prescriptive advice on how to proceed. Take a picture of this one too. Uh, we have a GitHub repo. We have a GitHub repo that gives you, uh, it's basically your cheat sheet. We give you the copy pasta here, right? Like these are, these are the policies that we recommend that you put in place to establish a data perimeter so no guessing. This isn't your high school physics class. We gave you the right answers. Um, also very recently in this, uh, repository we've also added some, uh, for a, uh, initial set of services, 20 something of them, uh, we've added some service specific guidance that kind of takes you through anything special you need to, you know, from a data perimeter perspective that you should be thinking about as you're using that service, so you no longer need to study them yourselves. We studied it, we put it up there for you. Um, so I hope you have a great rest of your conference here at Reinforce in Philadelphia, and David and I would love to take your questions if you have hard questions there for David. Um, we, uh, we're gonna take just out of, uh, out of consideration of the expo hall of the, uh, hall here. We're gonna take them out in the expo just back there. So if you're trying to ask us a question, we'll see you out there in just a minute or two. So thank you so much for coming. Have a great rest of your conference. Have a great time here in Philadelphia.
