# AWS re:Inforce 2025 - How the AWS obsession with resilience helps customers build for adversity

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=rrh7GtOsiEw)

## Video Information
- **Author:** AWS Events
- **Duration:** 55.5 minutes
- **Word Count:** 8,633 words
- **Publish Date:** 20250619
- **Video ID:** rrh7GtOsiEw

## Summary
AWS demonstrates its obsession with resilience through four core availability axioms that drive continuous innovation in cloud infrastructure. The presentation reveals how AWS achieves "antifragile" systems that become stronger after each incident by investing in regional isolation, availability zone resilience, rigorous testing, and overload protection. Through examples like regionalizing the Security Token Service and building automated fleet health systems, AWS transparently improves customer workload reliability while reducing operational burden on the customer side of the shared responsibility model.

## Key Points
- **Operational Excellence Culture**: AWS developers carry pagers and own the complete customer experience, including API design, availability, and performance - not just code development
- **Four Availability Axioms**: Region isolation, AZ resilience, rigorous testing, and overload protection form the foundation of AWS resilience strategy
- **Transparent STS Regionalization**: AWS successfully migrated millions of global STS calls to local regions without customer action, reducing latency from 230ms to under 20ms
- **Automated Fleet Health Management**: Lambda's fleet health service uses CloudWatch Contributor Insights to detect and automatically remove gray-failing instances that are still responding but performing poorly
- **Zonal Autoshift**: AWS automatically detects and shifts traffic away from impaired availability zones using outlier detection, protecting both AWS services and enrolled customer applications
- **Game Day Testing Culture**: AWS operates a dedicated test region for continuous chaos engineering and maintains competence envelope through regular failure mode validation
- **Metastable Failure Prevention**: AWS scientists developed METAPHOR tool and multi-stage testing strategy to identify and prevent feedback loop failures that can keep systems in stable down states
- **Antifragile Philosophy**: Every incident makes AWS stronger through systematic learning, sharing across teams, and building preventive mechanisms rather than just fixing individual issues

## Technical Details
- **STS Regional Migration**: Used Route 53 Resolver integration to transparently redirect VPC-based STS calls to regional endpoints while maintaining CloudTrail log consistency
- **Fleet Health Service Architecture**: Leverages CloudWatch Embedded Metric Format logs, Contributor Insights rules, and CloudWatch Metric Math alarms to detect instances contributing >66% of total errors
- **Token Bucket Velocity Controls**: Automated remediation systems use token buckets to prevent runaway automation and limit how many instances can be terminated within defined time periods
- **Zonal Event Detector**: Combines infrastructural metrics with service-level telemetry to identify availability zone outliers, with strict limits of one AZ shift at a time
- **Game Day Region Design**: Full production-scale region with three AZs specifically for testing power outages, network partitions, service failures, and novel rare events
- **METAPHOR Statistical Modeling**: Open-source tool that visualizes queue length vs. outstanding retries to identify metastable failure inflection points through arrow-based state transition graphs
- **Multi-Stage Testing Strategy**: Statistical modeling → laptop simulation → physical server emulation → production-scale testing to efficiently explore parameter spaces
- **Competence Envelope Expansion**: Regular exposure to growing failure mode sets through automated testing maintains and expands the boundaries of expected system behavior

## Full Transcript

>> WELCOME EVERYONE. WE
FREQUENTLY TALK ABOUT THE MANY BENEFITS OF CLOUD COMPUTING,
LIKE ON DEMAND CAPACITY, AUTOMATIC SCALING, CONSISTENT
SECURITY CONTROLS, AND MANAGED SERVICES THAT REMOVE
UNDIFFERENTIATED HEAVY LIFTING. BUT ONE OF THE LESS VISIBLE
BENEFITS OF RUNNING ON AWS IS THE FACT THAT OUR TEAMS
CONSTANTLY OBSESS ABOUT RESILIENCE IN OUR SERVICES AND
IN YOUR APPLICATIONS. AT AWS, WE SERVE MILLIONS OF CUSTOMERS
ACROSS 37 REGIONS, HANDLING BILLIONS OF REQUESTS A SECOND.
THIS SCALE PROVIDES US WITH UNIQUE OPPORTUNITIES TO INNOVATE
AND MAKE INVESTMENTS THAT MOST COMPANIES COULDN'T JUSTIFY IN
THEIR OWN DATA CENTERS OR APPLICATIONS. IT ALSO REQUIRES
US TO DIVE DEEP ON A MYRIAD OF DETAILS THAT OTHERS MIGHT
OVERLOOK, OR THAT MANY BUSINESSES WOULDN'T BOTHER WITH.
RESILIENCE IS MORE CRITICAL THAN EVER FOR YOU. FOR YOU, OUR
CUSTOMERS, TO DELIVER ESSENTIAL SERVICES ACROSS THE GLOBE THAT
POWER ECONOMIES, GOVERNMENTS, CRITICAL INFRASTRUCTURE AND
MORE. AND AFTER ALMOST AFTER OVER SEVEN YEARS OF WORKING HERE
AT AWS, I'VE COME TO LEARN THAT WE THINK A LITTLE BIT
DIFFERENTLY ABOUT RESILIENCE. THAT SUMMED UP REALLY WELL IN
THIS QUOTE. WE'RE REALLY PROUD OF AWS'S OPERATING RECORD, AND
WE HEAR FEEDBACK ROUTINELY FROM CUSTOMERS THAT WHEN THEY MOVE TO
THE AWS CLOUD, THEIR RELIABILITY IMPROVED SIGNIFICANTLY. BUT WE
STILL OCCASIONALLY HAVE INCIDENTS, AND TO SOME DEGREE,
THEY'RE INEVITABLE. OUR OBSESSION WITH RESILIENCE MEANS
THAT WE ARE CONTINUOUSLY TRYING TO LEARN AND IMPROVE FROM EVERY
INCIDENT, SO THAT WE'LL HAVE FEWER FUTURE INCIDENTS AND THAT
THEY'LL BE SHORTER AND SMALLER, AFFECTING FEWER CUSTOMERS. WE'RE
AIMING AS AN ORGANIZATION TO BE ANTIFRAGILE SO THAT AFTER EACH
INCIDENT, WE'RE BETTER THAN WE WERE BEFORE. AND THERE'S A LOT
OF INNOVATION THAT GOES ON TO ACHIEVE THIS. SOME OF THE
INNOVATION INCLUDES DELIVERING TRANSPARENT IMPROVEMENTS IN OUR
SERVICES THAT ARE MOSTLY INVISIBLE TO CUSTOMERS. OTHER
INNOVATIONS SEEK TO REDUCE THE AMOUNT OF WORK CUSTOMERS HAVE ON
THEIR SIDE OF THE SHARED RESPONSIBILITY MODEL, LETTING
AWS SHOULDER MORE OF THAT BURDEN FOR YOU SO THAT YOU HAVE LESS TO
DO. MY NAME IS MIKE AIKEN. I'M A SENIOR PRINCIPAL SOLUTIONS
ARCHITECT, AND I SUPPORT CUSTOMERS ACROSS AWS, BUILD AND
OPERATE RESILIENT WORKLOADS IN THE CLOUD.
>> AND MY NAME IS GAVIN MCCULLOUGH. I'M A SENIOR
PRINCIPAL ENGINEER IN THE AWS RESILIENCE ORGANIZATION. WE OWN
A SET OF PRODUCTS LIKE ROUTE 53 AND APPLICATION RECOVERY
CONTROLLER, AS WELL AS WORKING ACROSS AWS ON RESILIENCE
INITIATIVES. I ALSO WORK AS AN AWS CO-LEADER, WHICH IS OUR
EQUIVALENT OF AN INCIDENT COMMANDER, IF THAT IF THAT TERM
IS FAMILIAR. SO TODAY WE WANT TO PULL BACK THE CURTAIN A LITTLE
AND SHARE SOME OF THE BEHIND THE SCENES INNOVATION THAT HELPS
MAKE AWS THE BEST PLACE TO RUN MISSION CRITICAL WORKLOADS AND
BUILD FOR ADVERSITY. BEFORE WE GET INTO TOO MANY DETAILS, I
WANT TO TALK A LITTLE BIT ABOUT HOW WE THINK ABOUT OPERATIONS AT
AWS. WE CONSIDER OPERATIONAL EXCELLENCE TO BE A CENTRALLY
IMPORTANT PART OF BEING AN ENGINEER, AN ENGINEERING
MANAGER, OR FOR THAT MATTER, A LEADER, A ROLE GUIDELINES TALK
ABOUT IT. IT'S PART OF ALL OF OUR PERFORMANCE AND PROMOTION
DISCUSSIONS. AND IT'S A KEY PART OF WHAT WE ALL DO. OUR
DEVELOPERS DON'T JUST WRITE THE CODE. THEY CARRY PAGERS. THAT'S
DONE DELIBERATELY. SO THEY ARE THINKING CAREFULLY ALL THE TIME
ABOUT HOW THE SERVICE IS GOING TO OPERATE IN PRODUCTION. WE
NEED OUR DEVELOPER TEAMS TO TRULY OWN THEIR CUSTOMER
EXPERIENCE, WHETHER THAT'S THE WAY THE SERVICE WORKS, THE
SEMANTICS OF THE APIS, OR THE AVAILABILITY AND PERFORMANCE OF
THE SERVICE. SERVICE TEAMS ALL RUN THEIR OWN REGULAR WEEKLY OPS
MEETINGS WHERE THEY REVIEW RECENT INCIDENTS, LOOK AT
TICKETS, EXAMINE THEIR METRICS DASHBOARDS FOR ANOMALIES, AND
PLAN HOW TO IMPROVE THE OPERATIONAL STATE OF THE
SERVICES THEY OWN. THE COMMON EXAMPLE CONVERSATIONS YOU'D HEAR
WOULD BE LIKE, LOOK AT THAT GRAPH. THE P99 LATENCY OF THAT
API CALL HAS GONE UP BY TEN MILLISECONDS SINCE TUESDAY. WHAT
HAPPENED? WE OBSESS ABOUT LITTLE DETAILS LIKE THAT. WE HAVE VERY
HIGH EXPECTATIONS FOR ENGINEERS DIVING DEEP. IT'S NOT ABOUT JUST
RESOLVING ISSUES, BUT UNDERSTANDING WHY THEY OCCURRED.
SO WE CAN IMPROVE. AS WITH ANY SOFTWARE SERVICES, AWS SERVICES
HAVE ISSUES OCCASIONALLY. MOST ARE PRETTY INNOCUOUS, WITH MOST
CUSTOMERS NOT EVEN NOTICING. BUT WE HAVE TO TAKE EVEN THE SMALL
ISSUES SERIOUSLY. OBVIOUSLY, OUR TOP PRIORITY IS ALWAYS TO
MITIGATE ANY CUSTOMER IMPACT IMMEDIATELY, BUT OUR NEXT
PRIORITY IS TO LEARN. WE WANT TO ANALYZE THESE EVENTS, LEARN THE
RIGHT LESSONS, TAKE THE ACTION ITEMS SO THAT WE DON'T JUST
CORRECT A SPECIFIC ISSUE OR BUG, BUT MAKE US MAKE THE SYSTEM MORE
RESILIENT FOR FUTURE UNEXPECTED EVENTS. FINALLY, WE AIM TO SHARE
WHAT WE LEARN ACROSS TEAMS. OUR WORK AT THIS STAGE HAS THOUSANDS
OF DEVELOPER TEAMS. WE WANT TO SHARE THE LEARNINGS, MENTAL
MODELS AND BEST PRACTICES SO THAT ALL OF OUR TEAMS CAN LEARN
FROM EACH OTHER. THIS IS REALLY WHERE WE START TO ACHIEVE THE
ANTIFRAGILITY THAT MIKE IS TALKING ABOUT. AS AWS AND AMAZON
HAVE GROWN, ONE OF THE CHALLENGES HAS BEEN WORKING OUT
HOW TO SUCCINCTLY COMMUNICATE OUR CORE OPERATING IDEALS ACROSS
A LARGE ORGANIZATION. AND SO TO HELP WITH THAT, IN RECENT YEARS
WE'VE BEEN WORKING ON WHAT WE CALL AVAILABILITY AXIOMS. THESE
ARE A SET OF CONCISE BEST PRACTICES THAT OUR SERVICE TEAMS
ALL FOLLOW AND EXPECT OF EACH OTHER. SO HAVING THESE SHORT
AXIOMS GIVES US A COMMON LANGUAGE IN WHICH TO DISCUSS OUR
SYSTEMS. LET'S LOOK AT A FEW EXAMPLES. THE FIRST IS REGION
ISOLATION. WE EXPECT AWS REGIONS TO OPERATE HIGHLY INDEPENDENTLY.
SO IN THE RARE CASE THAT A PROBLEM OCCURS IN REGION A, WE
KNOW IT WILL BE LIMITED TO REGION A AND WILL NOT AFFECT ANY
OTHER REGION. THIS VERY SIMPLE PRINCIPLE HAS MAJOR AVAILABILITY
BENEFITS FOR CUSTOMERS, AS WE NOW HAVE 37 REGIONS AND GROWING,
IT'S CRITICAL THAT EACH REGION OPERATES AS INDEPENDENTLY AS
POSSIBLE. OUR SECOND AXIOM RELATES TO AVAILABILITY ZONES.
FOR THOSE OF YOU NOT FAMILIAR, AWS REGIONS ARE ALL BUILT WITH
THREE OR MORE AVAILABILITY ZONES FOR REDUNDANCY. THESE ARE
PHYSICALLY SEPARATED GROUPS OF DATA CENTERS SPREAD AROUND A
METROPOLITAN AREA. THEY HAVE SEPARATE INDEPENDENT POWER AND
NETWORKING. EVERY AWS REGION IS DESIGNED TO GRACEFULLY HANDLE
ONE AVAILABILITY ZONE BEING IMPAIRED OR IN THE RAREST CASE,
EVEN COMPLETELY UNAVAILABLE. SO WITH THAT IN MIND, OUR AZ
RESILIENCE AXIOM SAYS THAT EVERY REGIONAL AWS SERVICE MUST BE
ABLE TO TOLERATE AN IMPAIRMENT TO ONE AZ WITHOUT DEGRADING
THEIR CUSTOMER EXPERIENCE. WE PUSH TEAMS HARD TO MEET THIS AND
WE TEST IT PRETTY FREQUENTLY. AN IMPORTANT THIRD AXIOM IS
CENTERED AROUND TESTING. THAT CODE SHOULD BE TESTED PRIOR TO
GOING INTO PRODUCTION. WON'T BE NEW TO ANYONE, BUT THE AXIOM
GOES INTO QUITE SPECIFIC DETAILS ON BEST PRACTICES AROUND UNIT
TESTING, INTEGRATION ACCEPTANCE TESTS, THINGS LIKE THAT, AND
WHAT WE CONSIDER TO BE BEST PRACTICE. IN ADDITION, TEAMS
ALSO HAVE REGULAR USE OF CHAOS TESTING AND WHAT WE CALL GAME
DAYS, WHICH I'LL TALK ABOUT A LITTLE MORE LATER ON. AND THE
LAST AXIOM IS AROUND OVERLOAD PROTECTION. WE'VE LEARNED A LOT
OVER THE YEARS ABOUT PROTECTING SYSTEMS AGAINST OVERLOAD, AND
THE MANY WAYS OVERLOAD CAN ARISE. PEOPLE TEND TO IMAGINE AT
FIRST THAT OVERLOAD IS CAUSED BY BAD ACTORS, AND THAT CAN HAPPEN.
BUT IN PRACTICE, WE OFTEN FIND OVERLOAD IS CAUSED BY HOW
SYSTEMS INTERACT WITH EACH OTHER IN UNEXPECTED WAYS. AND SO OUR
SYSTEMS ALL HAVE TO BE BUILT TO DEFEND THEMSELVES AGAINST
OVERLOAD FROM ALL OF THEIR CALLERS. AS WITH PEOPLE, IT'S
CRITICALLY IMPORTANT THAT OUR SYSTEMS HAVE MECHANISMS TO
QUICKLY AND CHEAPLY SAY NO WHEN ASKED TO DO MORE WORK THAN
THEY'RE GOING TO BE SUCCESSFUL AT. AND AS WITH PEOPLE, DECIDING
WHO TO SAY NO TO IS ALSO REALLY IMPORTANT. SO LET'S EXPLORE A
SET OF RECENT EXAMPLES IN WHICH AWS HAS BEEN INNOVATING TO
DELIVER ON THESE FOUR AVAILABILITY AXIOMS.
>> WE'RE FIRST GOING TO TAKE A LOOK AT HOW WE IMPROVE THE
REGIONAL ISOLATION OF AWS SERVICES AND CUSTOMER WORKLOADS
BY EXPLORING THE EVOLUTION OF THE SECURITY TOKEN SERVICE OVER
THE PAST 14 YEARS. STS ALLOWS A GIVEN IAM ROLE WHERE A USER TO
ASSUME A DIFFERENT ROLE TEMPORARILY, USUALLY TO PERFORM
SOME TASK WITH DIFFERENT PRIVILEGES. IT'S KIND OF LIKE
THE SUDO COMMAND ON A UNIX SYSTEM, EXCEPT WITH ONE REALLY
IMPORTANT DIFFERENCE. STS IS AIMED AT ASSUMING PURPOSE BUILT
LEAST PRIVILEGE IAM ROLES. THE MECHANICS ARE THAT WHEN YOU CALL
STS WITH THE ASSUMED ROLE API, IT RETURNS A TEMPORARY SET OF
API CREDENTIALS FOR THE IAM ROLE THAT YOU'RE ASSUMING THESE
CREDENTIALS ARE SHORT TERM TYPICALLY EXPIRE IN AN HOUR. AND
SO STS IS USED IN MANY DIFFERENT WAYS. FOR EXAMPLE SINGLE SIGN ON
SYSTEMS. OR WHEN YOU AUTHORIZE AN AWS SERVICE TO TAKE AN ACTION
ON YOUR BEHALF. WHEN WE LAUNCHED STS IN 2011, LONG TERM
CREDENTIALS AND USERS WERE PRETTY STANDARD ACROSS THE
INDUSTRY. STS USE OF SHORT TERM CREDENTIALS AND RESTRICTED ROLES
WAS A SIGNIFICANT INNOVATION AT THE TIME. AND THIS APPROACH MAY
SEEM OBVIOUS NOW, BUT BACK THEN THIS WAS A GENUINE INVENTION. WE
WERE EXPLORING NEW TERRITORY, AND WE WERE STILL LEARNING ABOUT
HOW THE SERVICE WAS GOING TO BE USED BY CUSTOMERS. AND IT
QUICKLY BECAME APPARENT HOW POWERFUL AND IMPORTANT STS WAS
GOING TO BE. AS ITS POPULARITY GREW, SO DID ITS CRITICAL NATURE
WITHIN THE AWS ECOSYSTEM, AND MORE WORKFLOWS STARTED TO MAKE
USE OF THIS API. BECAUSE STS WAS DISTRIBUTING CREDENTIALS FOR IAM
ROLES, WE DECIDED TO LOCATE THE STS ENDPOINT IN THE SAME PLACE
AS IAM IN US EAST ONE. BOTH OF THESE WERE CONSIDERED GLOBAL
SERVICES, MEANING THAT THERE WAS A SINGLE ENDPOINT THAT USERS AND
SERVICES IN ALL REGIONS USED TO CALL ITS APIS. THIS SEEMED LIKE
A MINOR DETAIL AT THE TIME, BUT AS WE LEARNED MORE ABOUT THIS
NEW SERVICE AND HOW IT WAS GOING TO BE USED, WE STARTED TO
REALIZE THIS WASN'T REALLY THE OPTIMAL CHOICE FOR STS. AND SO
WHILE WE ORIGINALLY THOUGHT OF STS AND IAM AS BEING STRONGLY
COUPLED SIBLINGS, THERE WERE ACTUALLY SOME VERY REAL
DIFFERENCES IN HOW THE SERVICES WERE USED OPERATIONALLY. FIRST,
IAM IS GLOBALLY CONSISTENT. WE REPLICATE ALL OF YOUR ROLES AND
POLICIES TO EVERY ENABLED REGION, SO THAT EVERYTHING HAS
THE SAME VIEW OF YOUR IAM RESOURCES. AND THIS REQUIRES A
CENTRAL SOURCE OF TRUTH THAT'S MANAGED BY THAT GLOBAL ENDPOINT.
IN CONTRAST, STS IS LOCAL AND STATELESS. TOKENS, ONCE THEY'RE
PRODUCED, AREN'T STORED BY AWS. THE SAME TOKEN ALSO CAN'T BE
RETRIEVED TWICE, SO THERE'S NO SIMILAR NEED FOR A CENTRAL
SOURCE OF TRUTH AND GLOBAL CONSISTENCY. SECOND, THE IAM
APIS ARE TYPICALLY USED ONLY DURING IAM RESOURCE CREATION AND
UPDATES. THIS NORMALLY HAPPENS AT BUILD TIME WHEN YOU'RE
DEPLOYING THINGS THROUGH CLOUDFORMATION STACKS OR
RELEASING THINGS THROUGH A PIPELINE. STS, ON THE OTHER
HAND, IS USED FREQUENTLY DURING RUNTIME, OBTAINING TEMPORARY
CREDENTIALS FROM STS WAS BECOMING PART OF THE CRITICAL
WORKFLOW FOR APPLICATIONS AND TEMPORARY ACCESS IN AWS, AND A
LOT OF AWS SERVICES START DEPENDING ON STS AND WAYS THAT
THEY WOULDN'T FOR IAM. THIS ALSO MEANS THAT STS IS CALLED MUCH
MORE FREQUENTLY THAN THE IAM APIS ARE. SO AS WE LOOKED AT
THESE TWO SERVICES, WE BEGAN TO REALIZE ONE OF THESE ISN'T LIKE
THE OTHER. >> SO THE QUESTION WE STARTED TO
ASK OURSELVES ABOUT STS WAS, GIVEN HOW IMPORTANT IT'S
BECOMING FOR AWS SERVICES AND FOR CUSTOMERS, COULD WE MAKE STS
REGIONAL IN ORDER TO PROVIDE THE STRICT REGIONAL INDEPENDENCE WE
WANT FROM ALL OUR SERVICES? SO WE STARTED WORK IN 2014 TO
DELIVER REGIONAL ISOLATION FOR STS, AND THIS WAS A PRETTY BIG
DECISION FOR THE COMPANY AS IT'S GOING TO BE A LOT OF INVESTMENT.
BUT IT WAS THE RIGHT THING TO DO FOR BOTH AWS AND FOR CUSTOMERS.
SO WE BY EARLY 2015, STS HAS LAUNCHED REGIONAL ENDPOINTS IN
EVERY REGION GLOBALLY, AND AWS IS NOW ENCOURAGING CUSTOMERS TO
USE THEM OVER THE GLOBAL ENDPOINT. AND AS TIME PASSES, WE
SEE MOST CUSTOMERS ADOPTING THE REGIONAL ENDPOINTS AND THINGS
ARE GOING PRETTY WELL. BUT SLOWLY OVER TIME, IT STARTS TO
BECOME CLEAR THAT NOT EVERYTHING IS MIGRATING. IN SEPTEMBER LAST
YEAR, WE DID A DEEP DIVE INTO THIS QUESTION. ALTHOUGH REGIONAL
STS HAD BEEN AVAILABLE FOR NEARLY A DECADE, A SIGNIFICANT
SET OF YOU WERE STILL USING THE STS ENDPOINT AND NOT GETTING THE
BENEFITS OF THE REGIONAL ISOLATION THAT WE WANTED. WE
REALIZED THAT WE NEEDED TO CHANGE OUR APPROACH TO GET WHERE
WE WANTED TO BE. WE HAD MADE THE APIS AVAILABLE. WE HAD
RECOMMENDED THEM STRONGLY IN DOCUMENTATION AND BLOGGED ABOUT
THEM AND ALL THE THINGS WE COULD. BUT WE WEREN'T GETTING
THE TRANSITION WE WANTED. AND THE FUNDAMENTAL PROBLEM WE
REALIZED WAS THAT FOR A LOT OF CUSTOMERS, MIGRATION TO REGIONAL
APIS REQUIRED THEM TO TAKE ACTION. AND IT HAD BECOME CLEAR
THAT AT THE SCALE OF OUR CUSTOMER BASE, THAT WASN'T GOING
TO GET US THERE QUICKLY ENOUGH. >> SO YOU CAN SEE THE GLOBAL STS
API HERE RUNNING IN US EAST ONE AND THE OTHER 16, ENABLED BY
DEFAULT REGIONS AROUND THE WORLD THAT PEOPLE WERE CALLING IT
FROM. SO LET'S ZOOM IN ON NORTH AMERICA A BIT AND WE CAN SEE
WHAT'S HAPPENING A LITTLE MORE CLEARLY. SO EVEN FOR THE
WORKLOADS, RIGHT, IN ALL THESE OTHER REGIONS, CUSTOMERS ARE
REGULARLY MAKING CALLS BACK TO US EAST ONE TO GET THEIR SHORT
LIVED CREDENTIALS FROM THE STS GLOBAL ENDPOINT. OBTAINING THOSE
CREDENTIALS WAS INTRODUCING A CROSS-REGION DEPENDENCY, AND IT
DIDN'T PROVIDE THE REGIONAL ISOLATION THAT WE WANTED
WORKLOADS TO HAVE. NOW, THIS IS HOW WE WANT THINGS TO ACTUALLY
WORK. REQUESTS ORIGINATING FROM EACH REGION FOR STS CREDENTIALS
SHOULD BE ANSWERED LOCALLY IN THAT REGION. WE DON'T WANT
REQUESTS TO THE GLOBAL ENDPOINT TO ACTUALLY GO TO US EAST ONE
ANYMORE. SO A WORKLOAD IN US WEST TWO WHEN IT CALLS THE STS
GLOBAL ENDPOINT SHOULD BE ANSWERED IN US WEST TWO, AND THE
SAME FOR EVERY OTHER REGION ACROSS THE GLOBE. THIS WOULD
IDEALLY CREATE THE REGIONAL ISOLATION WE WANT IN STS,
WITHOUT CUSTOMERS IN THOSE REGIONS HAVING TO DO ANYTHING TO
GET IT. BUT THIS KIND OF CHANGE CAN BE COMPLEX. WE REALLY NEEDED
TO UNDERSTAND THE CUSTOMER REQUIREMENTS TO MAKE THE CHANGE
WITH NEAR PERFECT TRANSPARENCY. SO HERE'S WHAT WE DID.
>> SO AS YOU CAN IMAGINE, ONE OF THE BIGGEST CHALLENGES IN
RUNNING CLOUD SERVICES IS FIGURING OUT HOW TO MAKE BIG
CHANGES WHILE NO CUSTOMERS NOTICE. OUR GOAL HERE IS FOR
NEAR PERFECT TRANSPARENCY. SO LET'S LOOK AT SOME OF THE
REQUIREMENTS AND WHAT WE DID TO MEET THEM. THE CORE OF WHAT
WE'RE TRYING TO DO HERE IS THAT EVERY CUSTOMER GET THEIR STS
CALLS ANSWERED IN THEIR LOCAL REGION, WITHOUT THEM NEEDING TO
KNOW OR MAKE ANY CHANGES. THE FIRST THING WE DID WAS TO BUILD
OUT 16 PERFECT REPLICAS OF THE GLOBAL SERVICE, ONE IN EACH
REGION. SO WITH 17 REGION ENDPOINTS IN TOTAL, THE REGIONAL
SERVICE LOOKS SLIGHTLY DIFFERENT TO THE GLOBAL ONE FROM THE
PERSPECTIVE OF CLIENTS. SO WE COULDN'T JUST TRANSPARENTLY
REROUTE EVERYONE TO THE REGIONAL ENDPOINTS. WE DECIDED THAT FOR
NOW, THE SIMPLEST THING TO DO WAS TO ACTUALLY BUILD AND RUN A
GLOBAL ENDPOINT IN EVERY REGION. IT'S WORTH NOTING THAT AS OPT IN
REGIONS ALREADY ENFORCED USE OF REGIONAL STS, WE DIDN'T HAVE THE
PROBLEM THERE, SO WE DIDN'T NEED TO DO IT IN THOSE REGIONS. WHEN
WE SPOKE WITH THE IDENTITY EXPERTS, THEY ASKED US TO ENSURE
THAT THE CHANGE WAS MADE IN SUCH A WAY THAT WE WOULD ONLY EVER
ANSWER CLIENTS IN THE SAME REGION. SO, FOR EXAMPLE, GLOBAL
STS REQUESTS BEFORE WE STARTED WERE ANSWERED. IF THEY WERE MADE
IN SYDNEY WOULD HAVE BEEN ANSWERED IN VIRGINIA. WE WANTED
TO ANSWER THOSE REQUESTS LOCALLY IN SYDNEY, BUT NEVER TO ROUTE
THEM TO ANY OTHER REGION AND REQUESTS THAT ORIGINATE OUTSIDE
AN AWS REGION WOULD CONTINUE TO GO TO US EAST ONE AS BEFORE. SO
NOBODY GETS ANY SURPRISES IN TERMS OF WHERE THEIR REQUESTS
ARE BEING HANDLED. AND TO ACHIEVE THIS, WE USED AN
INTEGRATION WITH ROUTE 53 RESOLVER, THE DNS RESOLVER IN
YOUR VPC. WHEN YOU QUERY STS, AMAZON AWS.COM. NOW IN THOSE
VPCS, IT TRANSPARENTLY RETURNS YOU THE IPS OF THE NEW REGION
ENDPOINTS. WE MADE THIS CHANGE VERY CAREFULLY, ONE AZ AT A TIME
IN EACH REGION, AND MONITORED TO MAKE SURE ALL THE CUSTOMER
WORKLOADS WERE CONTINUING TO WORK AS NORMAL. ONE OF THE OTHER
MORE SUBTLE ASPECTS WE HAD TO THINK ABOUT WAS CLOUDTRAIL LOGS.
EVERY TIME YOU MAKE A CALL TO STS, AN AUDIT LOG IS DELIVERED
TO CLOUDTRAIL. MOST AWS CUSTOMERS CONFIGURE GLOBAL
CLOUDTRAIL LOGS SO THAT ALL OF THEIR LOGS ARE GLOBALLY
DELIVERED TO A SINGLE REGION THEY CHOOSE. BUT FOR CUSTOMERS
WHO DON'T, CLOUDTRAIL LOGS ARE GENERALLY EMITTED IN THE REGION
WHERE THE SERVICE IS RUNNING, AND AS GLOBAL STS WAS RUNNING UP
TO NOW IN US EAST ONE, ITS LOGS WERE BEING EMITTED IN US EAST
ONE. WE'RE GOING TO CHANGE WHICH REGION IT'S RUNNING IN. SO WE
NEEDED TO BE CAREFUL THAT EVEN WHILE WE ANSWER THAT STS REQUEST
IN, SAY, SYDNEY, WE MUST ENSURE THE LOGS STILL SHOWS UP IN US
EAST ONE AS NORMAL OR CUSTOMERS MAY NOTICE THE CHANGE. AND TO
ENSURE WE'RE BEING VERY OPEN ABOUT THIS AND TRANSPARENT, WE
ALSO ADDED AN EXTRA LINE OF METADATA INTO THE CLOUDTRAIL
LOG, SO YOU CAN SEE IN WHICH REGION WE ACTUALLY ANSWER THE
REQUEST. OVER THE LONGER TERM, OUR GOAL IS TO EVENTUALLY
DEPRECATE THIS GLOBAL ENDPOINT. THIS PROJECT IS ABOUT
ACCELERATING THE BENEFITS, GETTING YOU THAT REALLY EARLY.
WHILE WE SLOWLY WORK ON THE DEPRECATION. THE NEWER AWS SDKS
WERE ALL ALREADY DEFAULTING TO THE GLOBAL TO EXCUSE ME, TO THE
REGIONAL STS ENDPOINT, BUT WE REALIZED THAT A SIZABLE PORTION
OF CUSTOMERS WERE USING OLDER MAJOR VERSIONS OF THE SDKS, AND
THEY WERE DEFAULTING TO THE GLOBAL ENDPOINT. SO AS PART OF
THE LONG TERM STRATEGY, WE'RE ALSO UPDATING ALL SUPPORTED
SDKS, EVEN THE OLDER ONES, TO ENSURE THEY DEFAULT TO THE
REGIONAL ENDPOINT. NOW, THAT'S GOING TO TAKE A FEW YEARS TO
TAKE ITS FULL EFFECT, BECAUSE PEOPLE PICK UP SDK CHANGES AT
VARYING RATES. WE SET NEAR PERFECT TRANSPARENCY BECAUSE
IT'S NOT GOING TO BE PERFECTLY TRANSPARENT. THERE WAS AT LEAST
ONE ONE CASE WHERE WE FIGURED IT WOULDN'T MATTER. SO A REQUEST
THAT GETS ANSWERED OR THAT ORIGINATES IN VIRGINIA, EXCUSE
ME, IN SYDNEY UP TO NOW, WOULD HAVE TAKEN A SIZABLE PORTION OF
TIME GOING TO VIRGINIA AND BACK. AND WHEN WE START ANSWERING IT
IN SYDNEY, IT'S GOING TO BE ANSWERED MUCH FASTER. BUT WE
WERE PRETTY CONFIDENT THAT MOST CUSTOMERS WOULD BE COMFORTABLE
WITH THAT. >> SO IN APRIL OF THIS YEAR, WE
PUBLICLY ANNOUNCED THAT THIS WORK HAD BEEN COMPLETED. STS HAS
DEPLOYED A DEDICATED STS GLOBAL ENDPOINT IN EVERY ENABLED BY
DEFAULT REGION AND REROUTED TRAFFIC TO IT WITHIN THOSE
REGIONS FROM YOUR VPCS. SO NOW THAT IT'S DONE, WHAT WAS THE
EFFECT? AS THE WORK PROCEEDED THROUGH THIS PROJECT, THE STS
TEAM TRACKED THE NUMBER OF UNIQUE ACCOUNTS THAT WE'RE
SEEING MAKING CROSS-REGION CALLS BACK TO THE STS GLOBAL ENDPOINT
IN US EAST ONE. SO IN ORDER TO SHOW THIS, WE'VE NORMALIZED IT
AS A PERCENTAGE OF THE TOTAL CALLERS ON THE Y AXIS. SO AS YOU
CAN SEE, AS THE PROJECT STARTS, WE'RE OSCILLATING BELOW 100%.
AND AS THE PROJECT PROCEEDS FROM ABOUT MID-MARCH TO ABOUT
MID-APRIL, THIS NUMBER DROPS DOWN PRACTICALLY TO ZERO. MANY
THOUSANDS OF ACCOUNTS, WHICH WERE DEPENDENT ON CROSS-REGION
CALLS TO STS ARE NOW BEING ANSWERED LOCALLY. THEY'RE
GETTING THIS UPDATE AND THIS BENEFIT WITH NO CHANGES MADE ON
THEIR PART. >> I MENTIONED BEFORE THAT THE
OTHER NICE SIDE EFFECT OF THIS WAS GOING TO BE IN TERMS OF
LATENCY OF CALLS TO STS AS OBSESSIVE OPERATORS. THE STS
TEAM ALREADY HAD MONITORING ON THE PERFORMANCE OF CALLS TO
GLOBAL STS FROM EVERY REGION. SO AS CALLS PREVIOUSLY HAD HAD TO
TRANSIT FROM PLACES LIKE SINGAPORE AND SYDNEY TO NORTH
VIRGINIA, THE LATENCY OF RESPONSES TENDED TO BE DOMINATED
BY THE NETWORK. ROUND TRIP TIME. YOU CAN SEE THE STS TEAM'S
MEASUREMENTS OF LATENCY CHANGE AS THE PROJECT PROCEEDS. IT WAS
REALLY SATISFYING TO SEE CALLS OF UP TO 230 MILLISECONDS
DROPPING DRAMATICALLY BY THE END. EVERY DEFAULT REGION NOW
CONSISTENTLY SEES RESPONSES IN UNDER 20 MILLISECONDS. YOU GOT
TO KEEP IN MIND THAT THERE ARE MILLIONS OF CALLS GOING TO STS
EVERY SECOND THAT ARE BENEFITING FROM THIS, SO IT'S A REALLY BIG
CHANGE. SO THE FIRST AVAILABILITY AXIOM WAS ENSURING
OUR REGIONS OPERATE INDEPENDENTLY. AND FOCUSING ON
THIS AXIOM DROVE A MAJOR CHANGE TO ENSURE THAT ALL CUSTOMERS OF
STS HAVE THEIR CALLS ANSWERED IN REGION, EVEN THOSE CUSTOMERS
STILL CALLING THE GLOBAL STS ENDPOINT. AND SO THIS HAS
SIGNIFICANTLY IMPROVED BOTH THE RELIABILITY AND PERFORMANCE OF
STS FOR EVERYONE. >> THE NEXT AXIOM THAT WE WANT
TO TALK ABOUT IS HOW WE'RE MAKING OUR SERVICES AND CUSTOMER
WORKLOADS MORE RESILIENT TO AZ IMPAIRMENTS AND GRAY FAILURES.
TO SEE WHAT WE'VE DONE HERE, WE NEED TO SPEND A LITTLE BIT OF
TIME TALKING ABOUT A SERVICE CALLED AWS LAMBDA. LIKE RS,
LAMBDA HAS BECOME A CRITICAL SERVICE TO BOTH OUR CUSTOMERS
AND OUR OWN SERVICES. I REMEMBER WHEN I WAS A CUSTOMER AND AMODEI
AWS LAMBDA LAUNCHED IN 2014. BACK THEN, I WAS KIND OF A
NEWBIE ON AWS AND I DIDN'T REALLY UNDERSTAND HOW OR WHY TO
USE IT. I WAS STILL HUMMING ALONG ON EC2 INSTANCES, BUT IF
WE FAST FORWARD NOW, OVER A DECADE LATER, AWS LAMBDA HAS
BECOME A CORE COMPUTE PRIMITIVE FOR EVENT DRIVEN ARCHITECTURES.
I USE IT IN ALMOST EVERYTHING THAT I BUILD. IT'S A CORE
COMPONENT OF A NUMBER OF AWS SERVICES AND RUNS MISSION
CRITICAL WORKLOADS FOR CUSTOMERS. BUT TO GET TO THAT
POINT, WE'VE HAD TO INNOVATE A LOT TO MAKE IT RESILIENT. LAMBDA
OPERATES AT A MASSIVE SCALE. THEIR FLEETS OFTEN HAVE TENS OF
THOUSANDS OF NODES IN THEM. THESE ARE THE FLEETS WHERE WE
RUN YOUR LAMBDA CODES. AND LIKE EVERYWHERE ELSE IN AWS, WE
REALLY CARE ABOUT YOUR CUSTOMER EXPERIENCE. IT'S OUR TOP
PRIORITY. THIS MAKES MANAGING THE HEALTH OF THESE FLEETS
CRITICAL TO THIS EXPERIENCE THAT LAMBDA DELIVERS. BUT
OCCASIONALLY THERE CAN BE ONE SINGLE INSTANCE THAT BECOMES
IMPAIRED. IT DOESN'T ALWAYS OUTRIGHT FAIL. MAYBE IT STARTS
MISBEHAVING, BECOMING A LITTLE SLOWER, OR PRODUCING MORE ERRORS
THAN THE REST. AND WHEN YOU HAVE WHAT, JUST ONE BAD NODE IN A
FLEET OF 10,000, IT CAN REDUCE THE AVAILABILITY OF THAT FLEET
TO 99.99%. AND LAMBDA IS AVAILABILITY. GOALS ARE ACTUALLY
MUCH MORE AGGRESSIVE THAN THIS. THESE BAD NODES THAT ARE LIMPING
ALONG NOT COMPLETELY FAILED, CAN ALSO BE REALLY HARD TO FIND, AND
THAT CAN MAKE RECOVERY REALLY DIFFICULT AND REQUIRE MANUAL
INTERVENTION. AND WE WANTED LAMBDA TO BE MORE RESILIENT WHEN
THESE TYPES OF FAILURES OCCUR. AUTOMATED DETECTION AND RECOVERY
IS ESSENTIAL TO MAKING MAINTAINING REALLY HIGH
AVAILABILITY NUMBERS. SO TO DO THIS, WE NEEDED TO CHANGE OUR
APPROACH TO FLEET HEALTH OBSERVABILITY.
>> SO ESPECIALLY WITH BIG SERVICES LIKE LAMBDA, ONE OF THE
THINGS WE'VE LEARNED IS THAT, AS ONE CUSTOMER PUT IT IN A BLOG
SOME YEARS AGO, YOUR NINES ARE NOT MY NINES. A SINGLE NODE IN A
FLEET OF 10,000 EXPERIENCING FAULTS MIGHT, AS MIKE SAYS, ONLY
DROP THE SERVICE'S AVAILABILITY BY 100TH OF 1% IF YOU CONSIDER
IT AS A SINGLE AGGREGATE NUMBER. BUT THAT DOESN'T ALWAYS CAPTURE
THE REAL CUSTOMER EXPERIENCE. WHAT MAY BE HAPPENING IN
PRACTICE IS THAT MOST CUSTOMERS HAVE PERFECT AVAILABILITY AND A
SMALL NUMBER HAVE LESS. SO IF YOU AS A CUSTOMER HAPPEN TO BE
UNLUCKY AND THAT SINGLE BAD NODE IS HOSTING MANY OF YOUR LAMBDA
INVOCATIONS, YOU COULD BE HAVING A WORSE TIME. SO THIS IS WHY IT
IS SO IMPORTANT THAT WE OBSESS OVER EVEN THAT ONE INSTANCE IN
10,000. WE NEED TO DETECT AND REMEDIATE THIS VERY FAST. AND
WHEN THESE KIND OF SITUATIONS OCCUR, YOU MIGHT THINK, WELL, WE
JUST NEED BETTER HEALTH CHECKS, RIGHT? IT'S SIMPLE. AND WHEN A
HOST FAILS UNAMBIGUOUSLY, AS IN JUST STOPS ANSWERING COMPLETELY,
DIES WHATEVER. A SIMPLE SHALLOW HEALTH CHECK WORKS GREAT WHEN A
HOST IS DOWN OR STOPS RESPONDING. WE CAN DETECT IT AND
REMOVE IT EASILY. AND IF THE HOST IS ACTUALLY DOWN, YOU'RE
NOT GOING TO MAKE THE SITUATION WORSE BY AUTOMATICALLY REMOVING
IT. BUT THE SCENARIO WE'RE DEALING WITH HERE IS WHAT WE
CALL A GRAY FAILURE. THE HOST IS STILL ANSWERING. IT'S DOING SO
SLOWLY, INTERMITTENTLY, OR WITH UNUSUALLY HIGH ERROR RATES,
DEPENDING ON HOW WE'VE DEFINED THIS. AND THE HEALTH STATE OF
THE HOST IS SORT OF KIND OF AMBIGUOUS. IN PARTICULAR,
THERE'S A RISK HERE IF WE REMOVE A LOT OF GRAY FAILING NODES THAT
ARE ACTUALLY ANSWERING CUSTOMERS. AT THE SAME TIME, WE
COULD MAKE THE SITUATION WORSE. SO ONE APPROACH IS TO EMPLOY
DEEP HEALTH CHECKS THAT EXAMINE THE HOST APPLICATION MORE
STRINGENTLY TO IDENTIFY THESE GRAY FAILURES. IN DOING THIS,
HOWEVER, WE HAVE TO BE CAREFUL NOT TO SET OURSELVES UP FOR AN
OVERREACTION. THE HOSTS ARE STILL DOING SOME GOOD WORK,
AUTOMATICALLY REMOVING A LARGE NUMBER OF HOSTS MIGHT TURN A
MILD EVENT INTO ACTUALLY A MORE SEVERE ONE, AND THIS LEAVES US
WITH KIND OF A HARD DECISION. HOW DEEP CAN MY HEALTH CHECKS
SAFELY BE? IT TURNS OUT THIS IS A REALLY DIFFICULT QUESTION TO
ANSWER. SO WHEN WE'RE DEALING WITH GRAY FAILURES, SIMPLE
BINARY HEALTH CHECKS ARE REALLY HARD TO GET RIGHT. WHAT WE'VE
LEARNED IS THAT HEALTH CHECKS REALLY NEED TO BE HEALTH CHECKS
IN GRAY. FAILURE SCENARIOS NEED TO BE BUDGETED, SO WE LIMIT HOW
MUCH WE'RE WILLING TO TAKE OUT AUTOMATICALLY. AND THEY NEED TO
BE NON BINARY COMPARISONS. NO HOST IS EVER TRULY PERFECT. SO
WE NEED TO RANK AND REMOVE THE WORST HOSTS ONLY. SO LOOKING
MORE CLOSELY AT THIS TOPIC OF BUDGETING SUPPOSE TO DEAL WITH
GRAY FAILURES. WE SAY ANY INCIDENT IS UNHEALTHY IF ITS
ERROR RATE EXCEEDS SOME NUMBER. AND IN THIS CONTEXT, LET'S SAY
WE CHOOSE 4000 AND MAYBE THESE HOSTS ALL HAVE SOME SHARED
DEPENDENCY. AND MAYBE SOMEDAY THAT DEPENDENCY HAS A PROBLEM.
NOW ALL THE HOSTS START FAILING AT HIGH RATES. THEY MAY STILL BE
DOING SOME USEFUL WORK, SO WE WOULDN'T WANT TO TAKE THEM OUT
BECAUSE WE'LL TAKE THEM ALL OUT. SO WE HAVE TO HAVE A BUDGET.
MAYBE IT'S OKAY TO TAKE 10 OR 20% OF THEM OUT, MAYBE 30%, BUT
WE CAN'T TAKE THEM ALL OUT. I MENTIONED ALSO THAT IT'S A NON
BINARY COMPARISON. YOU CAN SEE PRETTY CLEARLY IN THIS PICTURE
THAT THE YELLOW INSTANCE IS VERY DIFFERENT. IT IS AN OUTLIER
RELATIVE TO THE REST. AND THAT'S NOT REALLY BASED AS A HUMAN
LOOKING. THAT'S NOT BASED ON THERE BEING SOME MAGIC THRESHOLD
WE CHOSE. IT'S BECAUSE YOU CAN SEE IT'S AN OUTLIER. THAT'S WHY
YOU WOULD TAKE THAT OUT. SO THIS IS WHAT WE NEED TO BUILD INTO
OUR SYSTEMS. WE'RE LOOKING EFFECTIVELY FOR THE NEEDLE IN
THE HAYSTACK. WE WANT TO FIND A SINGLE INSTANCE WITH PROBLEMS
WHILE THE OTHERS ARE FINE. SO HOW DO WE DO THAT IN PRACTICE?
HOW CAN WE COMPARE METRICS LIKE ERRORS AMONG A POOL OF TENS OF
THOUSANDS OF NODES? >> IN AWS SERVICES? WE SEND
STRUCTURED LOG FILES TO CLOUDWATCH LOGS, TYPICALLY USING
OUR EMBEDDED METRIC FORMAT THAT ALLOWS US TO COMBINE LOGGING AND
METRICS INTO A SINGLE PANE OF GLASS. AND BECAUSE WE HAVE
METRIC DATA IN OUR LOG FILES, WE CAN ANALYZE THAT DATA USING
CLOUDWATCH CONTRIBUTOR INSIGHTS. CONTRIBUTOR INSIGHTS ACTUALLY
STARTED AS AN INTERNAL AWS MONITORING TOOL THAT WE MADE
AVAILABLE TO CUSTOMERS AS WELL. IF YOU WERE TO ASK OUR SERVICE
TEAMS, YOU'D PROBABLY FIND THAT CONTRIBUTOR INSIGHTS IS ONE OF
THEIR FAVORITE FEATURES OF CLOUDWATCH. AND THAT'S BECAUSE
AT OUR SCALE, IT HELPS US UNDERSTAND INDIVIDUAL CUSTOMER
EXPERIENCES WITHOUT HAVING NEEDING TO HAVE AN INDIVIDUAL
CUSTOMER METRIC. IT'S WAY MORE COST EFFICIENT AND ALLOWS US TO
FOCUS ON JUST THE TOP END NUMBER OF CONTRIBUTORS. AND
CONTRIBUTORS. SO CONTRIBUTOR INSIGHTS ALLOWS US TO LOOK AT
VERY HIGH CARDINALITY DATA. AND THAT MEANS THAT WHEN THERE ARE A
LOT OF UNIQUE KEYS LIKE INSTANCE IDS, AND IT ALLOWS US THEN TO
GRAPH THE TOP N CONTRIBUTORS TO SOME DATA POINT FROM THOSE LOG
FILES, AND IT PRODUCES VISUALIZATIONS LIKE THIS. SO NOW
WE HAVE A METHOD TO VISUALLY DETECT WHEN THERE'S AN OUTLIER
FOR ERROR ACCOUNTS AND A FLEET OF THOUSANDS OF NODES. BUT WE
REALLY WANT TO DO MORE THAN VISUALIZE THIS, RIGHT. WE WANT
TO TAKE ACTION ON IT, AND WE WANT TO DO THAT WITH AUTOMATION.
SO IN AWS LAMBDA, WE BUILT A FLEET HEALTH SERVICE. WE HAVE
OUR WORKER NODES HERE ON THE LEFT, AND THEY SEND IN THEIR LOG
FILES TO CLOUDWATCH LOGS. AND YOU CAN SEE THIS LOG FILE HERE
INCLUDES DATA LIKE FAULT COUNTS AND THE INSTANCE ID. THEN WE
ANALYZE THAT DATA WITH CONTRIBUTOR INSIGHTS RULES. AND
IN THIS CASE WE'RE LOOKING AT INDIVIDUAL INSTANCE CONTRIBUTORS
TO FAULT COUNT. NEXT WE CAN CREATE A CLOUDWATCH METRIC MATH
ALARM ON THOSE RULES. IN THIS RULE WE'RE LOOKING FOR ANY
CONTRIBUTOR THAT'S RESPONSIBLE FOR 66% OF THE TOTAL ERRORS THE
SERVICE IS SEEING. WHAT WE FOUND IS THAT LARGE PERCENTAGES LIKE
THIS WORK PRETTY WELL. THEY'RE FAIRLY CONSERVATIVE, WHICH MEANS
THEY'RE NOT GOING TO PRODUCE A LOT OF FALSE POSITIVES, BUT EVEN
A LARGE VALUE LIKE 60 TO 70% WILL RELIABLY FIND UNHEALTHY
INSTANCES. ONCE THE ALARM IS TRIGGERED, A WORKER NODE
RESPONDS. SO THE FIRST THING IT DOES IS FIND THAT TOP
CONTRIBUTORS ID, THEN IT'S GOING TO CHECK ITS TOKEN BUCKET. AND
THIS IS A REALLY IMPORTANT STEP. WE DON'T WANT AUTOMATION LIKE
THIS TO RUN AWAY DUE TO SOME BUG OR AN UNFORESEEN CONDITION. SO
THERE'S A VELOCITY CONTROL ON HOW MANY TIMES THE HEALTH
SERVICE CAN DO THIS WITHIN A DEFINED PERIOD OF TIME. SO AFTER
WE ENSURE THAT THERE'S ENOUGH CAPACITY WITHIN OUR TOKEN
BUCKET. IF THE UNHEALTHY INSTANCE IS PART OF AN AUTO
SCALING GROUP, WE'LL CALL THE SET INSTANCE HEALTH API AND LET
AUTO SCALING DO ITS THING. OTHERWISE, THIS WORKER NODE
TERMINATES THE UNHEALTHY INSTANCE AND LETS THE SERVICE
THAT OWNS IT CONTROL REPLACING IT. AND SO THIS IS A SERVICE
THAT STARTED AS SOMETHING LAMBDA BUILT, BUT IT'S ACTUALLY BECOME
A GENERAL PURPOSE HEALTH SERVICE THAT LOTS OF AWS SERVICES TAKE
ADVANTAGE OF. IT'S AN EXAMPLE OF HOW A LOT OF INNOVATION COMES TO
BE AT AWS AND INDIVIDUAL TEAM, OR EVEN A SINGLE ENGINEER HAS A
GREAT IDEA, AND THEY BUILD SOMETHING THAT'S USEFUL FOR LOTS
OF DIFFERENT TEAMS, AND IT ENDS UP MAKING US ALL MORE RESILIENT.
ANOTHER CHALLENGE THAT WE SAW IN LAMBDA WERE GRAY FAILURES
AFFECTING SINGLE AVAILABILITY ZONES. AND THIS HAPPENS WHEN
SOME EVENT IMPACTS THE LATENCY OR AVAILABILITY OF RESOURCES IN
THAT ZONE. AND WHILE WE RUN YOUR LAMBDA INVOCATIONS ACROSS
MULTIPLE AZS, EVEN A SMALL IMPAIRMENT OF A SINGLE ZONE
COULD HAVE SIGNIFICANT IMPACT FOR YOUR RESOURCES. AND SO WE
REALIZED THAT THE SAME OUTLIER DETECTION APPROACH WOULD WORK
EQUALLY WELL TO FIND IMPAIRED AZS. IT'S ACTUALLY BASICALLY THE
SAME PROBLEM, BUT WITH A MUCH LOWER CARDINALITY. SO LAMBDA
ALSO BUILT A ZONAL EVENT DETECTOR. IT LOOKS AT DIFFERENT
METRICS TO TRY TO DETECT WHEN AN AZ MUST BE UNHEALTHY. AND THIS
ALLOWS LAMBDA TO SHIFT AWAY FROM THAT ZONE AND ONLY INVOKE YOUR
FUNCTIONS IN AZS THAT THEY ACTUALLY OBSERVE TO BE HEALTHY.
SO JUST LIKE THE FLEET HEALTH SERVICE, ZONAL EVENT DETECTOR
WORKS SO WELL THAT OTHER SERVICES WANTED TO USE IT TO.
AND SO WE WANTED TO BUILD MORE AND MORE CAPABILITIES THAT COULD
USE THIS TELEMETRY TO DETECT SINGLE AZ IMPAIRMENTS. AND SO
ONE OF THE THINGS THAT WE LAUNCHED IN 2022.
>> IS ZONAL SHIFT. AND SO ZONAL SHIFT IS AN API THAT ALLOWS US
TO MANUALLY OR AUTOMATICALLY SHIFT TRAFFIC AWAY FROM A
SPECIFIED AVAILABILITY ZONE. COMBINED WITH ZONAL EVENT
DETECTOR, WE NOW HAVE COMMON SIGNALS AND STANDARD TOOLS FOR
ALL OUR SERVICE TEAMS TO BE ABLE TO USE TO SHIFT AWAY FROM A
ZONE, AND WE ALSO PUT THIS CAPABILITY IN THE HANDS OF
CUSTOMERS. SO YOU CAN YOU CAN USE THIS TOOL AS WELL TO SHIFT
AWAY FROM A ZONE IF YOU IF YOU NEED TO. AND WE NOW SUPPORT ALL
NLBS AND ALBS AS WELL AS AUTO SCALING GROUPS AND EKS CLUSTERS.
SO WHEN YOU TRIGGER A ZONAL SHIFT WE INFORM BOTH THE DNS AND
LOAD BALANCING LAYERS. SO TRAFFIC STOPS BEING SENT FROM
THE OUTSIDE TO THE AZ. YOU SHIFTED FROM. AUTO SCALING SENDS
ALL NEW INSTANCE SCALING AND REPLACEMENTS TO THE REMAINING
ZONES. AND ECS LEARNS AND REMOVES ENDPOINT SLICES AND
STOPS SCHEDULING NEW PODS IN THE IMPAIRED ZONE. FOR SAFETY, WE
STRICTLY LIMIT SHIFTS TO ONE'S OWN AT A TIME. AGAIN, WE DON'T
WANT TO GO OVER BUDGET AND TAKE TOO MUCH OUT. WE ALSO RECOGNIZE
THAT FOR CUSTOMERS, DETECTING A ZONAL EVENT TO USE THIS TOOL WAS
KIND OF DIFFICULT. SO WE SAW ANOTHER OPPORTUNITY TO SHIFT
MORE RESPONSIBILITY TO OUR SIDE. SO WE'RE IN A POSITION TO LOOK
AT A BROADER SET OF TELEMETRY, INCLUDING OUR INFRASTRUCTURAL
METRICS, ALLOWING US TO BETTER DETECT THESE EVENTS AND RESPOND
MORE QUICKLY ON BEHALF OF CUSTOMERS. SO WE USE THE ZONAL
EVENT DETECTOR SERVICE THAT WAS BORN IN AWS LAMBDA AS A SYSTEM
WIDE SOURCE OF TELEMETRY ABOUT AZ HEALTH TO POWER ZONAL
AUTOSHIFT. WITH AUTOSHIFT, WE MANAGE ALL OF THE OBSERVABILITY
AND PERFORM THE SHIFT ON YOUR BEHALF. SO WHEN ONE OF OUR
METRICS LIKE CONNECTION FAILURES OR NETWORK REACHABILITY, SHOW
THAT ONE AZ IS AN OUTLIER, THE ZONAL EVENT DETECTOR GOES INTO
ALARM AND WE TRIGGER A ZONAL SHIFT. AGAIN, WE STRICTLY LIMIT
THIS TO ONE AZ AT A TIME. AND WE USE THIS CAPABILITY FOR BOTH AWS
SERVICES AND NOWADAYS FOR CUSTOMER APPLICATIONS AS WELL.
SO THIS MEANS THAT WHEN YOU'RE INTERACTING WITH A REGIONAL AWS
SERVICE LIKE SQS OR LAMBDA, WE'RE AUTOMATICALLY RESPONDING
TO THE EVENT SO THAT THE AWS SERVICES RECOVER QUICKLY. BUT
FOR RESOURCES THAT YOU ENROLL IN AUTOSHIFT, THE WE PERFORM THE
SAME ACTION AT THE SAME TIME. AND THIS PROVIDES A SINGLE
AUTOMATED RESPONSE DRIVEN BY ZONAL EVENT DETECTOR AND ENSURES
WE'RE ALL ABLE TO MAKE BEST USE OF AVAILABILITY ZONES. SO THE
SECOND AVAILABILITY AXIOM WAS RESILIENCE TO AZ IMPAIRMENTS AND
IN PARTICULAR TO GRE FAILURES. AND THE AXIOM INSPIRED THE
LAMBDA FLEET'S LAMBDAS, FLEET HEALTH SERVICE ZONAL EVENT
DETECTOR, AS WELL AS ZONAL SHIFT AND ZONAL AUTOSHIFT.
>> ALL RIGHT, THE NEXT AXIOM THAT WE'RE GOING TO LOOK AT IS
TESTING RIGOROUSLY. AT AWS. WE COMMONLY SAY IF YOU HAVEN'T
TESTED SOMETHING IN THE PAST WEEK, IT'S BROKEN. REGULAR
TESTING IS PART OF OUR OPERATIONAL CULTURE. IN ORDER TO
USE ZONAL SHIFT, WE ALSO ENROLL YOU IN PRACTICE MODE ON A WEEKLY
BASIS. WE PERFORM AN AUTOSHIFT TO MAKE SURE ALL OF OUR SERVICE
TEAMS AND CUSTOMERS ARE READY AND CAN ACTUALLY PERFORM A ZONAL
SHIFT WITHOUT IMPACT TO THEIR SERVICES AND APPLICATIONS. WE
REGULARLY DO THIS IN OUR PRODUCTION FLEETS, AND IT HELPS
BUILD THE CONFIDENCE THAT ALL OF THIS IS GOING TO WORK WHEN WE
NEED IT. JUST LIKE A FIELD GOAL KICKER, THEY PRACTICE OVER AND
OVER, GOING THROUGH THE SAME MECHANICS EVERY TIME. AND SO
WHEN IT'S GAME DAY, NOTHING CHANGES. THEY DO THE EXACT SAME
THING THAT THEY'VE BEEN PRACTICING ALL ALONG. FIELD GOAL
KICKERS TEND TO MAKE MOST OF THEIR EXTRA POINTS. IN FACT,
IT'S A STAT OF SOMETHING LIKE 95% SUCCESS. AND THAT'S BECAUSE
PRACTICE IS SO SIMILAR TO THE REAL THING. IT'S ALL MUSCLE
MEMORY AT THAT POINT. THEY HAVE A HIGH DEGREE OF CONFIDENCE THAT
THEY'RE GOING TO MAKE THE KICK, BECAUSE THEY DID IT OVER AND
OVER IN PRACTICE. IN MY UNDERGRADUATE DEGREE, I WAS A
PHILOSOPHY MAJOR, AND I NEVER IMAGINED THAT I'D BE STANDING ON
A STAGE QUOTING PHILOSOPHY, ESPECIALLY WORKING IN A
TECHNOLOGY COMPANY. BUT I THINK OUR MENTAL MODEL IS CAPTURED
REALLY WELL BY THIS QUOTE. WE ARE WHAT WE REPEATEDLY DO.
EXCELLENCE, THEN, IS NOT AN ACT, BUT A HABIT. AND WHILE WILL
DURANT AND ARISTOTLE WERE TALKING ABOUT ETHICS, I DO THINK
THIS WAY OF THINKING IS EQUALLY APPLICABLE FOR SO MANY DIFFERENT
DISCIPLINES. BY PRACTICING OVER AND OVER, WE BUILD UP THIS IDEA
OF EXCELLENCE. WHEN THE REAL THING HAPPENS, IT'S JUST MUSCLE
MEMORY. >> SO ONE OF THE WAYS WE DO THIS
INTERNALLY AT AWS IS WITH GAME DAYS. WE WANT TO KNOW THAT OUR
SERVICES ARE RESILIENT TO A VARIETY OF DIFFERENT FAILURE
MODES. SO INDIVIDUAL TEAMS WILL NATURALLY TEST THEIR SERVICE IN
PRE-PRODUCTION ENVIRONMENTS. BUT WHEN WE BUILD NEW AWS REGIONS
BEFORE THE REGIONS GO LIVE, WE TAKE A COUPLE OF WEEKS OUT AND
WE RUN A SERIES OF GAME DAY TESTS ON THE PRODUCTION SERVICES
THAT ARE READY FOR CUSTOMERS, BUT NOT YET SERVING CUSTOMERS,
AND THIS GIVES US AN OPPORTUNITY TO TEST IN REAL PRODUCTION
ENVIRONMENTS AT FULL SCALE WITHOUT IMPACTING ANY CUSTOMERS.
WE TEST A VARIETY OF THINGS LIKE POWER OUTAGES, NETWORK
PARTITIONS, SERVICE FAILURES, AND THESE HELP US VALIDATE HOW
SERVICES BEHAVE DURING FAILURES AS WELL AS HOW THEY RECOVER
AFTERWARD. THEY ALSO PROVIDE AN OPPORTUNITY FOR OUR ON CALL
ENGINEERS TO TRAIN FOR REAL EVENTS, EXERCISE AND REFINE OUR
RUNBOOKS, AND ENSURE THE REQUIRED OBSERVABILITY IS IN
PLACE. IT HELPS VALIDATE OUR COMPETENCE. ENVELOPE. PROFESSOR
DAVID WOODS AT OHIO STATE UNIVERSITY TALKS ABOUT
SOCIOTECHNICAL SYSTEMS AS HAVING A COMPETENCE ENVELOPE. THIS IS
AN ANALOGY WITH AN AIRCRAFT'S FLIGHT ENVELOPE, WHICH DEFINES
THE SAFE BOUNDARIES IN WHICH AN AIRCRAFT CAN OPERATE IN TERMS OF
ALTITUDE AND SPEED, AND PILOTS KNOW TO STAY WITHIN THAT
ENVELOPE AS IT'S THE SAFE, TESTED SPACE. IF THEY GO OUTSIDE
IT, THINGS BECOME MORE UNCERTAIN, AND SIMILARLY, A
COMPETENCE ENVELOPE FOR A SYSTEM IS THE SPACE WITHIN WHICH IT IS
KNOWN TO FUNCTION COMPETENTLY OR ROBUSTLY WHEN PUSHED BEYOND ITS
COMPETENCE, ENVELOPE SYSTEMS MAY STILL FUNCTION, BUT THINGS CAN
BECOME MORE PRONE TO FAILURE, AND WE HAVE UNCERTAINTY. AND
THIS COMPETENCE ENVELOPE CAN BE A USEFUL MENTAL MODEL TO THINK
ABOUT. TEST COVERAGE. IF YOU THINK ABOUT A SYSTEM YOU OWN AND
ALL OF THE MODES YOU EXPECT IT TO BE ABLE TO OPERATE IN, SUCH
AS FALLBACK PATHS OR FAILURES OF HOSTS. FAILOVERS OF HOSTS. WHEN
WAS THE LAST TIME YOU TESTED ALL OF THOSE MODES? ARE YOU STILL
CONFIDENT THAT THEY ARE ALL STILL WITHIN YOUR COMPETENCE
ENVELOPE? THEY'RE WELL TESTED. >> AND SO WE CAN TAKE THIS IDEA
OF A COMPETENCE ENVELOPE AND GENERALIZE IT TO DISTRIBUTED
SYSTEMS AS WELL. THERE'S A SET OF INPUTS, PARAMETERS AND
FAILURE MODES THAT OUR SYSTEMS OPERATE IN WITH EXPECTED
BEHAVIOR. WE'RE GENERALLY NOT NEAR THOSE COMPETENCE ENVELOPE
BOUNDARIES, SO WE'RE COMFORTABLE AND SAFE. BUT AS WE START TO
MOVE TOWARDS THE EDGE OF OUR COMPETENCE ENVELOPE AND EVEN
BEYOND IT, BY BEING EXPOSED TO OPERATING MODES THAT WE HAVEN'T
SEEN BEFORE, WE ENTERED THIS DANGER ZONE WHERE OUR SYSTEMS
START TO EXHIBIT UNEXPECTED BEHAVIOR TO UNKNOWN CONDITIONS.
IN OUR CASE, IT'S NOT FLYING OUTSIDE OF A MAX ALTITUDE OR TOP
SPEED. IT'S DEALING WITH FAILURE MODES THAT WE HAVEN'T PRACTICED
WITH. THIS IS WHERE SURPRISES HAPPEN AND WHERE DANGER OCCURS.
SO OUR COMPETENCE ENVELOPE IS DEFINED BY THE FAILURES WE
REGULARLY TEST AGAINST. AND WHILE I'M SHOWING OUR COMPETENCE
ENVELOPE, WHILE I'M SHOWING OUR OPERATING MODE EXPANDING, IT'S
ALSO TRUE THAT THE COMPETENCE ENVELOPE COULD BE SHRINKING IF
WE'RE NOT REGULARLY TESTING AGAINST THESE OPERATING MODES,
WE MIGHT NOT GET THE EXPECTED BEHAVIOR THAT WE WANT. AND SO
HOW DO WE MAINTAIN OR EVEN EXPAND THAT COMPETENCE ENVELOPE?
WELL, IN AIRPLANES YOU CAN'T REALLY CHANGE THE
CHARACTERISTICS OF THE FLIGHT ENVELOPE, RIGHT? WITH A PLANE,
WE'RE PRETTY MUCH STUCK WITH A MAX ALTITUDE AND TOP SPEED THAT
IT HAS. BUT FORTUNATELY IN DISTRIBUTED SYSTEMS, OUR ABILITY
TO PRODUCE EXPECTED BEHAVIOR IS DYNAMIC. BY REGULARLY TESTING
AND VALIDATING EXPECTED BEHAVIOR, WE CAN MAINTAIN THE
SIZE OF OUR COMPETENCE ENVELOPE, AND WE CAN GROW IT BY REGULARLY
EXPOSING OUR SERVICES TO A GROWING SET OF FAILURE MODES
THAT WE MIGHT ENCOUNTER. WE WANT FEWER SURPRISES. WE WANT
PREDICTABLE BEHAVIOR, AND WE DO THAT BY TURNING UNKNOWNS INTO
KNOWNS THROUGH CONSTANT TESTING. AND SO THIS IS PART OF THE TEST
REGULARLY, RIGOROUSLY AXIOM. IF WE HAVEN'T TESTED IT RECENTLY,
IT'S BROKEN. IT WON'T WORK AS EXPECTED. AND WE'LL FIND
OURSELVES IN DANGER OF NOT MEETING OUR GOALS. SO DOING GAME
DAYS FOR INDIVIDUAL TEAMS AND PRE-PRODUCTION ENVIRONMENTS AND
OUR NEW REGION BUILDS IS A GREAT START. BUT WE ONLY HAVE SO MANY
NEW REGION BUILDS GOING ON AT THE SAME TIME, AND THERE'S A
SHORT WINDOW BETWEEN WHEN ALL OF OUR SERVICES ARE READY AND WHEN
WE WANT TO GO GA AND START ONBOARDING CUSTOMERS. AND SO WE
WANT TO DO THIS TYPE OF TESTING WITH MUCH GREATER VELOCITY, WITH
ALL OF OUR SERVICES IN PRODUCTION ENVIRONMENTS.
>> SO THE RESULTS FROM THOSE NEW REGION LAUNCH GAME DAYS WE WERE
DOING WERE ACTUALLY REALLY SUCCESSFUL. SO MUCH SO THAT WE
DECIDED TO MAKE A MORE SIGNIFICANT INVESTMENT AND BUILD
AN AWS REGION DEDICATED TO THIS KIND OF TESTING ALL THE TIME. WE
WANTED TO BE ABLE TO RUN GAME DAY TESTS IN A SAFE SPACE WITH
NO CUSTOMERS ANYTIME WE WANTED, AND NOT JUST IN THE RUN UP TO A
REGION LAUNCH. WE FINISHED BUILDING THE TEST REGION WITH
THREE AZS AND A FULL COMPLEMENT OF AWS SERVICES EARLIER THIS
YEAR. TO BE CLEAR, IT WON'T BE OPEN TO US CUSTOMERS BECAUSE YOU
DON'T WANT TO BE IN THAT REGION. TRUST ME. AND IT WON'T BE ON THE
HEALTH STATUS DASHBOARD, BUT WE ARE ALL THE TIME USING IT TO
IMPROVE THE AVAILABILITY OF THE OTHER REGIONS. SO THE GAME DAY
REGION HELPS US TO FURTHER EXPAND OUR UNDERSTANDING OF HOW
THE SYSTEMS WILL FUNCTION IN RARE CONDITIONS. THE IDEA IS
THAT SERVICE TEAMS ALL TREAT THE GAME DAY REGION AS JUST ANOTHER
PRODUCTION REGION, BUT ONE THAT JUST DOESN'T HAPPEN TO HAVE ANY
CUSTOMERS IN IT. THIS ALLOWS US TO CONTINUOUSLY PROPOSE AND
SCHEDULE GAME DAYS WITHOUT WAITING FOR THE NEXT REGION TO
LAUNCH. SO WHAT ARE THE KINDS OF THINGS WE TEST? THERE'S A
STANDARD SORT OF SET OF BENCHMARK TESTS THAT ARE
IMPORTANT TO US, LIKE WILL POWER DOWN AN AVAILABILITY ZONE OR
DISCONNECT IT FROM THE NETWORK. WE'LL BREAK A COMMON DEPENDENCY
SERVICE TO VALIDATE SERVICES ARE RESILIENT TO THAT. WE'LL PERFORM
A COLD START OF A SERVICE AND SEE THAT EVERYTHING RECOVERS AS
EXPECTED. AND THEN SOME LESS COMMON THINGS LIKE IF WE HAVE AN
EVENT AND WE MAKE A CORRECTION TO A SYSTEM TO FIX IT, WE WILL
ACTUALLY RECREATE THE EVENT IN THE TEST REGION IN ORDER TO
VALIDATE THAT THE FIXES ACTUALLY WORK. WE ALSO CREATE SOME NOVEL
RARE EVENTS WHERE WE KIND OF SIT DOWN AND KIND OF BRAINSTORM AND
COME UP WITH THINGS THAT WE'VE NEVER SEEN, BUT WE WOULD LIKE TO
VALIDATE WHAT WILL HAPPEN. WILL WE RECOVER FROM THAT KIND OF
EVENT? SO THE THIRD AXIOM, FOCUS ON RIGOROUS TESTING. AND THIS
HAS INSPIRED NOT ONLY A SET OF HIGH STANDARDS IN TERMS OF
CONTINUOUS INTEGRATION AND DEPLOYMENT, BUT ALSO THE CONCEPT
OF RUNNING REGION WIDE GAME DAYS WITH ALL SERVICES AND BUILDING A
DEDICATED REGION ULTIMATELY FOR THAT PURPOSE.
>> OUR FINAL AXIOM IS ABOUT PROTECTING AGAINST OVERLOAD.
OVERLOAD IS ONE OF THE MOST COMMON SOURCES OF IMPACT THAT WE
SEE, AND WHILE WE HAVE A NUMBER OF PROTECTION MECHANISMS LIKE
LOAD SHEDDING, RATE LIMITING, ADAPTIVE RETRY STRATEGIES AND
MORE, THERE IS A TYPE OF OVERLOAD SITUATION THAT IS QUITE
RARE BUT CAN BE REALLY IMPACTFUL. WE'VE SEEN IT OCCUR
IN OUR SERVICES AND IT'S SOMETHING THAT WE LIKE TO
PREVENT. AND WHAT I'M TALKING ABOUT ARE METASTABLE FAILURES.
AT A HIGH LEVEL. METASTABLE FAILURES LOOK LIKE THIS. FIRST,
THE SYSTEM STARTS IN A STABLE STATE WHERE EVERYTHING'S GOOD,
BUT THEN OVER SOME PERIOD OF TIME, LOAD INCREASES AND WE
ENTER THIS VULNERABLE STATE. THE SYSTEM STILL HEALTHY. IT'S NOT
OVERLOADED YET, AND IT COULD OPERATE THIS WAY FOR MONTHS OR
EVEN YEARS. IN FACT, MANY SYSTEMS INTENTIONALLY CHOOSE TO
OPERATE IN THIS VULNERABLE STATE. BUT WHAT HAPPENS IS AT
SOME POINT THERE'S A TRIGGER, AND THIS TRIGGER CREATES A
CONDITION WHERE THE COMBINATION OF THE EXISTING LOAD, COMBINED
WITH SOME SPIKE, EITHER THROUGH RETRIES OR SOME OTHER SOURCE,
TRANSITIONS THE SYSTEM INTO THE METASTABLE STATE. IN THE
METASTABLE STATE, A FEEDBACK LOOP SUSTAINS THE OVERLOAD AND
KEEPS THE SYSTEM IN A STABLE DOWN STATE, AND IT DOESN'T
RECOVER. THE SYSTEM REMAINS IN THIS STATE UNTIL A BIG ENOUGH
CORRECTION ACTION IS BIG ENOUGH. CORRECTIVE ACTION IS APPLIED,
TYPICALLY DONE MANUALLY BY AN OPERATOR. AND SO IF THAT SEEMS
VERY ESOTERIC OR HIGH LEVEL, LET'S LOOK AT A CONCRETE EXAMPLE
WHERE WE'VE SEEN METASTABLE FAILURES OCCUR. WE SEE THESE
COMMONLY HAPPEN IN QUEUES. AND IT'S IMPORTANT TO NOTE THAT
QUEUES ARE ALL AROUND US. THEY MAY BE EMBEDDED IN LIBRARIES
THAT YOU USE OR PART OF HOW NETWORKING EQUIPMENT MANAGES
MOVING PACKETS AROUND THE NETWORK. SO LOTS OF POSSIBLE
PLACES WHERE A QUEUE COULD EXIST IN YOUR SYSTEM, EVEN IF YOU'RE
NOT USING ONE DIRECTLY LIKE AN SQS QUEUE. SO UNDER NORMAL
CONDITIONS, OUR SERVER HERE CAN PROCESS THE MESSAGES IN THE
QUEUE. AND THE QUEUED UP STAYS MANAGEABLE. IT DOESN'T GROW
BEYOND WHAT THE SYSTEM CAN SUCCESSFULLY PROCESS. BUT WHEN
THERE'S A SUDDEN LOAD SPIKE, RIGHT? IN THIS CASE THAT'S THE
TRIGGER EVENT. THE QUEUE DEPTH GROWS, THE SERVER CONTINUES TO
PROCESS MESSAGES, RIGHT. IT'S UNAWARE THAT THE LOAD SPIKE
HAPPENED, BUT AS IT'S WORKING ITS WAY THROUGH THE BACKLOG, IT
CAN'T GET THROUGH THEM ALL. EVENTUALLY, THE MESSAGES IN THE
QUEUE GET OLDER AND OLDER. AND THEN NEW REQUESTS COME IN. AND
NOW THE THINGS THAT WE'RE WAITING ON THOSE MESSAGES TO BE
PROCESSED HAVE GIVEN UP. LIKE A TCP CLIENT. THEY'VE STOPPED
WAITING. AND SO WE START TO HAVE WASTED WORK. WE'RE DOING WORK ON
MESSAGES THAT HAVE NO CHANCE OF SUCCESS. AND BECAUSE WE'RE
PROCESSING FIRST IN, FIRST OUT, WE'RE ALWAYS WORKING ON THE
OLDEST MESSAGES IN THE QUEUE. AND SO THOSE CLIENTS THAT GOT
TIRED OF WAITING, WELL, THEY RETRY THEIR REQUESTS. AND THAT
CONTINUES TO ADD TO THE QUEUE DEPTH. AND SO THE COMBINATION OF
OUR STEADY STATE REQUESTS AND THE WORK AMPLIFICATION OF THE
RETRIES HAVE CREATED THE SUSTAINING EFFECT, THE DEPTH IS
INSURMOUNTABLE. WE HAVE MORE WASTED WORK. AND SO EVEN AFTER
THIS LOAD SPIKE IS REMOVED, THE SYSTEM CAN'T RECOVER. IT'S DOING
LOTS OF WORK, JUST NONE OF IT'S USEFUL.
>> SO WE'VE SEEN THIS PATTERN ACROSS THE INDUSTRY OF THESE
METASTABLE FAILURES. AND THEY CAN LIE DORMANT IN SYSTEMS FOR A
LONG TIME, ESPECIALLY SYSTEMS WITH QUEUES. AND AS METASTABLE
FAILURES BY THEIR NATURE HAVE LONG RECOVERY TIMES AND THEY'RE
UNHAPPEN, RARELY, THEY ARE A HIGH PRIORITY FOR US TO FIND AND
PREVENT. SO WE BROUGHT TOGETHER A GROUP OF SCIENTISTS WITHIN AWS
TO STUDY THEM DEEPLY AND ASK, HOW DO WE EFFICIENTLY FIND THESE
SYSTEMS THAT MIGHT HAVE THESE RISKS SO WE CAN TEST AND FIX
PROACTIVELY? AND ONE OF THE BIG CHALLENGES WITH SO MANY SERVICES
AND MICROSERVICES IS HOW WOULD YOU FIGURE THAT OUT? LIKE WHERE
WOULD YOU START? AND HOW DO WE FIGURE OUT WHICH SYSTEMS TO LOOK
AT. AND DIRECT TESTING OF AN INDIVIDUAL SYSTEM IS PRETTY
EXPENSIVE. SO WE HAVE TO BE ABLE TO FIGURE OUT LIKE, WHERE ARE
THE PLACES TO LOOK? THE PROBLEM IS THAT THE PARAMETER SPACE IS
REALLY LARGE. EVEN FOR ONE SYSTEM YOU HAVE DIFFERENT QUEUE
LENGTHS THAT MIGHT BE PROBLEMATIC. YOU HAVE RETRY
DIFFERENT RETRY RATES. THE PARAMETER SPACE IS HUGE. SO THE
SCIENTISTS BUILT A MULTI STEP STRATEGY TO EFFICIENTLY SEARCH
THIS PARAMETER SPACE AND MAP OUT WHERE IT IS. WE SHOULD FOCUS OUR
TESTING. AND THIS CONSISTS OF A STATISTICAL MODEL THAT CAN
QUICKLY EXPLORE THE FULL SPACE FOR A GIVEN SERVICE. ON JUST
RUNNING ON A LAPTOP, A SIMULATION THAT CAN ALSO RUN ON
A LAPTOP AND ACTUALLY RUN SERVICES THAT TALK TO EACH OTHER
IN IN A SIMULATION MODE, AGAIN ON A LAPTOP, AND THEN AN
EMULATION, WHICH IS ON PHYSICAL SERVERS, BUT JUST RUNNING DUMMY
CODE WHERE THEY'LL SLEEP INSTEAD OF DOING ACTUAL WORK, BUT WHICH
CAN CHEAPLY IMITATE THE IMPORTANT CHARACTERISTICS OF THE
SERVICE. AND THEN FINALLY, ONCE WE'VE NARROWED THINGS DOWN AND
FIGURED OUT WHERE WE'RE CONCERNED ABOUT, WE CAN ACTUALLY
APPROACH THE TEAMS AND RUN THE REAL TEST AND BUILD THE SYSTEM,
SCALE IT UP OUT OF PRODUCTION TO DO THE TESTS. AND THIS STRATEGY
HAS SAVED US A HUGE AMOUNT OF TIME AND ENERGY. SO FOR THE
STATISTICAL MODELING PIECE, THE TEAM BUILT AN OPEN SOURCE TOOL
CALLED METAPHOR, WHICH YOU CAN GET ON GITHUB. THIS PERFORMS THE
CALCULATIONS AND CREATES THESE VISUALIZATIONS. IT'S A REALLY
LIGHTWEIGHT WAY TO BEGIN UNDERSTANDING WHERE A METASTABLE
FAILURE MIGHT OCCUR IN YOUR SYSTEM. SO IN THIS
VISUALIZATION, YOU CAN SEE THE QUEUE LENGTH ON THE X AXIS AND
THE NUMBER OF OUTSTANDING RETRIES ON THE Y AXIS. SO EACH
POINT ON THE GRAPH REPRESENTS A POINT IN THE STATE SPACE, AND
THE ARROW IS VISUALIZED. THE MOST PROBABLE TRANSITION IN THE
NEXT TIME WINDOW. SO WHERE YOU SEE THE ARROWS MOVING TO THE
LEFT, WE'RE PROBABLY DRAINING THE QUEUE WHERE YOU SEE THE
ARROWS MOVING TO THE RIGHT, WE'RE PROBABLY GROWING THE
QUEUE. AND SO YOU CAN SEE THERE'S LIKE THIS INFLECTION
POINT LINE ON THERE FOR THIS SYSTEM. AND THIS ALLOWS YOU TO
VISUALIZE THAT QUICKLY. THE TEAM RELEASED A PAPER RECENTLY WITH
THE DETAILS OF THIS RESEARCH AND PRESENTED IT AT HOT OS 25. IF
YOU'D LIKE MORE DETAILS, THE RESEARCH PAPER AND THE GITHUB
LINK ARE THERE AS QR CODES. SO OUR FOURTH AND FINAL AXIOM FOR
TODAY WAS FOR OUR SERVICES TO PROTECT THEMSELVES AGAINST
OVERLOAD. AND THIS AXIOM INSPIRED US TO DO A LOT OF WORK.
BUT INCLUDING FUNDING THIS WORK ON TIPPING POINT AND METASTABLE
FAILURES. >> SO TODAY WE'VE TAKEN A LOOK
AT SOME OF THE BEHIND THE SCENES WORK WHERE AWS IS INNOVATING ON
YOUR BEHALF IN AREAS THAT MIGHT NOT BE COMPLETELY OBVIOUS TO
YOU. AS CUSTOMERS, WE HELP MIGRATE ALMOST ALL OF THE
EXISTING GLOBAL STS TRAFFIC TRANSPARENTLY TO BE SERVED IN
EACH REGION LOCALLY, HELPING STRENGTHEN THE REGIONAL
ISOLATION OF CUSTOMER WORKLOADS. WE BUILT A FLEET HEALTH SERVICE
AND ZONAL EVENT DETECTOR THAT POWERS ZONAL AUTOSHIFT, ALLOWING
AWS SERVICES AND CUSTOMERS TO DETECT AND RESPOND QUICKLY TO
SINGLE AZ IMPAIRMENTS. WE BUILT THE AWS TEST REGION SO THAT WE
CAN RUN GAME DAY ACTIVITIES ALL OF THE TIME. XCELERATOR
ACCELERATING OUR ABILITY TO CONTINUOUSLY VALIDATE EXPECTED
BEHAVIOR IN OUR SERVICES, AND TO CONTINUE TO EXPAND OUR
COMPETENCE ENVELOPE FOR NEW OPERATING MODES. FINALLY, WE'RE
ACTIVELY PURSUING RESEARCH ON METASTABLE FAILURES TO HELP
PROTECT OUR SERVICES AGAINST RARE BUT REALLY IMPACTFUL
SCENARIOS THAT CREATE OVERLOAD CONDITIONS. WE'RE ALWAYS LOOKING
FOR WAYS TO INNOVATE THAT TRANSPARENTLY IMPROVES THE
RESILIENCE OF THE CLOUD, AND REDUCES THE BURDEN FOR CUSTOMERS
TO ACHIEVE YOUR RESILIENCE GOALS. OUR OBSESSION WITH
RESILIENCE AND OUR CULTURE OF INNOVATION MAKES AWS THE BEST
PLACE TO RUN MISSION CRITICAL WORKLOADS. THANK YOU ALL SO MUCH
FOR COMING TODAY. PLEASE REMEMBER TO FILL OUT YOUR
SURVEYS AND HAVE A GREAT REST OF YOUR RE:INFORCE. THANK YOU SO
MUCH.
