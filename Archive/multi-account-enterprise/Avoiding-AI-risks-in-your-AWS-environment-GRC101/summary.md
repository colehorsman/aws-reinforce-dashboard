# AWS re:Inforce 2025 - Avoiding AI risks in your AWS environment (GRC101)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=ntaLNOs38VA)

## Video Information
- **Author:** AWS Events
- **Duration:** 24.5 minutes
- **Word Count:** 4,891 words
- **Publish Date:** 20250620
- **Video ID:** ntaLNOs38VA

## Summary
This session by a Tenable Technical Director addresses the security risks associated with AI adoption in AWS environments. The presentation reveals concerning statistics from Tenable's research: 70% of AI workloads contain vulnerable packages (vs 50% for non-AI workloads), 14.3% of training data storage has public access enabled, and 90% of organizations leave SageMaker with root access enabled. The talk covers three main risk categories: cloud-native AI services vulnerabilities, insecure data storage practices, and library/dependency risks. The session concludes with security best practices including asset visibility, least privilege access, secure storage configurations, AI-specific vulnerability prioritization, and using Tenable's AI Security Posture Management (AISPM) and AI Aware capabilities for comprehensive exposure management.

## Key Points
- **AI Adoption Growth**: McKinsey survey shows exponential growth in AI usage, creating a "gold rush" mentality that often bypasses security considerations
- **Higher Vulnerability Rates**: 70% of AI workloads have vulnerable packages compared to only 50% of non-AI workloads
- **Public Access Risks**: 14.3% of Bedrock training data storage has public block access disabled, exposing confidential data
- **Over-Permissive Access**: 5% of organizations have over-permissive S3 buckets for AI data, enabling data theft and poisoning attacks
- **Root Access Problems**: 90% of organizations leave SageMaker with root access enabled, allowing system-level modifications
- **Data as Crown Jewels**: Training data must be treated as the most critical asset, as compromise can lead to model poisoning or data leakage
- **Vulnerability Management**: AI-specific vulnerabilities like CVE-2023-38545 in cURL require prioritized remediation strategies
- **Security Best Practices**: Implement asset visibility, least privilege access, secure configurations, and proactive vulnerability management
- **Tenable Solutions**: AI Security Posture Management (AISPM) and AI Aware provide comprehensive visibility and vulnerability detection for AI workloads

## Technical Details
- **Vulnerable Package Analysis**: AI workloads show 40% higher vulnerability rates due to newer libraries, open-source dependencies, and rapid development cycles
- **CVE-2023-38545**: Critical cURL vulnerability affecting many AI workloads used for data fetching, enabling unauthorized access and model tampering
- **Storage Misconfiguration**: Bedrock training data with disabled public block access settings expose proprietary datasets to internet access
- **Permission Analysis**: Over-permissive S3 buckets allow attackers to steal, inject poison data, or delete training datasets
- **SageMaker Root Access**: 90% of deployments maintain root privileges, enabling unauthorized software installation and system file modification
- **AI Security Posture Management (AISPM)**: Tenable's solution provides AI resource inventory, misconfiguration detection, permission mapping, and remediation guidance
- **AI Aware Vulnerability Detection**: Specific detection for AI frameworks like PyTorch and TensorFlow with version tracking and vulnerability correlation
- **Threat Research Integration**: Continuous updates for emerging AI threats like DeepSeek exposures with automated detection framework updates
- **Attack Surface Analysis**: Graph-based visualization of AI resource interconnections and potential attack paths across AWS environments
- **Exposure Management Platform**: Unified visibility across OT, cloud, identity, and vulnerability management with AI-powered risk prioritization

## Full Transcript

Well, hey, thank you for joining me on the last day of the event. Um, I appreciate you taking the time, especially after lunch, and hopefully I don't put you to sleep. Um, my, uh, talk today is about avoiding AI risks in your AWS environment, so very GRC focused, but also very pertinent to, um, kind of the times, right? AI becoming more and more of a thing. I think every session might must have had AI in it some way or another, uh, at the event and so really glad to, uh, to spend my time with you. So a little about, uh, yours truly I won't read this to you. um, I will say something kind of funny, um, hopefully. That picture, uh, had my wife in it. We went to Paris last year, yeah, last year, and I had to use AI to edit her out so it wasn't like my wife and I on, on, on this picture. I, I maybe I should have left her in, but she looked, it gave me kind of a funny look anyway, I've been doing this for a while, um, in a different capacities. I've been, um, on the technical side I've been on the sales leadership side I'm back on the technical side now, so I'm a technical director here at Tenable. Been here for over 3 years. Um, primarily focused on our cloud security products, right? So, I've been watching our, uh, our capabilities expand and expand, and, uh, hopefully some of you have experienced that. All right, so here's what we're gonna go over today. Number 1, AI adoption and challenges. So, uh, with any new technology, lots of challenges, uh, number 2, I'm gonna review some risks that we've actually uncovered or discovered as, as part of our, uh, tenable research organization and then finally I'm gonna talk about security best practices, uh, with us with, with tenableable obviously this being a a sponsored session I have to do something at the end that says, oh, and by the way we can, we can fix all of these problems for you, um, but anyway, let's let's proceed. Um, all right, so one of the things, there's a McKinsey survey that came out that said, uh, you know, the state of the state of AI that AI is expanding in usage, uh, more and more and more, right? I think it's safe to say that we've opened Pandora's box with this. And so you can see, um, not just the adoption of AI which I think is interesting, but also the the purple line which is generative AI which I use all the time. I don't know about everybody else, but I love asking AI to do my job for me. It's like one of the best things ever. It's like, hey, um, I need to write this and it's like, sure, I'll write that for you and I say awesome uh work smarter not harder, right? So, um, I didn't use AI to write this though just just for the record. All right, so AI uh is increasing. Whether we like it or not, AI is here to stay and growing. So uh you know tenable research so we have uh obviously we have a research organization at our company we conducted an analysis of our customer base and our workloads across all of the cloud environments um that we cover, right? And, um, we kind of took a look at who's using what and we can see um there's a lot of uh like a really high percentage of customers that are using AI in their environment in one shape or another and obviously there's other clouds on here but. Bedrocks on there, sage makers on there. And so uh we've got a lot of different tools, a lot of people using them. So if you think about the last two slides, this really highlights the amount of risk. OK, so if everybody's using this stuff and it's continuing to get more, um, how can our threat actors or not our threat actors, the threat actors take advantage of this and, uh, give you a bad time. So Um, as part of this, um, insights that we're gonna share today, um, we're gonna bucket into kind of three primary things. So, um, the first one is cloud, uh, native AI services, so that's, you know, bedrock, Sage maker, those kinds of things, the storage for the data, right? So you, you can have a model, but you've got to have storage for the model that says this is the training data that you're gonna use, you know, S3 buckets. I'm looking at you, but there's others as well. And then also there's uh lots of libraries involved in open source libraries and maybe internal libraries that are getting written um to consume or interface with that data in one shape or another. It's you can't just throw a model and some data and just say you're good. There's a lot of in between work that you've got to do um to make that work. So let's start with some trends. Um, first of all, the first trend is, is something that I think we're all kind of familiar with, or I know I am, um, with my background is, um, the reason why we're going to cloud, right, was always, uh, number one kind of velocity, right? So, so we wanted to go to, uh, to market quicker so we're saying cool, we're gonna go to the cloud, um, you know, we maybe we should save some money, but also we're gonna be able to do things quicker. And it was kind of the wild west, right? and and some would say that it's still kind of is the wild west a little bit in the cloud, but hey, now we've got this new thing to worry about called AI, and it's a kind of a gold rush, right, uh, for companies because if you can, there's a lot of things you can solve with AI like for example, you know, having me write all my stuff for me like that's awesome. I've solved that problem but you know interfaces on top of documentation or API calls or whatever it is, there's a rush to do this because there's such value that can be gained from AI but. Uh, with that comes risk, right? And, um, as we've seen in the past, um, you know, if you try to speed through that stuff and not really think about a security architecture ahead of time, what's gonna end up happening is you're gonna bypass it in the rush to get it out there, right? And we've seen this over and over again. I don't care what the technology is at the end of the day if you, uh, are in a rush to get something out, security often is the the kind of thing where they're like, oh we'll worry about that later, no problem. Um, it's just kind of like the joke about, um, there's no such thing as a testing environment, right? It's, it, once it's live, it's in production. So really need to be careful. All right, so we've identified a uh kind of a, a number of different risks. One is critical vulnerabilities which have not gone away, and I'm gonna talk about that. It actually gets kind of interesting. Um, the other one is, uh, public access, so public access to models, public access to, um, you know, kind of your learning data, public access to APIs, whatever. Um, also, in addition to that excessive permission, so, um. Maybe your users have too much access to your learning data. Maybe your models have too much access to internal company data that it shouldn't because the last thing you want your model to do is go learning on your HR system, for example, and then start spewing out, uh, HR records to your uh customers in a chatbot, and I'm kind of joking, but in theory it could happen, right? Like you, you, you could have a leak that way and it has happened in the past, maybe not so extreme. And then finally there's always the classic other misconfigurations that can happen in the cloud of which there are uh many to be said, that's for sure. Alright, so let's talk about AI specific vulnerabilities. So, um, we found that, uh, uh, 70% of all AI workloads had some sort of vulnerable package versus roughly about 50% non-AI workloads, which I think is really interesting. What that implies is that for some reason. If it's got AI on it, if there's something that that that's, you know, using AI or consuming II or whatever is on that, uh, machine or instance that it's higher there's higher likelihood for it to actually be vulnerable, right? And um that could be a lot of things I think um there's a lot of newness to this stuff so libraries are relatively new um there's a lot of unremediated vulnerabilities and everybody's working as fast as they can to do it. Um, and it's not open source software and I, I, I love open source software. I use it all the time, but you know you've got to rely on your upstream to or or downstream, uh, to, to handle that and obviously supply chain attacks or those kinds of things can make it, uh, a lot more difficult as well. And, and at the end of the day like the worst thing that can happen is your training data. Get stolen, so imagine your entire data sets get, get, get owned or or maybe even, you know, maybe even worse is that they get tainted, right? So somebody goes in there and starts changing things and now all of a sudden instead of helping your customers it tells them to go do something else right or I don't know, uh, send us Bitcoin or whatever. So, um, to give you a couple of specific vulnerabilities, there was, um, some interesting ones I actually used AI to, to query this to see, hey, is this, is this really a thing, and, and yeah, it sure sure is. So, um, you know, there's a SOX 5 vulnerability in Curl, which is, you know, a super popular, uh, web crawler get, uh, program out there, um, and it and it didn't uh get passed for a pretty long time and in a lot of cloud workloads it's there for a variety of reasons maybe it's being used to fetch data. Maybe it's there because they forgot to remove it or whatever it is, um, but at the end of the day that could be exploited to gain unauthorized access to the box that's that's running that stuff and then and then extracting model data and then again tampering with your your training right and so poisoning what it reports and and I think again that's there's a lot of danger and a lot of risk around that. And that's all down to one vulnerability. Um, there's another one, that I won't try to pronounce. I, I did try to use AI to tell it to me and it was like I don't really know, um, but, um, it has to do with an outdated package. It's just a, you know, I guess at the end of the day if you look at this, this is no different than normal vulnerability management, right? There's vulnerable packages. It just so happens that now a vulnerable package has a much bigger impact and so the takeaway though is that. There are now new and unique vulnerabilities um that can impact your organization in a different way than than normal like uh an old vulnerability that wasn't AI related doesn't have as much of an impact potentially because it doesn't have access to training data but again I'll go back and say it's just a vulnerability a vulnerability is a vulnerability and you do have to manage them like vulnerabilities at the end of the day. All right. So, um, here's one that's kind of fun. So 14.3%, which is awfully specific. Um, of training data storage had public, uh, block public access to disabled, which means that the public can access that, uh, that, uh, storage right for, for bedrock. So that's, it's a pretty big deal. Um, I don't think I can think of any situation sort of like here's an example of something, um, where you would wanna do that but it's pretty easy with like defaults and and just not paying attention or maybe I'm developing something to open that up. Um, and so, um, I think the risk of data is huge. There, um, and what this ends up meaning is probably there's a lot of confidential data that's that's being extracted or exfiltrated from from those models because if, if they find it, um, and that's that's no good, right? And so if you're not filled with bedrock, that's generative AI that's uh AWS product, uh, super cool stuff, all right. Next one, so, uh, data exposure risks in cloud AI workloads. So, um, in bedrock, 5% of organizations have at least one over permissive bucket. If an attacker gains access to that storage bucket, they could steal your data, they can inject poison data, and they can delete or modify data sets, right? And so now you've got this, you know, haywire hallucinating AI that's spitting customer data and and HR information out to uh chatbots, right, or whatever it is. I'm, I'm kind of making light of it, but again I would say that there is, there is risk, right? There is potential for that and there have been cases in the past where that's happened. Um, through, uh, prompt, uh, engineering, but anyway. Cool. So, um, we found, here's another finding that we found. So 90% of organizations left Sage uh maker with root access enabled. That's never a good practice. It doesn't matter what you're, what you're doing, but with root access, users can edit or delete system critical files, including those that contribute to AI models and install unauthorized software and modify, uh, environmental components. So you can go in there and add a of packages, change things around, essentially own the box, and because AI models are particularly sensitive, uh, or built on particularly sensitive, uh, proprietary data. Obviously there's a lot of risks and I just sound like a broken record, but exposing this data could be devastating to your organization. I think you're getting a trend here like AI could be great, but Uh, it could be bad as well if not taken care of and not in security not taken seriously. So you really have to treat your training data as your crown jewels. You can't just uh toss it in a bucket somewhere and hope that nobody access it or modifies it, um, and that those aren't uh vulnerable or exposed. Cool. All right, so let, let's talk a little bit about best practices. Um, so this is the classic best practice, the, uh, you can't secure what you don't see. So, um, one of the biggest things that we see in cloud security is tell me what I've got. It's not even like tell me what to fix. It's I've got all these resources sitting out in, um, on, you know, in AWS and I don't know what I've got and people are spinning up things all the time and things are getting modified, and I don't know what it is. It's the same thing with AI wear clothes and especially now that it's new and everybody wants to do this gold rush, I was saying what ends up happening is people are spinning this stuff up just to play with it. And maybe pointing to something like your crown jewels and saying oh yeah it'll be fine. Meanwhile they've left the door open, uh, you know, either a public bucket or whatever it is, uh, to the universe to see and so the first thing you really need to do is is figure out what you've got and where it is, and there are tools for that, um, AI secure posture management, which is one of those new, um, acronyms is one of them we'll talk about that, but there's also other um security products. All right, now, um, applying least privilege access controls, this is not a new concept, I hope, um, this is something that we should be doing anyway, which is if it doesn't need access to something, it shouldn't have access. Permission should be granted on an as needed basis and removed if they're not required, um, and over privileged accounts are probably one of the biggest security gaps in any workload, not just AI workloads, um, and so limiting access to uh to those to those models is the way to do it. I, I guess zero trust would be the word here, but, uh, I'm not sure if I really buy into zero trust. That's a lot harder than it uh it sounds, but anyway. Be careful, just limit access. All right. Securing your AI, uh, training data and storage, this is a, uh, I've been saying this over and over again, misconfigured storage is an easy target. It's been an easy target for, for, forever. Uh, remember the deep seek leak, um, it exposed sensitive data. There you go, um, chat history, secret keys back in details. I know I've accidentally pasted stuff in the AI before. I'm like, oh, I, I wish I could take that back and there's no deleting on the internet as a lot of people have found out the hard way, right? Um, so always enforce strict access, uh, policies and disable public sharing of AI related data. Um, what that means is like you don't really want to have people share your data with, you know, not, not through, uh, AI, right? And your AI should filter that data and make sure that it's not responding so you can do all sorts of constraints and things in AI to make sure that it's not barfing out data, um, but make sure that your models are secure or your data is secure excuse me, all right, um, prioritizing AI specific vulnerability remediation, so there's no such thing as no vulnerabilities in any environment. I've never found one. I don't think I ever will. They just don't exist because it's just a game of whack a mole. There's a new vulnerability all the time that you need to solve, and you solve that one and 30 seconds later you've got another one to deal with, and that's traditional vulnerability management, but the point is is that you should spend uh extra attention to uh AI workloads. Don't, uh, maybe, maybe bump those up in urgency or prioritization, right? So asset criticality high AI right? because that again is a could be a public interface to your, to your model, to your data to uh customer data, whatever it is, right? So implementing risk detection, finding unpatched CVEs, um, learning frameworks that should or shouldn't be there like pie torch or 10 tensor flow, excuse me. However, I would say that Security's goal isn't to get in the way of everybody. Like you have to develop a strategy around what you're gonna allow folks to do and what to use and so I would, I would do that, but remediation, remediating vulnerabilities is obviously a pretty, pretty important thing. All right. Uh, enforcing secure cloud configurations, so this is, this goes beyond, um, AI and it gets into just general best practices in the cloud like you, you need to have secure, uh, configurations no matter what you do, even if it's not touching AI. If you ever see like a a graph model of all the stuff in, in your AWS environment, you said everything is interconnected with each other just because I'm saying like focusing on this AI resource right here. Uh, doesn't mean that you don't need to pay attention to all the rest of the stuff because there's a whole lot of dependencies, there's a whole lot of things that ride along with that, um, that could potentially create an attack pathway that uh you're not aware of, right? And so you, at the end of the day, you've got to be secure everywhere. And boy am I making your lives a lot more difficult, right? So that's a, that's a really big thing to ask, but again, if you were to prioritize something, uh, focusing on the AI stuff and then after that, uh, secure. Cloud stuff and of course there's there's more right? so we have a report uh that I would encourage you to read on AI uh cloud AI risk report on Tenable's website. So definitely grab that. It's got a lot of these statistics in it, um, and it's a pretty interesting read, pretty revealing, um, and we also came out with a new risk report for 2025, I think actually today, so worth, worth a read. We've got uh a pretty stellar research team that's out there writing the stuff and doing research and we've got a lot of data points. Tenable has been doing this for a very long time and our research team is, uh, I'd like to think world renowned. All right. So the takeaway here though is that AI security must be proactive. You have to address misconfigurations and vulnerabilities before they're exploited, big shocker. You gotta do it. All right. It's a little bit about what we do and and by the way we're gonna end early uh there's no doubt I'm not gonna hit an hour and I I don't think anybody's gonna be upset about that but I will be over here down in uh down where you guys are to answer questions because I'd love to have a conversation with you and sit down and and talk um if you're interested, if you're not, that's that's fine too but let's get a little bit into what Tenable does to fix this because I just presented you with a whole bunch of problems and said good luck, um, that sounds hard, right? So I wouldn't be talking about it if I didn't have a solution for you and, and, uh, I don't know if I'm supposed to do this show of hands, who uses Tenable right now? Yeah. Yeah, fair number of folks. Cool. Awesome, yeah, so you may have heard a tenable one. Edible one is our overarching platform for exposure management which it unifies vision across uh different areas of your environment from from OT to cloud to identity to vulnerability management and parts of that are are AI focused so we've got as I had mentioned before AI security posture management or AI SPM as it's called, and we also have something called AI Aware. Which is that, you know, library detection uh uh CV detection on workloads is saying, OK, well you've got these, you know, pie torch libraries or whatever that are configured on this machine and they're vulnerable. And so what we aim to do is give you a 360 view of, of your environment, including AI. All right. So, uh, what I'm gonna show you is from our cloud security product. This is AISPM. I would have loved to do a demo, but I'm cursed by the demo gods. So whenever I do one of these things, something will break my laptop will fall on the ground and catch on fire or whatever. So I did some screenshots for you, but this is our cloud security product and in here. First and foremost, it's gonna give you a dashboard of things that are in there, but one of them it's gonna give you is AI resources, which you can see up there with the number 43, right? So this is how many AI uh resources I have in my environment. Cool. If I go into there now, I can see um different bedrock, uh, you know, Resources in my inventory so I can see what I've got. And then if I click into it a little bit more, I can find all of the uh misconfigurations, so things that are misconfigured are going wrong with it. So you could see, for example, I've got root access enabled like 90% of the world apparently. Um, and then finally we can show you a graph of how permissions are interrelated. So does this have access to something it shouldn't? What, you know, what is bedrock learning from? Does it have access to my, uh, you know, HR data, that kind of stuff? And finally it'll give you remediation advice, obviously it's not just enough to to point it out and say, hey, you've got a bunch of problems, and then, uh, not tell you how to fix it like I would have done with this presentation if I hadn't talked about this all right. Cool. Next up we'll talk about AI aware which I gave you a sneak preview of, but essentially this is um essentially vulnerability uh management for AI vulnerability specifically on workloads so we'll give you a whole lot of detail. On what's installed, so you can see here we've got Pytorch, um, which I like to pick on apparently, uh, installed on a, on a, uh, on an asset. So we can provide you recommendations on what to do with that, what versions you've got, what kind of vulnerabilities it has, and it's very specific and easy to filter on so you can say just give me all my AI vulnerabilities. You don't have to say give me my vulnerabilities and sort through it. It's pretty easy to do, um, and easy to remediate. And then I mentioned our world class tenable threat research team. So they spend a lot of time looking at AI threats and we continually update II threats all the time and in fact this page is accessible to anybody you can you can look at it, um, but we find stuff all the time, uh, like for example uh new exposure comes out for Deepeek. OK, cool, we'll have something on there and then we'll update our detection framework and start showing that. So we, we proactively find this stuff so you don't have to go read it in the news and then figure out how to detect it and that's pretty important. All right. Now we're gonna zoom out uh from features now and then talk about uh cloud security and, and then also uh Tenable one a bit more. So, uh, enable one is like I said, is an exposure management platform. It's, it's AI powered go figure, right, um, it unifies security visibility across your entire attack service so I mentioned OT, I, you know, identity. Cloud security, vulnerability management, and so on. As well as third party products, so we can bring in third party data and then ingest that and show you a view across other products that you might have in your environment to try to consolidate that data and show you something that matters and shows your risk across your, your, your again entire tax surface, right? Um, and so enable cloud security, which is the ISPM part, is a small part but a very important part of Tenable One, and you could, you see that like micro view in there with the, with the orange hexagon, that's, that's it's just that piece and then we've got it, you know, more hexagons for other products that have different capabilities, but, uh, to use an acronym as much as I don't like it, um, it's a full CAP or cloud native application protection platform including AI. Cool. All right, and then zooming out finally, my last slide here, so we're gonna zoom out to the macro level here. So we're, we're the market leader in AI powered uh exposure management. In fact, we pretty much invented exposure management, I believe, uh, and so what again what it does unifies visibility, gives you uh detail across all of your assets. It enriches that data with uh business contact so if we know how critical an asset is or you know if it's an AI workload for example, we'll surface that because it matters the most and then finally give you actions so it's not enough to just say hey again here's all your problems, good luck. Here are the actions that you need to take to remediate that, uh, you know, and hopefully you learn something there, um, but I think that's pretty much it for me so I really appreciate your time. That went pretty quick. I have to I have to admit you never know how fast it's gonna go so uh hopefully you enjoy your rest of your stay here in Philadelphia and uh with that I'll say, uh, thank you. All right.
