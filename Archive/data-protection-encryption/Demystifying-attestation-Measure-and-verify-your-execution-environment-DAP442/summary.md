# AWS re:Inforce 2025-Demystifying attestation: Measure and verify your execution environment (DAP442)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=2T-4vM3S0ys)

## Video Information
- **Author:** AWS Events
- **Duration:** 61.8 minutes
- **Word Count:** 10,281 words
- **Publish Date:** 20250620
- **Video ID:** 2T-4vM3S0ys

## Summary
This AWS re:Inforce 2025 session, presented by Sudhir (Specialist Solutions Architect in confidential computing) and Suresh Viragoni (Specialist Solutions Architect for GAI), focuses on demystifying attestation in trusted execution environments (TEEs) and confidential computing. The session particularly emphasizes securing workloads, including generative AI applications.

The presenters introduce AWS's unique perspective on confidential computing through a two-dimensional model: first, providing assurances that AWS as a cloud operator cannot access customer data and code, and second, offering mechanisms for customers to isolate and segregate their workloads in trusted execution environments. The AWS Nitro System is presented as the foundation for addressing the first dimension, providing built-in isolation and security features.

The session delves into technical implementations, particularly focusing on AWS Nitro Enclaves and Nitro TPM as solutions for the second dimension of confidential computing. These technologies enable customers to create isolated execution environments for sensitive workloads and implement cryptographic attestation to verify the integrity and security of their execution environments.

## Key Points
- AWS defines confidential computing as protecting data and code while being operated upon, using specialized hardware, firmware, and software
- AWS Nitro System provides "always-on confidential computing" by default for all EC2 instances
- The two dimensions of confidential computing: protection from cloud operator access and workload isolation capabilities
- AWS Nitro Enclaves is an EC2 feature providing isolated execution environments, not a separate service
- Cryptographic attestation is crucial for verifying that workloads are running in legitimate TEEs
- Nitro TPM offers an alternative approach to creating trusted execution environments
- AWS Nitro System separates networking, storage, and management functions into custom hardware
- The Nitro Hypervisor is lightweight and focused primarily on memory and CPU allocation
- AWS has validated their security claims through third-party verification
- Security features are included in AWS service terms, setting them apart from other cloud providers

## Technical Details
- AWS Nitro System consists of three main components: Nitro Cards, Network Security Chip, and Nitro Hypervisor
- Nitro Enclaves characteristics:
  - Full virtual machine with constrained resources
  - No EBS attachment (pure in-memory filesystem)
  - No ENI attachment (air-gapped)
  - Supports cryptographic attestation
- Nitro TPM:
  - TPM 2.0 spec compatible
  - Requires specific TPM drivers
  - Compatible with existing TPM tools (Go TPM)
  - Can make entire EC2 instance a TEE
- Network Security Chip performs boot validation by:
  - Putting host machine in reset mode during boot
  - Verifying known hardware firmware hashes
  - Preventing boot with unauthorized firmware

## Full Transcript

All right. uh, good morning. Thanks for joining us in this session. The session is about demystifying attestation. Um, quick show of hands before we get going. How many of you here are familiar with confidential computing? OK, that's good. Uh, how many of you are using conferential computing in AWS? That's good. That's good. All right, uh, so this talk is going to be about measuring and verifying your execution environments, very specifically trusted execution environments, right? So we're gonna be talking about a lot of confidential computing, securing your workloads, specifically securing your generative AA workloads as well. We, we have a cool little code walk through that we'll do later on. So let's actually get going. My name is Sudhir. I'm a specialist solutions architect. My focus area is confidential computing. Been with AWS for about 4 years now, uh, doing this, and, uh, did a few other things as well. With me. I have Suresh. Suresh. Hello everyone, I'm Suresh Viragoni. I'm a specialist solutions architect as well for GAI. I work with our LLM providers like Anthropic, DeepSeek, etc. Um, so we work together like we are cross team, we work together to put together a real world use case in GAI, um, on how to protect your data and assets like code or model weights, etc. and in this one we will kind of walk you through the code like Sudeep mentioned. Yeah thanks Sure. Uh, all right, so let's let's get going. It's a packed one hour, uh, agenda here. Uh, we'll start off with a very, very brief introduction to conferential computing, right? And then we'll move on to the types of trusted execution environments, the options that you have available on AWS, and then we'll jump right into code, right? I promise we'll keep the slides to a bare minimum. So let's start with conferential computing. This is very important to understand because AWSS perspective of confidential computing is a little bit different to what you may have heard already. Uh, our perspective starts with what we like to call dimensional model, right? So what I have on the screen here are two dimensions, dimension 1 and dimension 2, right? And the way we arrived at these is through talking to customers like yourself. And when we talk to customers, they primarily asked us with respect to conferential computing is hey we need two things basically. Number 1, dimension 1 is where customers have asked us. I need assurances that AWS as a cloud operator cannot peek into my data and code, right? Uh, and dimension two is where customers like yourself have asked us that, hey, I need mechanisms, tools, service features where I can further isolate and segregate my workloads into conferential computing into those trusted execution environments, right? So those are the broad two dimensions. Uh, before I get into the answers to those dimensions or the solutions for those dimensions on AWS, let's try to define what confidential computing is, right? So confidential computing for us at AWS is the use of specialized hardware firmware and software. To protect data and code while it is being operated upon. The important term here is while it is being operated upon, right? uh because protecting data and code in transit and at rest are mature constructs. You're probably already doing this in today's world with your normal workloads, right? No need to look at conferential competing for this. There those are things like on of encryption using key management systems, HGPS TLS, things like that, right? So that's the reason why conferential computing is all about protecting while it is being operated upon. So what do I mean by that? I'll give you an example. And before we go into that, right, um. Dimension one, if I just go back to the previous slide, the answer to dimension one of confidential computing where customers tend to see AWS operator as a threat, is pretty simply AWS Nitro system. AWS Nitro System is the underlying platform for all of Amazon EC2 instances. So if today you go and launch an Amazon EC2 instance. It by default de facto launches on AWS Nitro system. So to talk more about what AWS nitro system is, I'm gonna hand over to Suresh here. Yeah, thank you, Sudeep. So on the screen, uh, what you see on the right side is nitro system, but before we get into the details of like how niroy came to be, let's understand the classical system, right, like you know traditionally for decades, um, you know, the classical virtualization is like, you know, you have a hypervisor on your host machine. And then the hypervisor needs to do many things, right? It, it needs to make sure that, you know, it is providing necessary networking to the ECT instances, the, uh, storage, right, the local storage and the attached storage, etc. and also it needs to do monitoring, like, you know, when, uh, to launch instance where it needs to launch instance, what type of instance it needs to launch, and, uh, you know, collect metrics, you know, performance metrics, etc. those kind of things, right? And, uh, so that takes up like the hypervisor on the host, takes up memory and CPU and everything, right? So, um, so it takes some of the resources from the host machine. And then, um, you know, after working over a decade, um, a database like, you know, we are continuously trying to, you know, um, think on behalf of customers, how do we get like maximum utilization of the hardware? How do we bring more security to our customers. How do you, how do we get like maximum performance, um, you know, to the customers, right? Like kind of a bare metal type of performance. And during that process, what we did over the course of time is, um, you know, we reinvented how we do virtualization and, uh, that process, uh, enabled us like first we started with the networking isolation, so that's where. Uh, we started like, you know, taking out these, uh, responsibilities of the hypervisor away from that, uh, host machine. And then as you see on the bottom you see like we took out from the hypervisor we took out both networking, storage, all the management functionality into the custom hardware chips, right? The PCIE cards, um, and then we will talk about the details of like uh what exactly in the nitro system, but. During that process, essentially, uh, we, now you can see on the right side from a visual standpoint, like now you can launch more instances because we freed up all the computer and memory that requires by the hypervisor. Uh, and then what we left out with a very lightweight, um, you know, hypervisor, right? So we rewrote the hypervisor, uh, what we call it as nitro hypervisor, and, uh, so that's responsibilities is like, you know, it's very, it's, it's a very passive hypervisor. It's just only acted upon, uh, and then, uh, in the bottom, that's the isolation, uh, we did between the host machine and a separate box. All right, so what, what is inside the, uh, nitro system? Uh, so what you see is like the, the three main components of the nitro system is the nitro cards. Uh, these are, you know, very custom hardware like Sudeen mentioned in the beginning, uh, the custom hardware and firmware, um, that, uh, to isolate the storage and the block storage, etc. uh, including networking. And then the network security chip that is on the motherboard. Uh, the purpose of that one is actually like, you know, it. Uh, basically, like during the boot up of the host machine, it actually puts the host machine in a reset mode, uh, to make sure that only the known hashes of the hard uh hardwares, um, firmware is actually booted. So that's uh what it checks and if there anything uh changes, uh, then it, the system won't boot up. So that's how we make sure that, uh, there are no any, uh, vulnerabilities, um, uh, no, well any any non-known state, uh, firmware is not loaded, right? Uh, and then the hypervisor itself, right, which is, uh, like I mentioned, it's a very, we after we strip out everything from the hypervisor, uh, all the operations that you see on the left side. Uh, then it became just, you know, all it's doing is just memory and CPU allocation, right? Then that's how we are able to get bare metal, um, like performance on the virtual machines. Um, so Uh, yeah, in a sense, uh, now you have the host machine and then we separated out the box, right? Like where, uh, you have the security, networking and storage, all these are separated, uh, and that is the nitro system, and then, uh, we are able to dedicate more resources, uh, so we are able to get more utilization, uh, for ourselves so that we can be more cost effective and pass on those savings and also able to launch more and more new type of instances much quicker, uh, and then. Um, you know, good benefit of like, the isolation is actually what is bringing the confidential competing capabilities, um, and to talk more details, I'll pass on to Sudhir, um, uh, before I pass on, uh, you can, uh, read more about the, uh, nitro system. Uh, there's a white paper you can scan that. Um, we also, um, had it validated our claims, uh, through a third party as well that you can find in this documentation as well. And we, uh, we went even a step beyond whatever the security claims that we are making, we actually added to our service terms, uh, that also, you know, no other cloud provider has something done something like that, um, so with that, uh, I would want, uh, you know, uh, so to kind of dive deep into nitro enclaves so that. And nitro TPMs. Absolutely, thank you. Thanks, Suresh. All right, so hopefully. Dimension one of confidential computing and the solution to that which is simply any Amazon EC2 instance is clear to you, right? In, in other words, this is my favorite phrase to say, right? Amazon EC2 we call it always on confidential computing. And the reason we use that term is precisely because of this. AWS nitro system. Gives you that isolation protection assurance that we as AWS operator are isolated from your instances. There is no mechanism for us to get into your instance even if we wanted to, and that's by design and like Suresh said that uh unless let me just go back a slide unless you got a chance to take that, uh, QR code. We have elaborated about the security properties of NATO system in much, much more detail in that white paper, right? Uh, so please give it a read. It's a very interesting, uh, white paper at least for me. So now let's move on to dimension two, right? So dimension two of conferential computing, what was that? Uh, it was where customers asked us that give me tools, features, services to isolate. My own workloads and go build that trusted execution environment so that you can run your sensitive components whether it's a microservice or it's an analytics application inside the TEE, right? So that was dimension 2. The answer to dimension 2 on AWS, there are multiple options. So today in this session, what we'll do is we'll visit those TEEs. And we'll actually look at how attestation works for them, right, uh, because the talk is all about attestation, and I want to demystify the attestation for you. So let's get right into it. The first option for dimension 2 TEE is AW claves. I'm gonna be very, very brief here for AWclas because we got to get into the code, but here is what an enclave is. AW9 enclaves is an easy to service feature. That's the important bit, right? It's not a net new AWS service that you have to go consume. It's a feature of Amazon Easy 2, and what it gives you is an isolated execution environment. Like I said, so that you can bake your sensitive code and data that you want to protect from unauthorized access into this enclave into this isolated exhibition environment, right? So there are certain characteristics that you need to be aware of when it comes to enclaves, but the important thing that I would like you to remember is N enclave is not a small little container. It is not. It may look and feel like that, but it's a full blown virtual machine, so you could allocate. The amount of resources that's necessary for your workload, you just have to find the right size of EC2 instance, which you would anyways to run the workload on the EC2 directly instead of enclave. So the mental model here is. It's a full virtual machine It's constrained for the right reasons. It has meaningful defaults, it is very opinionated. So what are those characteristics that make it very opinionated? Let's look at it. An enclave does not have EBS attached. If you're familiar with EBS volumes, you cannot attach one to enclaves. What it gives you is a pure in memory file system. Enclave does not have ENI attached as well. Right, so if you, if you want to use your security groups and knuckles to communicate to the outside world, you cannot do that. There's simply no way it's isolated, it's air gap, it's cut off, remember? Um, and there are, there are other properties as well, but the most important property, which is what the session is about, is cryptographic attestation. So what is cryptographic data station? Well, we're talking about TEE trusted execution environment, right? And we're talking about runtime protection. We are talking about, we are not talking about transit or at rest, if you recollect from my opening statement. So the question simply becomes if I. Tell you that hey, I'm running an API and by the way, I'm running it in an enclave. And let's play with this. Let's say you are a consumer of that API. How would you know at one time that I'm actually in fact running the API inside a TEE versus a simple Amazon EC2 instance? It's not simple, but I'm just relatively speaking, right? That mechanism through which you can attest to the fact. That a service or your execution environment to be precise is a TEE is called cryptographic attestation. I have a slide for that. We'll come to that later. But let me talk about the second option. Like I said, there are multiple options for dimension 2 on AWS. There are multiple ways you can bake a TEE. The second option is using a TPM. Amazon EC2 again has a feature called Nitro TPM. You can enable Nitro TPM on EC2 instance when you create an army and use the army to launch the EC2 instance. It's in the, uh, uh, you know, it's a simple two-step process, right? You need to have the right TPM drivers, etc. But if you're already familiar with TPMs, Nitro TPM is a TPM 2. spec compatible feature, meaning any of those Go TPM or TPM tools that you are probably familiar with will just work drop that simple, right? Uh, but. What is nitro TPM based TE? Well, I have a slide on that later. It's a little bit busy. I will save that option as a solution, uh, towards how you can use a TPM to go with a TEE, right? But the idea is simple. The mental model just like an enclave is an isolated execution environment which is cut off even from the parent TC2. A TPM based TEE will give you the flexibility, will give you the ability to make an Amazon E to instance itself, the entirety of it as a TEE. So if I speak in terms of attestation because Nitro enclaves, I said, has cryptographic attestation, we look at it in the code. Similarly, Nitro TPM because it's a TPM 2.0 compliant feature, will give you the ability to get a TPM code. TPM code is used in at station. We'll look at the lid. All right. So hopefully now the two options of attestation is clear. So let's spend some, let's spend some, spend some time, excuse me, on what is at station. I'll take the nitro enclaves example. In case of nitro enclaves, an enclave has the ability to fetch something called an attestation document. This attestation document is, uh, if, if you're familiar with document formats, JSON is a very popular one. This happens to be in CBOR. CBO is concise binary object representation. It's an acronym. The reason we use that is it's lightweight compared to JSON. It's uh very fast in terms of parsing and verifying, hence why we chose CBA. But the most important property is this attestation document, regardless of how it's, uh, formed or created is signed by the Nitro hypervisor, right? And, and what, what it lets you do because of that fact that it's signed by Nitroypervisor is it enables you to do remote at a station. So if you go back a couple couple of minutes ago when I said how do you confirm or verify that the entity or process you're talking to is in fact running in a TE this is exactly the mechanism remote attestation is through which you can ask an enclave, hey, give me your identity, and then it provides you an attestation document and that attestation document is what you will verify. At one time to attest to the fact that this TE has certain properties that are desirable to you, this TE has some fingerprints or measurements that are known to you and if you want to proceed further with communicating with this TE or exchanging data with this TE, so on and so forth. OK, so enough about at a station. Uh, just one last add on to this. This is very specific to nitro enclaves. AWS nitro enclaves has out of the box integration with AWS KMS. What I mean by that is AWS KMS knows how to verify pars and consume an attestation document presented by a nitro enclave, right? The reason we did that is to make it easy to consume attestation. It does not mean that AWS KMS or for that matter any AWS API has some special privilege or connectivity to the enclave. No, anything out of the enclave, whether it's AWS or non-AWS, is outside the trust boundary of the enclave, right? The same should hold true regardless of which TEE option you choose. The same thing should also apply for the TPM-based TE. So if you're curious, why do we have two options when to use which? Well. Here's the mental model for that, right? AWSnron complex is an opinionated TE. It has a lot of opinions. It has a lot of meaningful defaults. It has constraints on what you can do or cannot do in terms of disk attached, ENI attached, GP attached, etc. TPM based EC2 on the other hand, I would put it on the other side of the spectrum, the other extreme end, where we are giving you a tool called TPM so that you can go build a TEE. Which can, if you choose to. Be exactly the same as the enclaves, so you can't figure that it in such a way that you can make an enclave out of it, or you can fit anywhere in the spectrum where you can say that, hey, I'm actually OK with the ENI attached. I'm actually OK with the disk attached because I know how to handle encryption address using tools like Lux, etc. right? And maybe your workload needs GPU attached. So maybe you're like, hey, I need a TPMSTE where I want to go launch it on an instance like uh. Uh, G5 or PF5E where there's accelerated cards available for me, right? So depending on your workload, depending on your needs, depending on your, um, uh, properties that you designed for the TE TPM based TE might be the answer for you, right? But if you want to just get up and running quickly, you don't need all those belts and wheels on TPM, uh, enclave TP TE sorry, might be the answer, right? So, so that's the mental model, more on the later, um. Now, what I would like to do is jump into the code, and to put some context around this, we'll take a very simple example. And to talk more about that multi-party competition example or multi-party collaboration example actually I'll pass it on to Suresh. So the multiparty in this one, just let's take a scenario and this is the scenario that we actually gonna show you the code uh as well. Um, let's take a very simple scenario where, um, you, you, um, there, there is a customer who wants to consume a model, a, a large language model or any kind of foundation model. And That consumer needs to know that it is actually really running in a trusted execution environment, right? There is no exfiltration is happening like, you know, they need to make sure that you know all the code that is uh necessary to make that is like the cus customer is actually verifying ahead of the time, right? And uh the model provider before they distribute the model to customer environment. They need to make sure that customer is not stealing their IP, right? It is not, they're not making a copy of the model weights and then, you know, doing something else, right, because it is not running inside their environment, it will be running inside a customer environment. Uh, so that is the scenario that we want to talk about. So there are two parties here, the consumer of the LLM and the LLM provider themselves, right? Uh, so LLM provider wants to protect their IP, which is the model weights, and the customer wants to protect their data, and, uh, they, they, they want to make sure that their data is not leaving to a third party model provider, uh, and it is indeed running in a trusted execution environment, um, that even their own administrator, you know, cannot see what is going inside, right? So that, that's the dimension to that we just spoke about, um. So this is the scenario that I just explained, you know, customer wants to protect the prompt and the query, and the model provider wants to protect the, the large language model. Now, how do you do that? How do you accomplish that, right? Uh, so the first thing that we do, uh, in this case is like the, this is the, the model provider is doing all these activities, right? So they have trained a model, and you have the model weights. What they can do is like they can use KMS, the AWS key Management Service, uh, and then they can request a data key. OK? So the data key, uh, is used to encrypt the model weights, right? Uh, so what they'll do is like they'll take the, uh, data key. And they'll take the model weights, the encrypted data key, and the model weights, and then they can put it in S3 bucket. Right? And then, What they can do is like they can attach a policy to the S3 bucket, um, saying that only, only if the. Uh Only a trusted execution environment can decrypt it, right? So that means like, uh, if when the consumer, uh, is actually launching this model. Uh, they, what they'll do is like the moral weights get actually downloaded, uh, into the uh trusted execution environment, in this case, the Nitro enclave, and then inside the nitro enclave, then, then only it can be decrypted because the data key is also encrypted in the S3 bucket. It can only be decrypted by the um enclave. So that's what the KMS policy can do using the um PCR values that which so they will walk into the details. Uh, so that's how uh this can be done. And then what you see is a busy slide what you see, uh, so I'll kind of walk you through. So, uh, you have a network like the purple box, and inside that you have a easy to instance and then on the right side you have a enclave. So the model weights are actually on the top, you can see the bucket where the model weights and the data key is encrypted, and then it has a policy that it can only be decrypted inside the nitro enclave. And from a customer standpoint, like customer who's actually accessing this model. Uh, they can actually attest to that it is actually indeed the trusted execution environment before they actually, you know, send the query. So these are the two things that we will see in the code. Um, and then, uh, you know, he'll, uh, in the, in the, in the core explanation, I think it, it will get more clear on all the what are the different proxy calls out there and, uh, how the moral weights are encrypted, etc. So it's since it's a code talk, let's get dive into the code details and then you will see all of this in the action through the lines of code. Cool. Awesome. Thanks. All right, let's jump right into the code. I'm gonna switch screens here for a second. Excellent. All right, uh, by the way, we're gonna look at two pieces of code today. One, the very first one up on the screen is all about attestation, right? I'm gonna show you how the attestration APIs look like in code. We'll use Python, by the way, we'll keep it very, very simple. It's like a hello world, um, and then the second piece of code is gonna be what Suresh just talked about. It's gonna be a multi-party collaboration example with LLMs. We'll look at that code as well, right? Uh, and if you want to follow along all of these code samples, uh, I would say grab this QR code if you want, this is a link to a GitHub repository where you have the entirety of links for today's session and beyond, right? So, so take, take, take, take that and then I'm gonna switch over back to the code example here. Is the code visible? Anybody wants a bit more zoom in? It's good? OK. All right. So the way we have structured this is. Nitro enclaves This is gonna be the first TE is what we'll look in look at in this code, right? It's a it's a TEE based on nitro enclaves nitro enclaves, the way it typically works is you partition your code, you separate your code. You don't have to necessarily do it, but you know, uh, assuming that there are only certain parts or components of your workload that are sensitive compared to the others. The mental model is you separate out the little microservice or larger monolith that is sensitive that you want to package into an enclave into something called a server for the purpose of this example right? and then the remaining portions of your workload is what will run on the EC2 which in this case is called the parent EC2. That's a client. So I've arranged the code into two folders client and server. And the build experience or the development experience with enclaves is pretty straightforward. What I'm gonna do is Let me zoom in this quad a little bit. What I'm, what I'm doing here is. Nitro CLI Nitro CLI is an open source command line interface. It's a tool that we open sourced. Uh, it's in the links that I shared in the QR code that is the one stop shop pretty much for night enclaves. You can use the tool to build enclaves, launch enclaves, terminate enclaves, pretty much cater to the life cycle of the enclaves, right? Here What I'm doing is I'm taking a small little hollow world example, containerizing it, containerizing the application that you want to run inside an enclave is a prerequisite. If you use nitro CLI, there are other ways to go with an enclave. Beyond the stock, but please go browse those documentation links, right? But as far as NTO CLI goes. It's a two-step process, containerize your application. So in this case, what I'm doing is I'm doing simple docker build, right? Uh, and I'm taking what's inside my server. I'll show you what's inside the server. Inside the server What I have is a rust application. There's no opinion on what can run inside an. It can be any programming language, anything that runs on a Linux. Operating system is OK here, right? The default operating system that we bundle with is a Linux kernel. Inside the enclave. So what I have here for the purpose of attestation is a small little program called attestation document retriever. Since this is written in rust. The first library that I'm using here. I called NSM API. NSM stands for Nitro Security module. NSM is a device. Emulator device Only available inside an enclave. You cannot do any of these attestation APA calls outside of EC2 because if you recollect it's a special property of TE of an enclave, right? So I'm using NSM API here. And The API to retrieve an attestation document. If I can find it here real quick, is. Get at a station document and that API takes 3 inputs. A public key Enhance and user data. Right. Uh, the reason you have public key, a, a little theoretical aside is. Every enclave as part of the attestation document can embed a public key inside the document itself. The relying party or the party which receives this attestration document. When it wants to send sensitive data or code only to be used by this enclave can use the public key to encrypt that information, right? So that's the reason for public key. nouns. You might be familiar with it from sessions, etc. and the answer is just to uh ensure that attestation document is not stale, it's freshness. Sometimes it is also used to mitigate replay attacks just so that you know, you know, hey, the client sends an no the server replies back with something, including the nos, so that's how the client knows it's talking the same language, it's talking in the same session, right? That's no and user data is, uh, you know, it's user data. It's, it's a section in the attestation document. That you can use to embed certain things for your application, right? Uh, a lot of use cases there, but end of the day, uh, NSMAPI is what you will use to send these three inputs when you request an attestation document. Like I'm doing here in the court and in the response you get back the Attestation document Bay 64 encoded end of the it's a C bar document, right? So all my server is doing here is getting the response, getting the attestation document in the response, and sending it back to whoever is calling this. That's it. Now if you go to the client. Now client's responsibility. Let's talk about cryptographic attestation, right? Imagine client is. A consumer of an API who's trying to ping the API running in the server and part of the handshake, the server is responding back to the attestration document that we just looked at, right? It's a Cboard document. One of the primary responsibilities of the client is to verify the attestation document. That is precisely one of the things that I'm doing in this Klein.ie in this Python program. Verification of attestation document is done using a couple of things. One is called semantic validation. Semantic validation is where you ensure the integrity, the structure, the schema of the attestation document is intact, meaning in other words, it's a valid attestation document because attestration document has a schema. As simple as that. The second step, this is the most important step is every attestation document I said that again, I'm gonna say this again, is. Signed by Nitro hypervisor. The PKI infrastructure behind the signing process, the route certificate to be very precise, is something we have made it public because for the right reasons you would want to verify the signature, right? It's available on a S3 bucket. What we use for the signing process is ACM PCA, two acronyms. If you're, if you're familiar with it, it's private certificate, the private certificate authority. It's a service, the service is called ACM. And there's a way you can get private certificates out of the ACM that are typically valid for 30 years. So an enclaves signing certificate is valid for 30 years. It's available publicly. So that's exactly what I'm doing in this code. I'll show you. I'm verifying the signature of the attestation document. Once I do these two, and once I know that it's a valid attestation document and it is signed by NA hypervisor, then I can proceed to parse the attestation document and get to something called PCRs. I'll show you that. All right. So We just quickly collapse the core. So All right. Um, so here, my client Python program is initiating the connection with the server or something called VSOC. If you're familiar with socket-based programming. VSOC is also called as Verta VSOC. The address family is AF VSOC. It's not something AWS has invented. This was done by Red Hat. It's already upstreamed into Linux kernel, so this is not new. You probably already have the drivers to talk to AFV sock sockets in your operating system already, right? So what I'm doing here is, and, and by the way, if you're curious, there are only a bunch of languages that are um. That have the native language bindings to talk to a VSOC based socket, right? So if you happen to use something like Java, you have to fall back onto native uh integrations, run native code written in CC++ or run a binary. You have to do some some sort of foreign function interfaces in those non-supported language. Python luckily for this purpose of the demo is one of those languages that support supports. VSOC. So, what I'm doing here is I'm opening up a VSOC, and I'm sending a nonce. Right, um, let me just get to that. No, it's right here. I'm opening up the VSOC and I'm trying to send the Nans to the server and then the server, you looked at the code, all it's trying to do is fetching an attestration document with the inputs that I have given to it. Important point here is if you recollect public key was one of the input to attestation document API. I am not sending the public key because my client is outside the enclave. Client is outside the trust boundary. All the client is doing is, hey, I want to talk to a TE. Give me your attestation documents so that I can verify you are the right TE. And just to make sure I'm mitigating replay attacks, I'm sending you an answer that's all, right? So this kind of by simply sending an answer, receiving an attestation document, and, and then I'm trying to do those two things that I elaborated, right? Your semantic validation and then verification of the signature. So where I'm doing that here just so that you get the sense of code is right, right here. It's part of verification of attestation document we'll get past the semantic validation, which is, hey, does it have the necessary fields that I need which is PCRs. PCR another acronym is platform configuration register. So when it comes to TEEs, you just have to remember this. This is like the thumb rule, right? You will measure certain things about the And then you verify those measurements, that's how you know your TEE hasn't been tampered with when it was built. Compared to when it is running, that's the whole point of a TE, right? Your execution environment, you want it. To be stable, not tampered with integrity intact, etc. So measurements are key. PCRs is just on the name for measurements for hashes. In this case, in case of enclaves, we're using SHA 384 as the hashing algorithm, so you have SHA 384 hashs, which is what you will check for in the attestation document after you verify. So in the verification process. I'm downloading the root certificate. And then I'm verifying the certificate chain that is included in the attestation document is intact, right? So there are 4 certificates to be very specific. Uh, each one of them, if you look at the details of the certificates, uh, corresponds to something like the instance itself, the A, and the overall region, and then it just bubbles up from there, right? So there's 4 levels of. Geo hierarchy if you will and we are attesting to to the fact that it's exactly the same right the instance in the AZ in the region, etc. and then of course the final final certificate, the leaf node corresponds to the enclave itself so there will be a common name on the certificate which literally tells you instance ID. Hyphen or underscore enclave ID right? So you get the idea you're trying to pin it all the way down the hierarchy with using these multiple certificates and then you have the root certificate that you use to validate this entire chain. So that's exactly what I'm doing here using just off the shelf tools in case of Python. I'm using a library called cryptography and once I'm done with validating the signature. I'll move on to Consuming the PCRs right now I know semantic violation is done. The PCRs are there. The non is there, my user data is there, and the signature is valid. So now I'm actually OK to go past the PCRs, which is what I'm doing here. And PCRs, one of the property you probably know this if you are working with TPMs is the value of a PCR when you mutate it. is not getting overwritten. It retains the history. In other words, if I give you a PCR hash. The way you arrive at the same hash is, I'll give you a naive example, right? So let's say if I'm telling you that, hey, my TEE is running. Lama 7B. Lama 7B is LLM that's inside my TE and that Lama 7B happens to have a GGUF file just terminology, but the end of the day you have a file and you can measure the file. So I do a SHA 3 to 4 hash of the file. I'll take the hash and let's say I decide to put it in one of the PCRs. The operation is called extending. You're not overriding so whatever value was there previously combined with this new hash is what results in a net new hash, net new measure that goes in the piece. The reason we do that is if you want to know whether the asset or artifact that you're trying to measure has been tampered with. You get a net new hash that's different but inclusive of all the previous states if that makes sense, right? That's how you know something has been tampered with. Because if you simply had the ability to overwrite something you would just overwrite it and say that hey, my client recognizes this particular PCR why don't I spoof the PCR, right? You get the idea. So once I verify the attestation document, I move on to PCRs and then. I'm moving on to further attributes like nons we talked about nouns and then finally time stamp. There is a notion of a pseudo TTL. I'm not calling it as a TTL because there's no attribute called TTL on that testation document itself, but there is certificates, those certificates have expiry time on top of my head. I think it's about 3 hours, give or take. So if you have an attestation document that is valid but it has an expired certificate, go ask for a new attestation document, right? You want to verify that it is still fresh, in other words, all right. Uh, so you get the idea. So this client.ie is, uh, going through all the steps that you would have to do to verify an attestation document presented by a TEE. That attestation document has certain interesting properties like PCRs. End of the day you live and breathe PCRs, right? Your TEE is PCRs for you. Your TE has to be measured and your number one responsibility as consumer of the TE or uh company that talks to the TE is to make sure those measurements are known measurements, right? So essentially my client or pie is gonna get to those PCRs and then I'll do a simple check saying that hey. If PCR 0, I know I haven't introduced you to what those PCRs are, but PCR 0 is a measure of the entirety of the enclave. So even if any single bit of code changes between the kernel, the operating system, the kernel flags, command line parameters, intraFS, finally your application, any of those change, the PCR 0 is gonna change. And it's a one way hash function, so there's no way you can reconstruct it unless you know the original components that went into it, right? So my client or pie has to confirm that the PCR is a known hash that it can trust and only then it will proceed to consuming the service process or what have you that's running inside the TE hopefully hopefully that's clear. Everything else in this example is just, I would say logistics behind consuming CO. I know I mentioned that this attestation document has a data protocol, serialization protocol called Cboard, so you use a library to marshal and marshal between Cboard to JSON, to strings, right? But that's about it. Now, let's look at the build experience back to back to how to consume this. Nitro CLI takes a prerequisite, which is a container image. So the first and foremost thing that I'm doing is I'm taking my server, all the code inside the server, and. Building it right, so rust has its own tooling on how to build and arrive at a rust binary. That's exactly what I'm doing here and then I'm using Docker build as my packaging mechanism. Out comes an image, right? That is the image that I'll use in the next step to go build an enclave. Here what's noteworthy to call out is pretty simply Nitro CLI. Nitro CLI has a command called build Enclave. It takes the Docker URI or wherever, whichever registry your Docker container is, uh, posted to in the previous step as input as simple as that, right? And the output of this build command. Build Enclave command is simply an EIF file. EIF, an acronym stands for Enclave Image file. Enclave image file has its own image specification. It's in the links that I've shared with you earlier. Uh, take a look at that. There is, there is a certain, uh, that image specification will help you understand. The headers, the sections within the enclave image file, how the simple init process works within an enclave, because like I said, enclave feels like looks like a container, but it's not, but there are certain things that are container like the number one thing you probably want to know is inside the enclave you can only run one process as part of the in it as your pit one. You cannot, well, you can still run processes in the background, but, but the idea that I'm trying to paint is just like container, there is this notion of you do the bootstrap and then kick off with just one process, right? Just, just bear, uh, keep that in mind, uh, in other words, there's no systems D. And that's on purpose. We kept the Inuit process very simple. The in process is there in the image specification as well, right, uh. In the interest of time I'm then just gonna move, right? So you will see that the theme repeats nitro CLI like I said, caters to the entire life cycle. So I've used it for the bill. You will see that I'm also gonna use it for. Launching the enclave. So launching the enclave is in run run server.SH. It's a simple shell script. All it's doing is, uh, it's terminating any existing enclaves. Uh, by the way, if you're curious, you can launch up to 4 enclaves from a single EC2. So what I'm trying to do is just clean up anything existing and then I'm launching the enclave itself. Now the important thing to note here is. When you launch an enclave, this is the uh options that you have. You can choose how much VCPU and memory you want to donate to this enclave, right? Uh, that is one of the configuration parameters that NIO CLI accepts as part of the run enclave experience, right? So that's exactly what I'm doing here. I'm saying NIO CLI go launch this enclave for me. Here is the file. Here is the CPU account, here is the memory. And by the way, with VSOC, you get a couple of things that you can read upon just like IP addressing scheme you have IP address and port. VSOC has context ID and port. Context IDN port is what you will use as your addressing scheme when you talk to an enclave or an enclave talks to the outside world because that's how you navigate the VSOC channel, essentially. OK, um, just one last thing, uh, there is, uh, something called debug mode when you launch an enclave. This is purely for development and testing purposes. So if you are curious to know what's happening inside the enclave, uh, what you get out of debug mode is, uh, the, uh, is, is the standard out. Whatever is being printed to console inside the enclave will be piped out to your console wherever you are running this command. The only reason I bring that up is how do you identify when it comes to attestation whether the TEE is running in debug mode or production mode? Well, the answer is very simple. If PCR 0, which is the entirety of the enclave, the measure of the enclave, or PCR 1 and 2, PCR 1 and 2 are also measures of little different things about enclave. PCR 2, most importantly is a measure of your application code. So anything that you containerize in this case, that simple last application, that results in a PCR 2 hash. So these PCRs, when you run an enclave in a debug mode, are all zeroed out. That's the number one indicator that you're talking to a TE in debug mode. In other words, you probably don't want to unseal your sensitive data to something that's running in debug mode, right? That's purely for development testing purposes. So that's one thing I'll call it and the rest of, um, the code is pretty simple. All I'm trying to do is launch an enclave and then finally just describe the enclaves. Uh, describe enclave is just, uh, to spit out some metadata about the enclave that's running right now currently. So that's about it. That's a simple code, a hello example. This, by the way, is available on those links. Uh, we posted a report card if I can get to it. Um, AWS nitro samples. This is exactly is there. How to consume an NSM API. There are a bunch of other examples there as well. So if you want to go try it out, all right, so that was demystifying attestation, um, attestation document. What are the fields in it? How do you consume the NSM API, and most importantly, how do you verify before you consume the PCR values because like our toes, you want to measure and verify your TE, otherwise there's no point in consuming a TE. You're just blindly trusting whoever you're talking to. So hopefully that grounded you in attestation primitives. Now we're, it's time to move on to the next example for that I'm passing on to Suresh. Yeah, so now that you understand how the attestation process looks like, uh, in the form of code, you know, I briefed you on that example of like an LLM provider and a client, and this code actually kind of walks walks you through that. So this is basically an extension of what so they actually showed, but with a real world use case where we are, you know, creating a, you know, um. Data key from KMS and then encrypting the model weights and then putting it in an S3 bucket and then uh you know and sealing them um what do you what do you mean by sealing uh making sure that it can only be decrypted in a trusted execution environment that matches with the PCR values that so they just spoke about, right, only can be unsealed and that's the key, right? So that actually guarantees the runtime guarantees. And like I mentioned, some folks who miss the beginning, this is about runtime, um, you know, security guarantees, right? Uh, um, so let's get into the details. And by the way, this example you can follow, uh, we posted this as a workshop which you can run. Obviously the experience is gonna be a little bit different. This is all scripty because it's a code walk through. The workshop has much more streamlined. You'll get right into sealing and unsealing those two actions that, uh, Suresh was talking about. He's. So it's similar to the previous example, and in this case, the server is where uh the code where it um the the the code inside the server is what we are kind of planning to package inside the Nitro enclave. And uh uh we have a Docker file. The first step is to build the application Docker file. And uh the build build application is exactly doing that. It is taking a base image and then creating uh what called LLM server and in this case, what you see here is like uh the Docker base file. It has all the dependencies that it actually requires, you know, you know, as you see, like, you know, just primitive, you know, base Linux image and then installing lots of dependencies that it actually requires. There's nothing special in the base image, but in the darker file. Uh, this is what get built from the base image. And in this one, what we are doing is like we are building, um, you know, the OLAMA installation. OLAA is basically an inference library that helps you to host an LLM, uh, in this case like a LLM in the format of a GGUF file. Um, so that's what we are doing here. uh, so that once the application is built, then the next step, uh, that what we are doing is, um, basically. Uh, so if you see that in the RB file like the envelope encryption models, um, shell file, so that is here. What we are doing in this one is basically using uh KMS. Uh, we are getting a data key and then uh we are basically getting the encrypted data key into this particular file. And then we are also getting the plain text format of the data key and then using the plaintext uh format of the data key, we are actually encrypting the model weights and here you can see, um, the API key is um. Inside a, uh, I mean, the, the encryption keys inside this one, and then we are taking the model weights that are downloaded and then encrypting with that key. And then finally uploading the encrypted. Um, data key And the encrypted model weights that is encrypted using the data key. Um, and all these files are basically uploaded onto the S3 bucket, right? But, um, Now, how do you make sure that you're sealing it, right? You need to make sure that you update the KMS policy that it can only be decrypted inside a nitro enclave. So that is the next step. So if you see like uh uh before we go into that um you know, ceiling part, let's go into the building, the enclave part, which is basically taking the uh uh the container that we just built and then making sure that we are actually building a. Um, yeah file. So that is going into the build enclave one. So let's go into see that and uh what you see here is very similar to what uh Sudesh showed earlier and, uh, you know, we are using the same nitro CLI, uh, to build the enclave. And one important thing that you should, uh, these three things are already kind of mentioned in the previous example. Uh, what we are looking here is basically like providing the certificate as part of the measure measurement, so that, uh, that certificate will be distributed to the client, that the client can actually verify that it is indeed that particular um image. And uh here we are, uh, finally just building the enclave EF file. So is there anything to add here, um, yeah, yeah, it's the same primitives right by now you probably noticed that we are still using NIO CLI. We are still using the same build on curve primitive. You, you should get the idea by now that beyond the basic PCRs, beyond the basic measurements, you still have the capability to measure characteristics of the TE that are important to you, whether it's an application code, whether it's a measure of a configuration file, etc. and then use. NSM API to extend a higher order PCR anything that's above let's say 16, you got 32 PCRs in total at your disposal. Use one of the PCRs to extend the hash about things that are interesting to you in the TE and then verify and attest to them. In this case, what we're doing is we are measuring and hashing the. LLM model weights because I want to know that I'm talking to a TE B the TEE hosts my LLM server, and C. It is serving a very specific model to me, not just a general LLM, right? Maybe it's a specially tuned LLM, fine tuned LLM specific to my domain, right? So you get the idea. The primitives remain the same. Yeah, yeah, so we have 2 minutes, um, just wanna quickly run through the complete example. So here is where the, the 4th step we're sealing the model so you can see like the KMS policy is updated that it can only be uh decrypted that matches the KMS policy that we have created and then finally, um, like the client. Uh, the client is the one that which is very similar to what Sudhir has spoke to in detail in the previous one, making sure that it is checking the right PCR values are there, uh, and then, then only it actually makes the request to the model in this case, uh, so. And and all the busy lines of code that you see there are just proxies. So if you look, look at the architecture diagram and again it goes back to the fact that your TE. Depending on your use case, restricts the network access to the outside world for the right reasons, right? So, uh, in case of night enclaves, to be very specific, any time that you want to talk to an external service, which is pretty much anything that's outside the enclave, you got to go to the VSOC channel, hence the need for some code that can adapt VSOC to INET and vice versa, right? So in this example, in this case, in the workshop as well, what we have done is we have decoupled. The VSOC to iNET marshaling and marshaling using adapters. We used a pretty popular open source tool called SOAT. You can write your own adapter as well, but the idea is to decouple my application code from the adapter code from the bridge code, right? It's just a design pattern that we chose to use. Yeah. So from a visual standpoint, this is the one, right? Like you can see the VSOC in the middle. Right between the host machine and the enclave, and those are the things that you are actually seeing here in the code. Uh, with that, uh, we have 2 minutes of any questions you can take, yes please. Yeah, that is not. So the question is, is there any part of the EIF file that's not measured or attested by PCR 0? Well, EIF file again this is in the image specification as well uh PCR 0 yes it measures the entirety of the uh enclave image, but there are certain section headers between components of EIF. I think we probably have a visual as well in the, uh, in the image specification report. There are certain things that does not make sense to measure, right, because it's like measurement about measurement. For example, if there's a EIF section and then there is a signature for the EIF section. You probably don't want to double measure it because the signature is attesting to the fact that what what it's signing is not tampered with, right? Um, so yes, there are some nuances, but from a practical purpose, yeah, the idea is PCR 0 just caters to the entirety of the, uh, ongoing image file. Good question. Yes, just confirming that nitro CLI is separate from the AWS CLI that you use? That's correct. Yes, the question is, is, uh, nitro CLI a separate component? Absolutely it has nothing to do with AWCLI. Nitro CLI is, uh, something that we have written very specifically for nitro enclaves, and we have open sourced it, uh, but the reason we did not call it nitro enclave CLI and just nitro CLI is the idea is it should cater to all the TE options on AWS and not just enclaves. It just happens to have sub commands. That are arecle specific, right? But yes, it's a separate piece of code. And it's open source as well, so there's a GitHubre for that. Uh, and again, you know, if there's any questions, uh, our contact details are on the last slide, please reach out to us and most importantly, let us know what else you would like to see in a talk like this. The idea behind this talk was to just deep dive right into at a station. Uh, we just spent a few minutes on introduction just so that we are all on the same page, but if there's one thing that you would take away from this talk, it would be this, right? On AWS there are multiple options to build your TEs. Enclaves just happens to the first one that we have launched. It's an opinionated TE. The other TE, the TPM based, nitro TPM TE is probably what gives you the most flexibility as of right now. That is generally available. The only prerequisite is you just need to find an instance where both enclaves and TPM are enabled. And the reason for that real quick. It will take a full hour to explain that, but the reason for that is, uh, TPMs, if you're familiar with it, they support at station but they do not support remote at station on AWS. So we use enclaves in that architecture in the topology only to support remote at station. Otherwise, in a TPMATE, the entirety of your workload is running on the host CPU, not inside the enclave. Enclave can just come and go just to support at station, and that's about it. And that's the reason why. You could go build your accelerated compute workloads, workloads like, uh, LLMs, etc. on TPM based TE, whereas it may not be possible on night onl based TE if you want GPU based influencing. It is still possible because we just show you an example where we're actually running it on a CPU based uh instance where the idea is maybe for your workload CPU influencing is enough, right? Uh, so, so that's what I would like to leave you, uh, leave you with. And again, thanks for joining this talk. Uh, it's probably not one of those, um, super high level talks. It's probably a lot of boring details, but these are very important boring details, at least in my mind. So thanks again. Thank you all.
