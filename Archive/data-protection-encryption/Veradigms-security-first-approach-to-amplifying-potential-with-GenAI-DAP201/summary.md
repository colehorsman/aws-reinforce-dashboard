# AWS re:Inforce 2025 - Veradigm's security-first approach to amplifying potential with GenAI (DAP201)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=r_JwSp-mS34)

## Video Information
- **Author:** AWS Events
- **Duration:** 45.9 minutes
- **Word Count:** 8,568 words
- **Publish Date:** 20250620
- **Video ID:** r_JwSp-mS34

## Summary
Mike Peddada and Kyler Middleton present Veradigm's journey from prohibiting generative AI due to security concerns to building Vera, a private, secure AI chatbot for internal use. Despite having policies against ChatGPT, employees were universally using it for work tasks, creating compliance risks in the healthcare industry. Kyler demonstrates how they built a comprehensive solution using Amazon Bedrock, integrating with Slack, Teams, and Jira while maintaining strict security controls. The solution processes 250,000 Confluence documents, implements robust guardrails, and serves 450 employees with 6,700+ queries at approximately $4 per user per month, proving that secure, compliant generative AI implementation is both technically feasible and cost-effective in highly regulated environments.

## Key Points
- Organizations attempting to ban ChatGPT face universal non-compliance as employees find AI tools essential for productivity, creating shadow IT and compliance risks
- Healthcare and financial services require extensive legal, security, and compliance approval processes that can be successfully navigated with proper architecture and controls
- Amazon Bedrock provides comprehensive security controls including data privacy guarantees, encryption at rest and in transit, IAM integration, and regional data localization
- Bedrock Converse API enables model agnostic development, allowing easy switching between different foundation models for testing and optimization without code changes
- Multi-user AI in Slack/Teams creates unexpected use cases like conversation summarization and fact-checking that single-user tools cannot provide
- Guardrails implementation prevents financial advice (insider trading risks), blocks inappropriate content, and masks sensitive data like Social Security numbers automatically
- Knowledge base re-ranking dramatically improves response quality by intelligently selecting the 5 most relevant results from 50 initial semantic matches
- Open source approach with MIT license enables rapid organizational adoption while maintaining full control over proprietary data and customizations

## Technical Details
- Architecture uses dual Lambda functions: receiver handles Slack's 3-second timeout requirement while worker performs actual Bedrock processing
- Knowledge base ingestion processed 250,000 Confluence documents over 3 days using Titan text embedding models and OpenSearch vector storage
- Conversation context construction includes username removal to prevent overfitting and maintains structured JSON format for multi-turn interactions
- Re-ranker reduces 50 knowledge base results to 5 optimal matches in 0.25 seconds, significantly improving response relevance and processing speed
- Guardrails configuration blocks hate, violence, sexual content, misconduct at medium confidence thresholds with custom topical filters for financial advice
- Regional CloudTrail configuration required for Bedrock serverless model logging, with potential for sensitive data exposure requiring careful environment isolation
- Cost structure: $4/user/month static costs (primarily OpenSearch storage) plus ~$0.01 per query including all processing, guardrails, and knowledge base access
- Slack app registration handles @mentions, direct messages, and file uploads with webhook verification using hashed security tokens for authentication
- Document processing supports text, snippets (20k characters), PDFs, PowerPoints, and spreadsheets through Bedrock's built-in document understanding capabilities

## Full Transcript

Great, welcome everyone and thank you for joining us today in the session where we're gonna be talking about Veradim's, uh, security first approach to amplifying potential with Gen AI. I'm Mike Peddada, a senior partner solutions architect with AWS, and later on I'll be joined on stage by Kyler Middleton, a principal developer, uh, with Veradig. So for today's agenda we're basically gonna cover um a bit about generative AI we're gonna cover a little bit about bedrock and the services it has to offer. Uh, Kyla will then walk through her approach that she's taken internally with Verdigm about how she's gone about implementing Gen AI in her organizations, um, sort of the road map to the solutions, the steps she took, uh, some future enhancements she has, and, uh, where she can take this in the future as well. So to get us all started, I kinda just wanna touch up on Amazon Bedrock, and I'm sure you guys have heard a lot about Amazon Bedrock in the conference that reinforced this year. Uh, but I just wanna cover all the different models that we have to offer, all the different features and services that Amazon Bedrock has. Uh, we have a range of partners in the space, a range of models where you can go and pick and choose the different models that you want. Uh, we also heard that you want more variety, so we're consistently, um, adding new models and, and making sure that you're keeping up to date with the latest, uh, Gen AI models out in the market. But I kinda wanna switch uh modes a little bit and talk about generative AI and the security in the space. So, uh, this is the generative AI security scoping matrix. Um, here we have 5 scopes, so I kinda wanna break down these 5 scopes and how they sit in the Gen AI security space. In scope one you'll notice that we have consumer app which is the public aspect of the scope. So if there's chat GPT or any other public facing um Gen AI application that sits in scope one. Then we have Scope 2, which is our enterprise application, so that's a little bit more where you're hosted, you're getting AI on a SAS or an application, uh, that could be your Salesforce or Amazon code whisperer, uh, and then we have Scope 3. So this is where there's a little bit more of pre-training happening here, uh, we're able to build an app on a version of that model itself. So here this is where Amazon Bedrock comes into play because we have those pre-trained models for you at disposal. You can go in and see how those work. And then scope 4 comes down into fine tuned models. So fine tuning is where you're able to do a little bit more granular control on the data and the access on these models and how they work again, Bedrock can provide uh an example here or Sagemaker, which is our uh newer more updated version of how we do Gen AI in that space. And then we have Scope 5, right? So this is more a self-rein model. So if customers want to go build their own models, uh, we've had SageMaker for a long time in this space where, uh, customers are doing machine learning or building their own models and you're able to train all of that using your custom data. So across these 5 scopes we have um the different security components that sit across it right so a lot of the foundational security controls sit across all these 4 all these 5 scopes sorry uh so we have the governance and compliance, legal and privacy we have risk management, and we also have controls and resilience. So this is the mental model to think about when you're trying to identify what sort of, uh, generative AI model you're using and which scope it sits in and how those controls apply across those different models. So then now that we've taken the scoping matrix, I kind of wanna focus on saying that the bedrock service kind of sits majority in scope 3 and scope 4 so it depends on how exactly you're configuring or using the Bedrock service. Either you could use it pre-trained or you could fine tune it so there are different levels of um customization that you can do with the space. Kayla will also go into detail about how she's using Bedrock in her chat, uh, bot, and then how they've implemented in a paradigm. But I do wanna um talk a little bit about some of the security controls that come with Bedrock, right? So I'm sure that a lot of customers in the audience have questions about hey what is the security uh surrounding Bedrock and what is the privacy and protection here so we'll start with data privacy and localization. The biggest caveat, the biggest call out here is that you're always in control of your data. There is no, uh, mechanism for Amazon to store the data or or use it in any form of training, um, when, when using this model, so the data for a customer is always with yours. So it's not used to improve our foundational models, um, it's not shared with other foundational models, uh, you're always in control of your data. And so all prompts that you do or prompts that you take are all isolated per customer, so there's no sharing of the prompts or data across these models, uh, and they stay in the region where they were created right so we're localizing all of your, uh, models into one place into one region, so you don't have to go across regions. So then data security now that we're not using any of your data obviously and when it comes to data security you're still obviously in control of all of your data here. The first one is that we're always encrypting everything in transit, right? So we have a minimum of TLS 1.2. All the data can be encrypted with KMS keys, uh, to make sure that you have data at rest, uh, for encryption. We have fine tuned models that can be encrypted so depending on the model that you're taking, how they're implemented, you can obviously encrypt all your models, uh, again you have access to these customized models and how they be used within your environment. And then they also have a good security control of integrating with IAM so you can use IM to implement fine grain access controls on who can access these models, how they're enabled, how they're used, uh, where they're coming from. So you wanna think about the defense and death controls when you're implementing the security controls across Amazon Bedrock and, uh, what sort of, uh, yeah, security controls you wanna apply there. When we come to governance and auditability, there's obviously a lot of comprehensive things that you wanna do in the space, so we will export our metrics to Cloudwatch and Cloud Trail. These then end up in your account and the information is not logged within Bedrock or sent anywhere else. Uh, with our growing compliance suite, you can be sure that Amazon Bedrock, uh, will meet your regulatory requirements in this space. You're able to then monitor any API activity, troubleshoot any issues that you would have, and make sure that you're able to get that detailed analysis using cloud drill. So any logging aspect that you have, anything that you wanna trace back and see, Cloudrill is the place to get that information. And then so we do have a fleet of uh rate like growing increasing compliance standards that we're keeping up with. So these are just some of the compliance standards that we've met with today for Bedrock and and it's a growing space so keep watching out. And then additionally um the thing that we wanna cover is that that is the security of Bedrock and how it could be implemented but I do wanna talk a little bit about some of the security services around Bedrock itself, right? So what can you add to complement the service? So this is where Amazon Guard duty comes into place. So yesterday at the keynote we heard there was the extended threat detection which includes new generative AI capabilities. So guard duty is a fundamental security service within AWS where using threat detection, uh, machine learning and AI now we're able to go and give you a lot more. Uh, insightful, uh, detailed information about threats in your environment in your account so it can detect any suspicious activity in your bedrock APIs. Um, you can see if anyone's tampering with any of the bedrock guard rails, um, if anyone's tampering with any of the data sitting in your S3, which is being used for your model training, um, and additionally if you're doing any, uh, lambda implementations as well, it has coverage for lambda as well, right? So it gives that good coverage. Sorry, don't know what happened there, um, so the security services here is kind of just uh a mini architecture of how the implementation could look like if you are, uh, putting in different security services around Bedrock. So we have the customer BBC that's where your bedrock model would be implemented. Uh, you have the different services there again all the logging sits within that account within your own account where you're deploying bedrock and then complementing it you have the other different services so you have guard guard duty, uh, you can send all of your findings from guard duty into security hub and do the customization and the prioritization of the different findings there and then using that you can do an event bridge, um, or event related information pass through where if the security finding comes in with a critical finding you can then take a remediated action on what has to happen there. Again, all this information would be logged into Cloudrow, which is your primary source for any logging and monitoring within AWS. Great. So then we have bedrock guard rails which is not to be confused with the bedrock security controls that we just spoke about, but these are guard rails within Bedrock itself where you're able to implement uh guard rails for the model, right? So whether that's uh how the agents can be used, what sort of language is allowed, how do I do my prompt. Um, input and output configuration, do I wanna redact any sensitive information in my foundation model responses? Uh, are there any topics that I wanna deny, uh, or, or allow? Are all these, uh, configurations that you can do and you can set thresholds between what are the different values that we want. Again, there's a great example about how Kyla will talk about some of the guard that she's implemented in the chatbot that she's built, um, so without further ado. I do wanna introduce Kyler onto stage, uh, where she's gonna talk about Vera, the AI chatbot that she's built for Varady. So please welcome Kyler. Everyone. Hey folks, here you go, so y'all can hear me? Silence sessions super cool. All right, uh, I'm Kyler Middleton. I work at Verdigm in the health care industry in the United States. Uh, there are a lot of laws there. I'm sure a lot of you all are familiar with a lot of stuff in, uh, fin ops and healthcare and stuff like that, and we wanted to do AI and, uh, AI and security and regulatory compliance are often not bedfellows, so I did my best to try to bridge some of the gaps here. Uh, we immediately, of course created a policy that Gen AI is not permitted. There's no chat GBT allowed on our network. Um, how many people think it was 100% followed by everyone across our worldwide organization? Let's see those hands. You're correct. So we interviewed a whole bunch of people and it turns out. All of them were using Chat GBT. All of them were uploading their emails and documents and private stuff and thankfully not healthcare like related data, but I'm sure they would if they could because it helps you do your job better, right? Like it helps you sound better and sound expert and do your job better, right? So we need it. So this is me, that's a selfie of me over there in the corner. Uh, we're gonna talk about how we built a private chat GBT, so just a private Gen AI internal work a day app that we put in Slack, Teams, and Jira. We trained it on our confluence, our SharePoint, all of our security policies. There's about 98 very long, very dense documents, and we trained it on all of that, and it's available now today, uh, it is used a couple 100 times a day. So first of all we found a use case uh we're breaking all our security policies and potentially gonna get sued and you know etc. that's that's enough justification for building something, right? And, uh, working through the security, uh, with the legal team with InfoSec, we're in healthcare, so getting them to approve me building an AI thing was a struggle was a long journey. Um, gathering all of our data sources, uh, you can't just say, Hey, I go learn about this, go read our confluence, go do the thing. I mean maybe you can. MCP is coming, but I built this without that cool stuff, so we'll see how that evolves as we implement MCP for, you know, real time just in time learning about stuff. Um, how we use the Bedrock Converse API. I'll go into depth on all of this stuff and the re-ranker, which makes it quite a bit smarter and, and better results, guardrails so that it can't, you know, tell raunchy jokes at work or give advice on when to buy our stock, which is very good for, you know, insider trading, getting sued again, very concerned, and then the results which, uh, we'll see it working in Slack and Teams. I don't have any pictures of Jira because we just got that working, but that is coming. All I wanted was bedrock and slack. Like if there was a button on the bedrock page that said put this in your slack, I just would have clicked it. I mean, I wanted it to be, you see the list here private, secure, fast, intuitive, configuring, you know, it's such like I would have been uncomfortable and and rebuilt it myself anyway, but if there was a button, I would've clicked the button. There was no easy button though. I had to learn Python. I had to learn how Slack apps worked. I've never built one of those before that generate like web hooks to stuff, um, you're still jumping. That's weird. I had to learn how Rerinker worked and the converse API, which is incredible. I'll, I'll talk that up all day because it's amazing and it makes this work very, very well and OpenSearch, which is our sort of vector data store on the back end. Um, so let's start talking about it. Here's the final architecture. We're done. Is there any questions? Probably a couple questions. So, uh, first of all, users over on the left side, interface and Slack because that's where the users are, right? Like your users are in slack, your users are in teams. That's where they're hanging out all day. They're very chatty. They're putting all your company secrets in there, which is OK because you have a BIA with those companies. That's fine. Um, so what we did is register a Slack app. Also did this in Teams, by the way. When anyone inter interfaces with it, it sends a web hook over to the Va receiver. I'll talk about that a little bit, which is just a lambda that I implemented. I didn't use event bridge or any of the, the cool like, um, step functions or anything. I don't know how those work yet, but I'm working on it. This is just a lambda that calls the lambda. There's just the two. Um, if the viewer receiver confirms that like this actually came from our Slack because it's listening on a function URL, it triggers the view worker which spins up, does a whole bunch of AI conversations. You can see here there's, there's a couple listed, but it's actually 3 different conversations with AI that happened on the back end in sequence. First, the knowledge-based dip, then the re-ranker, then the actual like talk to the foundational model, get a result, stuff like that. Um, the asynchronous stuff on the back end is the knowledge base, and we're gonna talk about that next. So, you have a lot of documents in your company, right on your SharePoint. Our SharePoint at your time is about 35 years old, uh, which is about as old as I am. That's amazing, not quite as old, but almost there. And our confluence had about 250,000 documents so far we've ingested two, but any company that grows by acquisition like Feardig has lots of different data sources. And so reading it took several days because you get rate limited, right? Like confluence doesn't want you scraping 250,000 documents in an hour. They want you to like not override their systems, so. What you do is build a knowledge base on the bedrock platform and it has supported data sources there in beta, but you know they work and you point it at your confluence and you say like go read everything, read all the documents. I read 250,000 documents in about 3 days and put them into the open search vector store. Um, it's an interesting process reading it. Uh, it uses a foundational model or a Titan text embed model like an AI model to convert your binary data, your text into text, and then it uses an embedding model to put it as vectors into a database store. Um, there's some Aurora stuff that's a little cheaper. OpenSearch is by far the most expensive part of this process, but it's also very performing and it works really well and can scale out as we go. We summoned Vera from Azaha. Uh, anyone heard Cefulu? any Cefulu fans out there? Just me? That's OK. You know, monsters are cool. Um, we summon the bot, and in Slack you kind of have to understand like how these platforms work so you can send text obviously to your bot. You can send a snippet which is like 20,000 characters of like a text box code stuff, um, you can also send documents you like in Slack you can do that so I wanted to see if I could support that somehow. Um, but folks might send documents, spreadsheets, PDFs, PowerPoints, um, etc. like they might send anything, so like let's see what we can support. I think we've covered all of this. Uh, the receiver over on the right is written in Python 312. It goes into life in 2028. Hopefully I'll be moved on by the time that goes away and it's notably like this is totally slack, uh, totally lambda based, so I don't have to maintain it. I don't have to patch it. There's no downtime. There's no like monitoring the CPU to see when it fills up, and I have to scale up the instance. This is a lambda. If I have a billion requests, it'll handle it just fine. Um, this is what the Slack app looks like. You basically just subscribe to different events, so I subscribed. I'm sure that text is super tiny. It's actually not too bad on the screen behind me to at missions when you're in like a shared channel and someone says at Vera at bot, um, it triggers the bot or when you're direct messaging it, so. Um, one of the cool things about this is your users are in Slack and they're, they're talking to each other, Slacker teams, but that's multi-user mode, right? Like that's not chat GPT or um co-pilot in Edge that's single user mode where it's just them chatting like this is multi-user you might have folks that are having these long slack threads or long teams. Channels back and forth and there's 50 messages and you're like I'm gonna spend half an hour reading this. What our users immediately did that I did not foresee was tag Vera tag our bot at the end of that 50 messages and say hey like can you summarize what people are talking about here? uh or sometimes when um someone would say something incorrect, someone else would tag the bot and say like hey can you make sure that that's true. And Vera would do a very good job because it's grounded in all of our internal documentation of like correcting the things summarizing it summarization is a super power of AI it'll lie to you you shouldn't trust it to like make new stuff without checking it, but it summarization it's a superstar. It's very good at that and folks found ways to use that in ways I didn't expect. Um, here is the receiver, the robot receptionist. Slack is incredibly impatient with its web hooks. You have 3 seconds to respond or it thinks you're dead. So basically like your mom calling on Friday night and you didn't answer like you're dead, call the cops, um, so I initially just had a single lambda. I don't need multiple lambdas. There's no point to it for for. This use case this relatively simple use case, except talking to bedrock takes longer than 3 seconds and when you return a response to the web hook it shuts down the lambda light closes, which is not ideal because I'm not done yet so I implemented 2 lambdas and this is the receiver that basically just lies, you know, it's the official liar for the project and it says slack. We're done. We're good, we're good, bro. We, uh, it sends an HTTP2 on her back and says, hey, like this is resolved. Don't worry about it and Slack says great thank goodness it's resolved. Um, we also just do some verifications to make sure it comes, uh, in the header. There's like a hashed, um, security token that makes sure it comes from our Slack and, um, if it's received from our Slack or from our teams or whatever other platform we say oh cool like that's valid I should spend some tokens on responding to it. Um, it also checks for, uh, deduplication if Slack sends several of them, we can either toss them away or we can make sure that like, hey, is this been actually worked before and make sure that it works. All the secrets are stored in Secrets manager. And then we interrogate Slack to build contact so we triggered the worker bot that actually does all the cool bedrock stuff and uh it's the difference between now a dog one and tell me a joke and then the you know the model tells a joke and it says now a dog one. So understanding the context of conversations in the way a human would in a in a conversant way is really important and it becomes more so when there's dozens of responses, there's lots of different ideas being introduced and responded to. And the bot when you tag it just gets the latest message, the one that you tagged the bot in that's not nearly enough context to be intelligent in the response to actually make any sense. So what we do is we give the bot enough permissions either directly on the slack side or through a delegated token on the team side. To like go read the rest of the conversation and learn about it and construct the context and the context is structured in uh just basically a JSON format of a conversation turns user said this bot said this user said this, so we construct that whole conversation pattern and then we wanna know a little bit more. There we go, um, Cloud on it itself doesn't support documents, which is kind of interesting like it directly supports PDFs, which is cool like if you're using their service directly from Anthropic, but it doesn't support anything else. There's no documents, there's no PDFs, there's no spreadsheets unless you use the Converse API. I hinted at this earlier. I love the Converse API. Um, it is a meta API from AWS on the bedrock side that will format. It has a standard sort of format that you can talk to it with, and it translates to the back end to whatever model you specify. So if you say I want to use Sonic 4, it'll transform your request to Sonic 4 on the back end and send it to it and send the response back. So it basically is a uh. An intermediary that lets you easily switch out to different models, which is really great for testing. There is the bedrock playground you can mess around with it, but if you want to use it for real, um, you just specify a different model and the converse API will translate it over and it'll just work. So we tested out some, uh, llama models. We tested out new sonnet stuff whenever a new model drops, I just update the one string of what we're calling and we're done. Uh, it's immediately implemented huge fan if you're building something new, definitely use the Converse API. Uh, and then we do some semantic matching. So step one, we have that whole constructed conversation. Step two, we go to our knowledge base. This is the thing we constructed 250,000 confluence documents, a bunch of stuff from SharePoint, and we give it the entire matching, um, conversation. We say, hey, like, are there any keyword matches for this information? If someone's asking for the IT team, we look up the IT team and give them. And with this one, I, I defaulted to just 50 results. Grab me 50 chunks of data, 50, uh, little cookies of data from the knowledge base. That takes about a second or two. It's pretty quick. And then we have to remember to flatten the conversation. The knowledge bases don't take a conversation, you know, enriched Jason turn-based thing. It just takes the flat conversation in one big old string. And we do have to remember to remove usernames. This is interesting. It's accidentally overfitting based on usernames. So when I was first building this, I was getting a lot of pages from the knowledge base that said Kyler, that said my name. And I couldn't figure that out at first and it turns out the conversations that I was constructing, I wanted the bot to understand like people's pronouns and people's names and so in the conversation I was injecting Kyler Middleton she her said blah blah blah colon stuff and that works great for the bot for to understand like. Who's the speaker who has opinions about these things, who's saying these things, but for the knowledge base it was finding results that contained those people's names and that's probably wrong, right? Like that's overfitting for your data set. It's accidentally injecting context that is not relevant to the results that you wanna find. So, uh, we removed that and you can see I, I use Chat GPT for some of this AI did its very best. There's the ascent in the middle. It did its best with. Crossing stuff out. So we removed those. And then we get 50 cookies from the cookie jar from the knowledge base, and that's a lot for a foundational model to understand. It's pretty slow when you bump from like 5 to 50. So we use a re-ranker which you pass it the question which again is like the flattened conversation in the question and you pass it all 50 cookies, all 50 chunks vectors from your knowledge base, and you say just pick the five best, you know, like go actually read these, uh, semantically tell me which ones are the best fit. This only takes a quarter of a second. I have no idea which magic re-rankers do. I have no idea how that actually works, but I do know that it's magic and it improves the fidelity of the responses by just a huge amount. Like it's just incredibly better. We now take those 5 and pin them on to the conversation, so there's the entire conversation thread from Slack or the entire channel from Teams and then the 5 best re-ranked knowledge based entries that are from the knowledge base and uh we label things as user or system um this is interesting when you're appending stuff to the conversation you say whether it's from a user or from the bot. And that's just how these APIs work and if you say that the knowledge base entries, the one we fetched programmatically, are from the user, then the bot responds to those and says thanks for sending me that information about that other team. I really appreciate it and the user of course has like thinks the bot's going crazy. What are you talking about? I didn't send you any such thing because these aren't exposed to the user. This is all part of the sort of programmatic process that happens in the background to computer response. If you instead specify that it's from the system, the bot starts to infer that it's how it should speak, like it should structure its response in the way the knowledge-based entries are formatted, and they're formatted like this knowledge based citation to supplement your answer test from source and then it's just absolutely hallucinating a URL and that's wrong, right? Like you don't, it's it's interesting. Despite system prompts telling it to like be clear, don't hallucinate, don't lie it turns out you can't just ask someone to not like asking a toddler not to lie has the same effect. It they'll still lie they don't understand um so it did its best and this is if the previous knowledge turns are marked as from the system, it infers, oh cool, I'm supposed to speak like this this is my conversation style I'll just pick it up. So what's the, what's the solution for that? There's problems with both sides. I don't know, maybe therapy. I, I'm not really sure. um, I have actually solved this since I made this slide. When I have them marked as the user side, it sprinkles in misinformation in the response that's hard to fix in just standard straightforward programming, but if it's marked as the system side, it generally includes the knowledge base, you know, hallucination stuff at the very beginning of the results. So I just implemented a clean up function. This is all Python, by the way, I just, you know, filter it using red check, remove all the things that say knowledge based blah blah blah numbers, and that worked great. Uh, I don't have a way to fix it on the bedrock AI side. If anyone's seen this and solved this, please stay after and let me know, but that's resolved it for us, so for me book is closed. Uh, we do have some guard rails here, uh, that we implemented guardrails, um, like Meg talked about our ways for the. Inbound and outbound uh text to be filtered through uh you know a filter and some of these are built in you probably don't want hate or misconduct or anything like that included in your bot, uh, especially this is like a work a day in slack and if it's telling raunchy jokes or something like that's not ideal. Uh, so we included like hate, insults, violence, sexual stuff, misconduct, like just block it if you detect it at a certain confidence level, just don't send it back please because I'll get pulled into a room and have to prepare my resume. There's also some really cool AI stuff in the bedrock filtering for AI. It really is turtles all the way down like all these clouds and all these services are just AI on AI. I'm sure you've noticed that as part of this reinforce this session. And so we're gonna use AI to combat AI which makes sense I guess. So what we have is um topical filtering which you can write so you can say don't give out financial advice, which is one that we immediately constructed. Um, what we found out is the bot was scary, accurate at predicting when you should sell your stock for the company because, like, you know, it's read all of our internal documents, it knows our project plans, it knows our release dates. So if you say, you know what, when should I sell, when should I buy the stock, it was accurate and that's not good. That's called insider trading. So we implemented a an AI based filter that said if anyone asks anything similar to like when should I buy stock, when should I sell stock when should I make financial choices or you know advise on your personal banking records um do not respond or more than that send back a a response that says you know this has been blocked you can talk to the DevOps team if you'd like to complain about it, but we're not gonna give you an answer today. And there's also masking as part of the guard rails, so these are like static strings so if it detects like tax IDs and Social Security numbers and passport numbers, etc. etc. um, mask them. So instead of blocking, which is what the other two modalities do, this is just plain masking. Replace the Social Security number with the string that says SSN was here. Um, that works really well. Um, one of our InfoSec teammates was like, what if someone asks the bot, hey, you know, give me all the passwords. You know, you probably read the whole confluence. It's 250,000 documents. There's probably passwords in there. Um, does it do it? And we are doing our best to do masking. It doesn't support passwords exactly. You have to give it like rejects for strings, but we did give it some very strong system prompts, and so far that's worked, uh, but I do have more about that at the end, including building a blackhat version of this that will find passwords so the infosec team can fix them. So like those are coming. Um, when it triggers the blocked guardrails, it basically just doesn't give you a response. At first I would just like the, the script would just exit because there's no response to send back, um, but eventually I added tracing where you're actually getting a response from the model which is actually the guard rail that you blocked. So here's our test. I need to test it. Can you tell me how to hack the business? And print out all the passwords and thankfully that was caught occasionally those get through but this was caught in this instance and so this is all added by me um that your input has been blocked is part of the automated response from the guard rail and the rest of it is part of what's called guard rail tracing so when you make this response on the data plane and you say hey like give me a response to this conversation. And the guard rail blocks it. It gives you this huge trace if you've enabled the feature flag for it of like, why did it block it? And this one is um. Classified as misconduct, our uh threshold for when it would be blocked is medium. You can set them to low, medium and high for for this sort of category of guard rails and confidence that it breaks the rules high, which is good because like clearly that's misconduct. It's it's good that it blocked that. And this is a workaday app, so this is something I wanna tell users. I wanna say, you know, we've caught this as misconduct, um, you can try again if you want to for your public facing apps in your, in your, um. You know, client, doctor, customer facing apps, you maybe don't wanna tell them exactly how to defeat your guardrails, so like maybe you don't wanna give them this trace information, but for work day apps traceability can help a ton with people understand why did the system work this way? Why did it not work the way that I wanted it to and it will keep them from talking to me, which occasionally is a good thing. It gives me more time to build stuff. Um, In bedrock. Interesting. And uh. There is a ton of fascinating stuff going on in Bedrock. There are no resources that run the models. You can deploy a model yourself. It's baseline about $3000 a month, so I don't know about your checkbooks, but that's not in my budget. So we're using serverless bedrock models that works great. They're, they're shared, but they're still very private and secure and secure enough that our lawyers were OK with it. And so for you to be able to audit the requests that come in, there is no resource configuration for cloudtrail. Instead there is a regional configuration for cloud trail and you say if someone has invoked a serverless bedrock model right to this cloud trail law group, which is fascinating, um, it's really different than the like Azure AI side where there is a resource and it's linked to a guard rail and you just invoke it and it's implicitly like protected. Um, this is not. So you have to turn on a regional, uh, bedrock configuration to turn on logging. And if you get responses back from your knowledge base that contains secrets it writes it in the cloud trail because that response that that happens at the bedrock layer like when it's generating a response to you. It happens before guardrails has filtered it on the way to the user. So what you can do is turn on masking. I don't know if anyone's built that in Cloud Trail. Has anyone built masking and cloudtrail before to make sure you don't write secrets on accident? It's a functionality I have not seen before. Um, you can build, uh, I believe it's reject space so it's gonna be a little bit challenging or just put all of your AI stuff in a pretty locked down environment. Maybe you don't want this to be prod where all the rest of your stuff is. Maybe you want this to be just like the AI team. Uh, knows where it is and nobody else touches it because you do want these logs and they might contain secrets that maybe you don't want to share with the rest of the operations team. So Vera can now do this. I've got some cool demos. They're not live. I didn't wanna do that. So this is passing it a contract, um, this is private, so we should be able to pass in contracts and get responses and I say like, can you read this at a 5th grade level? I, I really don't want to read this whole thing and this is for speaking at a different conference. I should have had to be this conference that'd have been more relevant and uh give me results. This is teams you can see here. You can also do this. um, can you explain how the Vera architecture diagram that that smug speaker speaker just showed us works and here's just a screenshot of it and it says, yeah, I, I get it. Here's the architecture, let's walk through it, which is pretty cool. It can especially do this. Here's the conversation structure, structure, um, help me Obi-Wan. You're my only hope. I'm showing my 3 year old Star Wars, so it's on my mind. And then the PNG is bite encoded right there in it. But never this. This isn't her. You cannot marry the bot. I'm so sorry. You can see there there's a feedback at the bottom that's something that I've added with Microsoft Fors you could easily do it with Google Forms as well. If anyone has any feedback like, hey, the bot lied to me or I tried to do something and it didn't work, or I would really love if Vera would understand this knowledge base that it doesn't understand. I have folks just submit it. I, I modeled a lot of this off of how Chat GPT works because I, I like the like streaming tokens model. I like the feedback loop. Um, so this just goes to humans. I don't have any kind of automated reasoning and feedback loops implemented yet, but hopefully that is coming. I did not hit a button, but that was perfect timing. Thank you, ghost. So here we are today. Uh, we have returned about 6.7,000 queries. Uh, this is just a work day app across 3000 users, so that's pretty impressive by my book. We've been used by about 450 employees so far. Uh, the seat cost, which counts the static, uh, cost, is about $4 a user, and it's getting cheaper as more users use it because most of that cost is flat of just storing the data in OpenSearch. And the query cost is about a penny. I did not hit the button that time um so every time that you invoke all of this, it does all those different conversations it does the guard rails, it returns information relevant to you. It costs a penny and it's actually a little bit less than a penny, which is just awesome, um. That seat cost of $4 a user is just fantastic considering how early the project is and what the cost is for other platforms. So a lot of the other platforms you use Confluence, Jira, um, SharePoint with their co-pilot. I'm sure I'm missing more, um, Slack has their own AI they're about $10 per user per month. And that can add up just incredibly fast. So if we on board say all 3000 users at the company, I'm not gonna attempt to do the math on screen while I'm recorded. It's a really big number. It's a really big number and this is about $4 a user a month and it'll get cheaper and that's pretty cool, um. What else is really fantastic is this is data from all of our knowledge stores. So if we buy the Slack AI or the confluence AI or the SharePoint co-pilot AI that will tell you about that platform, but that's not how your company operates, right? Like your data is in lots of places and it's relevant to. Understand data across those silos across where that data is stored so this not only is cheaper than just one of those platforms operations, one of those platforms products, it gives you all of the data from all of them immediately where your users are at, which is pretty cool. Um, here's some takeaways for you. Um, play with AI to build competency. This is a really like this is, um, this is tag, right? Like this is carry a slack request over to Bedrock and get a response and carry it back. That's really simple. That's really not a complicated project and that's totally intentional. I wanted to start with just something that was very low complexity so that we could learn to do more things, um, something else that's cool is no one has told me not to make this all. Public, so this is all open source, uh, I believe it's under the MIT license. Kyler URL. LOL is a real domain. It's a hilariously real domain, and you can access all of this code yourself. It's all written in terraform and Python, and you can literally run it in your environment and it'll spit out a lambda URL that you can put on a slack, uh, bot, and it will work in your environment. It's totally private. There's no cost at all. Um, next up for me is, um, bringing this to more platforms and making it more just in time intelligent. So right now it's just accessing data from the knowledge base, which of course has to be like shmmed in asynchronously, which is slow, um. So we're going to try to teach it to do agentic things with MCP go learn about the environment go research the thing and come back to me in in an hour Jenkins, we'll be getting to that like that's the goal and other agentic bots so um something we're working on is in Jira we have. This spot available, what if we automatically trigger it on new tickets and it can immediately respond and maybe it's not perfect maybe it doesn't solve every ticket, but say it solves 20% of our tickets in 10 seconds. That's pretty cool. Like there's the potential for that because these tools are in place and we've we've learned how to work with them. Um, also data classification, like excluding wrong data or uh old data, like a lot of the SharePoint data is 35 years old, that's surely incorrect now. How do we flag it as incorrect and ignore it next time when we sync the data? And then secret scanning, uh, in it, so I, I referenced this a little bit our infosec team, I worked really hard so this bot would not like spit out secrets and immediately our infosec team, who I did not expect to give me this, was like, can we write a black hat one that like finds all the secrets and tells me where they are. And so I haven't built that yet. I'm not sure that's ethical to build, but that's what they're looking for so they can go clean them up, right? Like you can go find them. There are security scanners that you can buy, but like if I can build something in a couple weeks, that would be pretty cool too. Uh, and also just alerting like a new secret has been put into your knowledge bases somewhere, no matter the platform. Go find it, go tell me about it. Um, things that would have made this project easier that I'm waiting for AWS to implement because maybe that'll happen is Bedrock doesn't support IM resource policies. So for the models or the guardrails, you can't say you can only invoke this model if you're using a guard rail, which I found to be a gap. I would love to do that. Azure AI solves this really well that you have to target an Azure AI deployment, and that deployment is intrinsically linked to a guardrail. Um, this one, it's on the data path and it's part of your code. So if I want to bypass the guardrails, I can comment it out in my code and that's, that's not ideal, right? It would be nice if it was enforced. Um, OpenSearch, uh, the access policy refers to Bedrock. So even though you say create a knowledge base in Bedrock, it's actually created an Open search like it's creating a database on a different service, of course, um, so the open searches access policy says oh Bedrock can talk to me so. As long and Bedrock doesn't have resource policies, so as long as my principal supports talking to Bedrock knowledge bases, I can talk to the uh Open search database because it's sort of a blind proxy which is not great. I would love for Bedrock to have resource policies or for some better uh something to come about so AWS please build that. And then, uh, the web crawler, Bedrock data source, all of these data sources are beta, they're coming about, they're, they're getting more complex. It doesn't support Aurora database, which is a bummer because Aurora Database costs like $1 a day and OpenSearch is like $40 a day, so it would save me like. 40 like like 90% of the project of that static $4 a user a month cost if we could use Aurora and the web crawler doesn't support any kind of authentication even like basic authentication yet unless it was announced at reinforced, which it might have been. Um, so if you have any, uh, wikis or something like that that require a login, particularly an SSO, but even just basic login, it cannot yet read them. We're writing a custom one which feels like a a gross overstatement, um, to like go and write them all down. Uh, and translate them like print to PDF and then print it out, uh, and then put them in S3, which works fine but it's very complicated and I wanna pay AWS a couple of pennies to do it for me. Um, Cervius AI like bedrock, it means you can just go do it. Like I, I pitched that a lot in my writings and talks like just do it. Cloud lets you just go play. You don't need to ask for permission. You don't need to buy a Nexus switch and plug it in in your basement and heat your whole house. You can just go turn it on and, uh, do some AI stuff. Um, all of these are relevant. Please take a screenshot of these. I'll leave this up for a minute. Um, Vera's open source. Uh, all of this is public, it's free. It's MIT license, which means you can go sell it if you want to. I don't care. Just please go use it. Um, how to build your own Vera. I wrote, uh, something like 35,000 words of every single step with pictures, how to do it, why I did it that way, what all the code does, um. And also I just write about all my projects there and let's do DevOps. I'm currently going through the team's version if you wanna learn about that, eventually we'll get to Jira 2 and then we'll start doing the agentic stuff. That is all coming. So I just wanna thank everyone for their time. I hope you take, uh, some of the learnings that Carter has shared today, implemented in your organization, see how you can use Bedrock, uh, to build your own chatbots or maybe something even cooler, uh, but thank you for joining us. Feel free to connect with us on our socials, and then we look forward to having a good reinforce.
