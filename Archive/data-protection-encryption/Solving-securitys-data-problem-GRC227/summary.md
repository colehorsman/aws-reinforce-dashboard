# AWS re:Inforce 2025 - Solving security’s data problem (GRC227)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=KI8-jcmbONA)

## Video Information
- **Author:** AWS Events
- **Duration:** 18.4 minutes
- **Word Count:** 3,738 words
- **Publish Date:** 20250618

## Summary

This session addresses the fundamental challenge facing modern security organizations: the exponential growth of security data outpacing budget increases, creating an unsustainable economic model for traditional SIEM-centric approaches. The presenter, drawing from extensive experience running large-scale SIEMs, explains how data volume typically grows 30-40% annually while transaction volumes can increase 150% year-over-year, forcing security teams into impossible budget constraints. The core problem extends beyond volume to include variety (organizations often discover 300-400 data sources when they initially estimated 28) and value mismatches where critical detection data receives the same treatment as compliance logging.

The solution framework centers on data classification and strategic tiering, moving beyond the traditional "all data in one place" approach that has dominated security thinking for decades. By categorizing data into three tiers—immediate detection and incident response data (kept in SIEM), occasionally accessed data (moved to data lakes at ~20 cents/GB vs $1/GB), and rarely accessed retention data (archived in services like AWS Glacier)—organizations can achieve 5-20x cost improvements while maintaining operational effectiveness. This approach requires treating security data with the same rigor as business intelligence data, implementing comprehensive asset and identity programs, and enriching data through telemetry pipelines.

The presentation emphasizes that this transformation requires cultural change rather than just a technical project, involving cross-functional collaboration with GRC teams, application owners, and business stakeholders. Success depends on implementing data maturity models, establishing vendor-neutral formats, separating analysis from retention, and building comprehensive strategies that view security teams as business value providers rather than cost centers. The ultimate goal is redirecting 80% of engineering time from data collection maintenance to analytics and threat detection activities.

## Key Points

- **Unsustainable Data Economics**: Security data growth (30-40% annually) dramatically outpaces budget increases, with transaction volumes often growing 150% year-over-year creating impossible cost constraints
- **Hidden Data Source Explosion**: Organizations typically discover 300-400 data sources when initially estimating 28, with complexity continuing to expand as cloud adoption accelerates
- **Data Tiering Strategy**: Three-tier approach - immediate detection data (SIEM), occasional access data (data lakes at ~20 cents/GB vs $1/GB SIEM cost), archive data (AWS Glacier for 10-20x cost savings)
- **Quality Over Quantity Philosophy**: "No security problem was ever solved with less data" - focus on better data through classification, enrichment, and strategic value alignment rather than volume reduction
- **Asset and Identity Foundation**: Strong asset and identity programs essential for data quality, reducing context switching from 21 browser tabs to single-pane analysis for SOC analysts
- **Cultural Change Requirement**: Data strategy transformation cannot be a project but must be an ongoing cultural shift involving GRC teams, application owners, and business stakeholders
- **Engineering Time Reallocation**: Successful implementation allows redirecting 80% of engineering time from data collection maintenance to analytics, ML models, and threat detection activities
- **Vendor-Neutral Approach**: Use of tools like Cribl Stream for telemetry pipelines enables data sharing across business units and vendor agility (5-week vendor transitions vs. 3x cost increases)
- **Business Value Positioning**: Transform security teams from cost centers to business value providers by sharing enriched data across the organization for broader analytics use cases
- **Data Drift Detection**: Implement monitoring and reporting to detect when data formats change, working with GRC teams to enforce remediation schedules and maintain data quality standards

## Technical Details

**Data Tiering Architecture:**
- **Tier 1 (SIEM)**: Real-time detection data, immediate incident response data, daily operational data at ~$1/GB
- **Tier 2 (Data Lake)**: Weekly/monthly access data, compliance reporting, historical analysis at ~$0.20/GB (5x cost improvement)
- **Tier 3 (Archive)**: Long-term retention, regulatory compliance data using AWS Glacier for 10-20x cost savings
- Classification-based routing through telemetry pipelines for automated data placement

**Telemetry Pipeline Implementation:**
- Cribl Stream for high-quality data processing and routing
- Single event production with multiple format outputs eliminating duplicate agents
- AI-based OCSF (Open Cybersecurity Schema Framework) mapping for common schema normalization
- Data enrichment at pipeline level reducing SOC analyst context switching

**Data Quality and Enrichment:**
- Asset and identity program integration for comprehensive context
- Automated enrichment eliminating 21-browser-tab investigations
- Real-time threat intelligence and vulnerability scanner integration
- CMDB integration for asset ownership and context
- False positive reduction through enriched decision-making datasets

**Cost Optimization Strategies:**
- 15-20% of SIEM data typically unused in 90-day periods - candidates for immediate migration
- Agent consolidation reducing CPU overhead and infrastructure requirements
- Hardware reduction (50% in implementation example) through efficient data collection
- Centralized data processing eliminating duplicate feeds from single sources

**Data Maturity Model Progression:**
- **Patchwork Level**: Ad-hoc data collection, reactive approaches
- **Efficiency Level**: Standardized processes, some automation
- **Strategic Value Level**: Business integration, comprehensive analytics, proactive threat hunting
- Multi-year transformation timeline with incremental improvements

**Vendor Neutrality and Agility:**
- Vendor-neutral data formats (OCSF, JSON, etc.) for platform independence
- Cribl Replay capability for data lake to new platform migration
- 5-week vendor transition capability vs. 3x cost penalty scenarios
- Data sharing across business units for increased organizational value

**AWS Service Integration:**
- Amazon Security Lake for centralized security data storage
- Amazon S3 for vendor-neutral data retention and sharing
- AWS Glacier for long-term archive with regulatory compliance
- Integration with AWS analytics services for advanced threat detection

**Cross-Functional Integration:**
- GRC team collaboration for data quality enforcement and findings remediation
- Application team engagement for log format standardization
- Business unit integration for shared analytics value
- IT and security analytics team repositioning as business value providers

**Monitoring and Control Framework:**
- Data drift detection for format changes and quality degradation
- Usage analytics for data value assessment and optimization
- Regular data classification reviews (monthly/quarterly cycles)
- Control frameworks for sensitive data and sovereignty requirements
- Automated reporting for data quality metrics and business value demonstration

## Full Transcript

So we're here today to talk about solving security data problem. At the end of the day, no IT, no security problem was ever solved with less data. It's always the right data. But where is the right data? Where does that come from? And why is it always such a struggle? The fundamental issue is your rate of data is increasing dramatically faster than your budget. This is something I got to see up close and personal running large scale SIMs for years. My data was going up 30-40%. The worst thing I ever had was I get called into the principal's office. My CIO wants to know why I'm spending all this money, and I'm thinking to myself, our transaction volume is growing 150% year over year. You're lucky, buddy, that it's only growing my my log volume is only growing 30% year over year. But of course that's that's just the nature. You always have to conform with the business. You have to where's the money going to come from to pay for all this. Other big issue is volume variety. The, the volume of data is increasing astronomically. Cloud service providers in particular, you're getting so much more data than you ever got before. And even though it's in decent formatting, it's still, it's lots of data. And then there's also now Variety recently worked with a company who was saying, yes, we're gonna plan our SIM migration. We only have 28 data sources, and I'm thinking to myself, no, you probably have about 300. And this is so you start we're talking to them and by the end of the project about 6 months later they actually had 400 different data sources and so it just it it creates the significant amount of complexity and finally value. Not all your data is created the same. And that's the problem with these days with with a modern security data strategy you're forced to treat all your data the same. My data that drives the detections is treated just like data that is driving my compliance. And so this is where we're gonna talk about ideas and how do you separate your data? How do you match cost and value. Let's talk about how we get here. It's just it's been going on for years. We're always talking about the idea that that we have to put our data all in one place to get value. It always provides that struggle. OK, we're put our data in one place, we're gonna analyze it in one place, we're gonna get value in one place, and this is all just doesn't work anymore. So we have to talk about doing things differently and we talked about it for years, the idea of the whole. Big data error. We're gonna bring data together. We're gonna do do data lakes. It is one problem after another and this also comes in the idea that IT and security people are not necessarily data people. Managing data is a skill set and the type of discipline that's been long brought into the business intelligence of the database sphere is only recently coming into the IT and IT and security sphere, and this is where things have to change. Let's talk about the data you can't afford. I think it's a great example. There's so much signal, but you have to identify what do you really need versus what do you, what do you want versus what you actually need and this is a real struggle. You look at all and this goes back to so many data sources, things are always changing. How do you track this down? How do you know what you want? And this is where where we regularly talk to the customers classification, spend the time to classify your data to understand what's meaningful, what's not meaningful. Have you mapped all your detection data to your detections? Do you know what's in scope? because then that tells you everything that's not detection oriented is probably a little bit less important than those data sources. By the way, I didn't say this, but if someone has a question, please, you know, let, let me hear you. And also we talk about the old thing about more data and more problems as data expands it creates compounding issues over time, especially as retention rates climb. It's very tough. Teams try to say, OK, we're gonna run our security program off for 30 days. What happens if you don't detect your problem until 60 days. Where's that data gonna come from? As it grows and grows you have compounding issues, compounding with with waste, false alerts, problems, the time think about everyone who's had an IR that's taken forever because you've had to restore data. From 9060 a year ago and so much data it takes you weeks to get it back. That's always a lot of fun, especially when you're having to report to your board and to your C level executives every hour of your status and your status for a couple of solid weeks is restoring data and so that creates its own set of challenges. This is where the idea we start talking about data tiering. This is where it's critically important. Use your classifications of data to now decide what's gonna be important and consider splitting up your data. The idea of what do you know you need, so typically detections, immediate incident response, data you use every day. Data that's maybe that you're gonna use this goes to the idea that you put this into your data lake. And so when you start thinking about what, what kind of data fits into these categories, start looking at how you use your data. You run your reports and say, OK, this is the data that we use every day, every hour. This is what our reports run on. This is what our instant response runs on. This is what our so runs on versus data that you're only touching once or twice a week. Think about moving moving that data to your data lake, but the next most important question of what a lot of people forget is what. Data you don't touch at all. It's rare when I go to work with a customer we start looking at their data and find easily find 15 or 20% of the data that's in their SIM hasn't been touched in the last 90 days. You not not using that data, you have to really ask yourself why is it there. So this is consider at least moving that data into your data lake at the least if, if not also putting that data into your deep archive. AWS Glacier is a great place to store data that you don't necessarily want to use but you have to retain. So just, but it's really important understand how you use your data. So this is something we're more than happy to help you with but just and go through this exercise not just once but at least every month, if not, you know, quarterly it's just keep that up and understand it so this way you can always be as efficient as possible. And so this is something that we really pride ourselves to cribble to give yourself that ability using a telemetry pipeline based upon classifications. Now you can easily route your data where it needs to go. And just to give you an idea of why this matters, just everyone's contract is a little bit different, but it's not unusual for a SIM to be about $1 a gig. You could put that same data into a typical data lake at 20 cents a gig. So now from a cost basis standpoint, dramatically changed the game. The idea that now I could put 5x the amount of data that I put into my data lake and I could put my SIM for the same price. So now it opens up a world. World of possible options for you and even more so if you start putting start moving data into the glacier so now it's exponential 10, 20x more data for the same price and from a leadership perspective stabilizing your cost is very important. You can make a very strong business argument. That if I can flatten out my spend I can maintain what I'm doing. I don't necessarily have to spend less. The biggest, the biggest thing to think of is just control your spend, forecast your spend, and so this is a way to get more data for the same amount of money. The oldest problem in security is I got 10 terabytes of data and 5 terabytes of license. How do I work this out? And this is how you do it. That's all talk talk about data you can't see and this is also issues around quality. I frequently talk about we we frequently have conversations with with companies around less data. I like to talk about better data no security problem is ever solved with less data. So let's talk about how we can improve your data quality, give you more visibility. I love this this field because it's just, it's, it's a huge great explanation the idea especially with a high quality asset and identity program of who is this person and this is a great way to talk about. in most organizations, the identity of a person is expressed in a multitude of ways. There's not just one way, so it's important to understand that have a strong asset and identity program to do this, and it's all about data quality. This is a foundational issue. Everyone's been through it where you're having an incident response and you're having trouble responding because you don't know the who, you don't know the what, and you don't know when the where it all starts with with your your asset identity program. Let's talk about the data you can't use as well, and this is another great example data that you just doesn't have any meaning. This is where I always talk about what's the value of enrichment? How can we make this data better? Very old story that I had this is years ago when I got started in this process. I sat down with a with a sock analyst and I said, show me what you do when you open up a case. Dude opens up a case, you know, then comes in, pulls out 21 browser tabs to understand was this case worthy of doing something with of all this context shifting CMDB looking up in in in the vulnerability scanner who owns this, all these different questions and it was a good solid hour of context switching to finally make a decision. As this is a great example of the value of high quality data. So what we did was we sat down with my team. It was like, how can we get rid of the browser tabs? So this is where we use a combination of enriching data in the telemetry pipeline and also soar to give this, give it every time there's a case, a complete answer. So less browser tabs, faster decision making, better decision making because false positives are a killer, but also the context switching of trying to find out what you're doing, it leads to burnout, it drives people crazy. So it just it helps work life balance. It really helps the ability to just process information and get things done faster. And finally, a big element also is it helps on board new analysts faster. You typically analysts are shuffling in and out of so pretty quickly. You don't have time for someone to just learn everything and know it and be able to look at it. You want to be able to help help even a junior analysts make a quality decision. And this talk about also what got you to 2025 won't get you to 2035. This is the idea that we always talk about with data tiering. Familiar concepts, even though they're familiar, they're comfortable, can't can't afford them anymore and it's always a problem. You don't, you know, no one likes the idea of saying I have a SIM, I have a data lake, but I always have to ask you the question is can you afford to not have that? Can you afford to have a SIM that doesn't have all the data you need to answer your questions? This is where you try to tease apart those use cases and get your SIM focused on detections and move your other use cases into your data lake. And so that that's where we have a conversation about data modernization fundamental qualities of data. This is another issue you see all the time. Why does IT and security data not have the same fundamental importance to the business that your BI program does? Your BI program has this endless schemas and standards of hundreds of people literally dedicated to the data data of the business. And that if someone went out and changed the format and that the impact of the end of day reporting, there's gonna be hell to pay. But if someone makes a change that impacts your data and your your formats and your security, more often than not, that's treated with a shoulder stroke. So this data is just as important. It has to have the same planning, the same rigor, and the same thought behind it in order, in order to have a comprehensive set of answers of how do we secure the business, how do we, how do we how do we keep the business safe, and also how do we stay efficient. Always think about being efficient and effective. So this is always talk about the future in terms of the idea of data strategy and this is where we start talking about, you know, what does it mean to you? How do you do this? How do you clean this up? Think about your architecture. Think about the things that drive you crazy. Agents are a great example. Let's start about simplifying your data collection where you produce an event once and share it everywhere it needs to be shared, even if it's in different formats. No more duplicating agents, no more having a firewall that has 12 different feeds coming out of it in order to support different parts of your business. Think about all the CPU that's burned up trying to duplicate data across the business. Instead produce out of it once, use a high quality telemetry pipeline like Pribble stream, and then be able to share that data. Always then focus on your data quality, normalize it. This is one thing I'm really excited about is you started working with um uh AI-based OCF mapping. So now you can take all the different types of data that you have and without a whole lot of effort map that to a common schema, put that data into your data lake. And now the the format differences start to disappear. Always think about how we how are we gonna have reporting to understand once we onboard data now that we know that it's that there's an issue there we know that the data standard it hasn't changed that the technical term is drift. How do you detect that? There's always things to think about also talk about your sensitive data. How do you manage that? How do you manage data sovereignty and these are where we start building that strategy for getting more value out of your data. Always remember collecting data is a cost storing data is a cost the values and analytics, a well prepared data strategy can help you spend less time on collection, less engineering time and collection, and spend more time on analytics. This is something with my team that was just probably the most. Breathtaking thing, it's like 2018ish when we started implementing Cribble was I was able to take 80% of my engineering time who was previously dedicated to collecting data because we're maintaining this massive open source stack. We were able to thin this down, get rid of duplicate agents, get rid of about half the hardware we're using, and, and, and then automate it. And now within a year I'm shifting. 80% of that time over into now getting more value from my tools so now we're able to run ML models detections, anomaly detection baselines, things we would have never had the time to do otherwise. But because we simplified our collection, took, took all that pain out, we were able to now restore data and share data everywhere. This all comes back to the comprehensive stre comprehensive planning. And so let's also talk about a road map. This is something that's just I think it's really underestimated. I talked to leaders about this, yeah, absolutely we're gonna start a project to do this, and I try to try to set the expectation is this is not, cannot be a project. It has to be a culture change. You have to constantly be looking at your data, understanding your data, reporting on your data, put controls in place, work with your GRC team, for example, that if you find an issue with data controls, you're now feeding it back to the GRC team. To chase it down, issue findings and get a remediation schedule. Always work with the greater business to work on your data. It has to be a team sport. Your team can't be the one, can't be the one doing it. I'm a big fan of using the GRC process. Put their annoying superpowers into, into, you know, work for you. This is something I remember we get to go. I'd always have all sorts of findings on my, you know, I had to deal with and I was like, how can I use these guys to do what I need to get done? And this is because we had tried for years it's like, you know, I would tell the app team, hey, you know this, this, this app log you have, it doesn't have the right data, it doesn't do this, it doesn't do that. So we were able to work with them now I have the GRC team chasing it down. You become the coordinator, work with work with everyone else in your business to get this done. And this also comes back to it's why you need a strategy. And then look at the data maturity model. Think about where you want, where you are and where you wanna be. The most commonplace is somewhere in between patchwork and efficiency. You're trying your best. You wanna get strategic value, but this is where in order to get forward, you in order to get to the end to strategic value, gotta have a plan and also a multi-year plan. Think about you're turning a very large ship one degree at a time. At that middle model you're working together, you're building over time you're spending it you're working with data formats and so you always it's lots of small wins. I can't stress this because this kind of thing just doesn't change. Think about how many formats you have and over time you're gonna bring in new data sources. The biggest thing I see is bringing app logging now. Lots of security teams looking at app logging and so now your scope of data has gotten even bigger, so just. You have this long term view and it's important. Don't try to do it yourself recruit the rest of your business. And also separating your analysis from your attention. This is another from a strategy standpoint. Always have your attention in your own storage. Amazon Security lake is a great example. There you go. Um, uh, S3 is another great place to go. The idea is that vendor neutral formats are the key. This is. Another great great win for my team was because we could use something like ribble replay to pull data out of our data lake we're now able to share it across the business. Data that's shared across the business has more business value. You're now able to talk about we're bringing value to the business we're not just a cost and there's all sorts of interesting things you can do as you grow your profile in the business because now you're part of the business you're, you're, you're providing data. Nothing I did pissed off people in the business more when I renamed my team from the Splunk team to the IT and security analytics team and just trying to change that narrative of what we did for the business. So think about it's progress, it's process, it's people, it's not just tools, it's not just the destination so always look at your data for value. always keep looking what can we do with this this data to bring value? There's a million pieces that you can do and more than happy to share. We're gonna be back in the booth. Also, also have engaged, leave your name. We'll, we'll set up a meeting. We we'll talk to you how you can turn your IT and security data into value across the business. This is one of the fun quotes. The idea is that you know we're able to solve more problems. This is another great team. They solved, they had a a observability vendor that gave them a quote for 3 more 5 weeks before renewal because they had crippled stream in place. They were able to transition from one vendor to another in about 5 in 5 weeks, so they didn't have to pay that 3X. And that's because they had a strategic plan for how they're gonna handle their business and so it was just it was really enlightening this the idea that because they have that data strategy in place, they're able to move forward and be agile. Well thank you very much for your time. We have a couple of minutes if you have questions, but thank you and please come to the booth. We're we're right over there. Thank you.
