# AWS re:Inforce 2025 - Your DevOps stack has a blind spot: Data resilience (DAP321)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=I4T2WP3FaWA)

## Video Information
- **Author:** AWS Events
- **Duration:** 19.6 minutes
- **Word Count:** 2,742 words
- **Publish Date:** 20250621
- **Video ID:** I4T2WP3FaWA

## Summary

This session highlights a critical blind spot in DevOps practices: the lack of data resilience for SaaS tools that organizations depend on daily. While teams have built robust infrastructure resilience with cross-region replication and automated failover, they've overlooked protecting critical business data in tools like Jira, GitHub, Salesforce, and other SaaS platforms. The presenter argues that DevOps tools have become prime attack targets because they have privileged access to source code, cloud configurations, credentials, and production resources. Despite 58% of organizations experiencing SaaS data loss, 79% mistakenly believe their SaaS vendors protect their data. The session emphasizes that SaaS vendors explicitly disclaim liability for data loss in their terms of service, making data protection the customer's responsibility under the shared responsibility model. The solution involves implementing a modified 3-2-1 backup rule specifically for SaaS data and treating data resilience as an audit-ready compliance control rather than a nice-to-have feature.

## Key Points

- **Infrastructure vs. SaaS Resilience Gap**: Organizations excel at infrastructure resilience but neglect protecting critical SaaS tools that teams depend on daily for productivity
- **DevOps Tools as Attack Vectors**: DevOps tools are prime targets because they have privileged access to source code, cloud configurations, deployment processes, secrets, and production resources
- **Real-World Incidents**: Recent examples include Atlassian's 2-week outage, GitLab's 18-hour data loss incident, CrowdStrike's 8.5 million device impact, and supply chain attacks via trusted repositories
- **Perception vs. Reality**: 79% of IT teams wrongly believe SaaS vendors protect their data, while 58% have actually experienced SaaS data loss
- **Vendor Liability Disclaimer**: All major SaaS vendors disclaim liability for data loss in their terms of service, making data protection the customer's responsibility
- **Shared Responsibility Model**: Vendors handle unlikely infrastructure failures, but customers are responsible for daily operational incidents like accidental deletions and configuration errors
- **Business Impact**: Software development lifecycle stages are interdependent - data loss at any stage prevents progression to the next phase
- **Compliance Requirements**: SOC2, ISO 27001, GDPR, and HIPAA all require demonstrable data recovery capabilities as verifiable controls
- **Market Trends**: Only 15% of enterprises currently prioritize SaaS backups, but Gartner predicts this will reach 75% by 2028 as awareness grows
- **Proactive Advantage**: Organizations implementing SaaS data resilience now gain competitive advantage over those waiting until after experiencing data loss

## Technical Details

- **Modified 3-2-1 Rule for SaaS**: Maintain 3 copies of data (production + 2 backups), store 2 backups on different cloud instances, ensure 1 backup is independent of primary SaaS provider
- **Attack Surface Expansion**: 156% year-over-year increase in malicious packages in open source repositories, 33% increase in shadow downloads from untrusted sources
- **DevOps Tool Vulnerabilities**: Jenkins servers and GitHub Actions can be compromised to modify source code before production deployment
- **Credential Management Risks**: DevOps tools manage service accounts, API tokens, and secret keys that provide access to entire cloud infrastructure
- **Supply Chain Targeting**: Jargas stealer attack used malicious packages disguised as popular AI tools to exfiltrate data from developer machines
- **Business Continuity Dependencies**: Each SDLC stage (discovery, planning, build/test/deploy, operations) depends on data from previous stages stored in different SaaS tools
- **Audit Requirements**: Recovery plans must be demonstrable to auditors, not just documented - requires tested and verified backup strategies
- **Operational Resilience**: Need ability to continue business operations even when primary SaaS vendor is completely unavailable
- **Cascading Failure Prevention**: Vulnerability in one DevOps tool can cascade across interconnected systems due to shared credentials and integrations
- **Human Error Mitigation**: Processes must account for accidental deletions, failed ETL processes, configuration errors, and AI tool mistakes as common causes of data loss

## Full Transcript

Thank you. Good morning, everyone. Last week a prospective customer came to us after discovering that a particular Jira automation. Unintentionally corrupted a critical project that their internal teams relied on in order to bring value to their customers. Everyone from their product management group through to their customer support team depended on this product to do their job and do their job well. Now, in contrast, that same product infrastructure, it was fine. Their DevOps team had done a phenomenal job setting up things like cross region replication, automated failover, and yeah, they had backups for critical data stores. They probably could have survived a meteor strike hitting one of their cloud data centers, and yet they could not survive a simple disruption to a tool that their teams internally relied on. For years the industry has focused their DevOps practices on building resilience for production level infrastructure, and what that's done is it's left a massive blind spot around criticalas tooling that everyone relies on to do their jobs and do their jobs well. Think about it, your engineering team lives in tools like Jira and GitHub. Your sales team, they're not that productive without Salesforce. Your marketing team, they've got all kinds of automation set up in HubSpot, and finance is running the business right from spreadsheets. And yeah, we're all using something like Microsoft Teams or Google Workspace in order to communicate with each other. When one of these systems fails or you lose data. Your whole business could actually come to a stop. And yet we treat these sass vendors like they're bulletproof. My name is James Selski, and I have been helping businesses protect critical SAS data for well over a decade now. And I am here to tell you that your DevOps stack has a blind spot, and it's going to cost your business one of these days. By the end of this conversation, You're going to understand why DevOps tools are becoming a primary attack vector and what you can do to protect your businesses going forward. Let's start with something that happened recently. Really should have got everyone's attention. Does everyone know who Patrick Oet is? Couple nods. If you don't know, he's the CIO at JPMorgan Chase. He published an open letter to vendors. And he pointed out 3 critical things that are going to form the foundation of everything that we're talking about today. First, he said that software providers must prioritize security over rushing features to market. The Silicon Valley days of move fast and break things, that just doesn't work when you're dealing with critical enterprise data. Second, we need to modernize our security architectures and optimize our SAS integrations. Security cannot be an afterthought if you are building tools today. You really need to be baking security in at the ground level. And third, we need to work collaboratively to prevent the abuse of interconnected systems. Your DevOps tools do not operate in isolation. A vulnerability and one can easily cascade across systems, and that is a bad thing. My friends, when the CISO of one of the world's largest financial institutions publishes a letter to the industry, he's not making suggestions. He is delivering a warning. So what makes DevOps tools such an attractive target? Well, the answer is right there in the name. They power your developer operations. They administer your CICD pipelines and therefore they have access to your source code, your cloud configurations, and your deployment processes. If someone can compromise your Jenkins server or your GitHub actions, they can modify your source code before it hits your production servers. They also provision your secrets and credentials, managing your service accounts and your API tokens and your secret keys. Just think if one of these things gets compromised, it could open the door to your entire cloud infrastructure. And finally, they're often responsible for managing your production compute resources. This is where your sensitive customer data lives. It is arguably the most valuable resource and greatest liability to your company. So let me summarize, if any of these areas becomes compromised, they offer a direct path for attackers to modify code, exfiltrate or corrupt data, and just otherwise completely disrupt your operations. I think what I'm talking about today is theoretical. This threat is real and it is happening right now. Earlier this year, we saw something called the Jargas stealer attack, very fun name, but this is where a bunch of malicious packages were uploaded to the Python package index. They were disguised as popular tools for foundational AI models, and their real intent was to exfiltrate data directly from developer machines. Any F1 fans in the audience? Couple, nice. You remember the Hungarian Grand Prix last year? Remember Mercedes AMG having some trouble maybe getting their their cars to the grid? OK, Standard race fans, they maybe thought that they were dealing with an average race day incident, but the reality was, and the security folk knew that that same day CrowdStrike was having a really bad time where over 8.5 million Windows devices were being impacted by a corrupt update. OK, we're talking impacts not just to Formula One teams, but to banks, to hospitals, to airlines, and a bunch of other companies all around the world. Now what makes these examples particularly troublesome. is that they were facilitated via trusted resources. The Python package index is a trusted facility to deliver software packages and libraries. And yeah, CrowdStrike is a very, very trusted security vendor. The problem is that when these trusted sources become threat vectors, traditional security measures will struggle. Let's take a look at the Amazon, or excuse me, the Atlassian outage from 2022. This is where a routine maintenance event didn't go quite according to plan. And the impact was over 2 weeks of downtime to a swath of Atlassian customers. OK, 2 weeks. Think about that. No access to critical tooling that your team needs in order to be successful. And this is going back a little bit in time, but it demonstrates the point. Back in 2017, Gitlab suffered a similar problem when a routine maintenance event to their core database not only created an 18 hour outage, but their own recovery procedures failed to bring back data exactly as their customers expected. And what that means is in that 18 hour period customers actually lost data. These are not stories about incompetent teams. These are really world class engineering organizations, so you have to think to yourselves, if it can happen to them. It can really happen to anyone. And this problem, it's only getting bigger. Barely a day goes by without news of a new issue that's popped up. OK, it didn't make the slides, but just last week there was a startup out of India. Who didn't have a well structured off boarding program. They terminated an employee who was not performing, and they neglected to revoke access to their production systems. That employee was none too happy, and he used that elevated privilege to log into their production environment and completely wipe out their infrastructure. Reports were that about 55,000 customers were impacted. Just a few weeks ago, Coinbase found themselves dealing with a potential $400 million problem because an overseas contractor was bribed to misuse their access to internal systems. Reports there were about 70,000 customers were impacted. Shadow downloads were up 33% in 2024. If you don't know what those are, shadow downloads are when customers pull repositories from untrusted sources. And the reason why you should care about that is because Sonotype published in a report that there was a 156% year over year increase in malicious packages identified in open source repositories in 2024. So what that means is there is a high likelihood that your people are pulling in malicious packages to your software. Attackers are specifically targeting the software supply chain because that's where they know they're going to get the biggest bang for their buck. And let's be honest, our internal processes are actually facilitating this because teams are being asked to do more with less and at lower cost. And so that means they're finding shortcuts and the bad guys know this and they're trying to take advantage. OK, so by now, hopefully you appreciate the weight of what we're talking about here. So let's bring it back to you all specifically. According to Forrester, about 58% of organizations have experienced data loss in SAS applications. That's a lot. That suggests that data loss is not actually rare in these tools. In fact, most organizations have experienced the problems even if yours hasn't yet. Now here's the wonky part, 79% of IT teams mistakenly believe that SAS vendors protect their data. Now that is a massive gap between perception and reality. Think about that for a moment. Most of your peers think that their SAS data is protected. And yet more of them are experiencing SAS data loss every day. I mean, if that's not a fundamental misunderstanding of who is responsible for what. Then I do not know what is. So let me try and clear this up for you because it's not a secret, it's actually clear as day in every terms of service that you probably do not read. This is an example that I've taken from a very well known and widely used SAS vendor, and it says you understand and agree that we will not be liable to you or any third party for any loss of profits, use, or goodwill or data. Again, every major SAS vendor has some variation of this language in their terms and conditions. And so if they are not responsible for your data, then it's on you to take the necessary steps to protect your organization. And I'm not here to point fingers or assign blame. I'm really here just trying to help people understand the reality so that you can plan accordingly. That reality is rooted in that thing that we call the shared responsibility model. Hopefully you all know what it is, but just quickly, this tells us that platform providers, they take responsibility for the unlikely events. We're talking about major cloud infrastructure failures. But the things that happen every day, those tiny little oopsies, the accidental deletions in Jira, or the failed ETL process in confluence. Or configuration error to a Jira automation or even issues with agentic um tools and yeah, despite what your CEOs are telling you, AI can and does make mistakes from time to time, recovering from those types of incidents is your responsibility. You need a plan for when things go wrong because, and I invite you to say it with me, it's not a matter of if but when. This brings us to the fundamental best practices for data resilience. OK. Hopefully you've heard of the 321 rule at Rewind, we've modified it a little bit for SASS, but it's basically this. You want to have 3 copies of your data. You want the production copy, a backup, and a secondary backup. You want 2 of those backups stored on 2 different cloud instances, and here's the critical part, you want to make sure that one of those clouds is not your primary SAS provider. Think about that for a second because if the SAS vendor gets compromised, has an outage, or decides to terminate your account, which that does happen, you need access to your data via a separate channel. I'm not just talking about technical redundancy here. I'm talking about operational resilience. You need to be able to recover your business operations if a SAS vendor is completely unavailable, and we've already seen a bunch of examples of how that's happened in recent times. Let's talk about real business impact here for a second. This slide shows the typical software development life cycle. In the discovery phase, you're probably using something like confluence to capture your requirements or maybe the research for a project that's spanning a bunch of years. In the planning phase, you might be using something like Jira in order to manage your project planning. And when it comes time to build, test, and deploy, you're probably storing source code and cloud configuration in a tool like Bitbucket or Azure DevOps or maybe even GitHub. And then operations time, once the system's running, you're likely capturing things like run books again maybe in confluence and using something like Jira service management in order to actually manage and administer things. Here's the thing that's critical though that I want you to think about. If you were to lose data at any point along the way. You would have a very hard time proceeding to the very next step in the process. And the reason for that is because your software development life cycle is not a series of independent steps. Each stage along the way depends on the one that precedes it. And so the question I want you to ask yourself is, how much would it cost your business were you to lose data at any point along this life cycle. While you're noodling the model, uh, the dollars and cents there, let's talk about compliance for a second. Whether you're dealing with SOC2's availability criteria, or ISO 27,0001's recovery requirements, or maybe a privacy regulation like GDPR or HIPAA. They all have something to say about data resilience because it is at the core of business continuity. The point here is that you cannot think about resilience as a nice to have anymore. It really needs to be an audit ready, verifiable control in your risk mitigation strategy. When an auditor wants to see your data recovery plan, it is not enough to just describe it to them. You actually have to be able to demonstrate it. And trust me, when crisis strikes, you don't want to be figuring it out while the buildings burning down behind you. OK, so we know that about 3 out of 5 businesses have experienced SAS data loss, and yet according to Gartner, only 15% of enterprises are prioritizing prioritizing SA backups today. The good news is that Gartner is further predicting that that number is going to reach around 75% by 2028, and the reason for that is unfortunately more and more businesses are going to experience the problem, but also because smart folks like you are attending sessions like this and just in general awareness of the problem is growing. So I want you to ask yourselves, do you want to be part of the proactive 15%? Or do you want to be part of that 25% in 2028 who are scrambling to catch up after experiencing data loss event? I believe that organizations who are proactive, they are bound to have a competitive advantage. And those that wait while they're going to be answering tough questions. About being blind to the problem. Alright, time for takeaways. Unfortunately for you, I only have 3 of them. First, Know the risks. DevOps is a prime and growing attack surface. They have privileged access, they manage credentials and resources, and so this makes your DevOps tools and those who administer them attractive targets to the bad guys. And remember, mistakes and human error can be just as catastrophic as real targeted attacks. Second, build resilience. Don't just look at your infrastructure. Look at your entire SA stack and software supply chain. Look at where you depend on external services and where human error can cause those cascading problems and then mitigate, mitigate, mitigate. Third and finally have a recovery plan. I'm sorry to say, but hope is not a plan. Ignorance, that's not a plan either. Neither is relying on your Saas vendor. To protect your critical business data. I'm talking about a real, tested and verified backup and recovery strategy. If you can't demonstrate that you can recover your critical business data to an auditor, you do not have a plan. The bottom line is this Sas data resilience is not someone else's problem. It is yours and you cannot be blind to it anymore. I want to thank you for your attention. I hope that you all got something of value out of this, and as well as some actionable next steps. Enjoy the rest of your conference.
