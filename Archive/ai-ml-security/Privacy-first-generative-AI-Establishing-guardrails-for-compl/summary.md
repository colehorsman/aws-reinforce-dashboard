# AWS re:Inforce 2025 - Privacy-first generative AI: Establishing guardrails for compliance (COM224)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=GAjWNoxgkYY)

## Video Information
- **Author:** AWS Events
- **Duration:** 20.5 minutes
- **Word Count:** 2,816 words
- **Publish Date:** 20250618
- **Video ID:** GAjWNoxgkYY

## Summary
The presentation focused on implementing privacy-first approaches for generative AI applications on AWS, particularly addressing the growing shift from traditional graphical interfaces to conversational AI interfaces. The speaker emphasized the critical need for protecting user data and implementing proper security guardrails when building AI applications.
The session detailed AWS's commitment to responsible AI development, introducing key tools like Amazon Macie for data classification and Amazon Bedrock for implementing secure AI applications. Particular emphasis was placed on establishing proper guardrails and security measures to prevent data leaks, protect sensitive information, and ensure ethical AI usage.

## Key Points
- The shift to conversational AI interfaces requires new privacy and security considerations compared to traditional graphical interfaces
- Responsible AI development must consider multiple dimensions including fairness, explainability, privacy, security, and governance
- Data classification and protection should be implemented before training or using AI models
- AWS provides built-in guardrails through Bedrock, but additional custom guardrails should be implemented
- Organizations need to actively prevent sensitive data leakage and hallucinations in AI responses
- Regular monitoring and adjustment of AI security measures is essential

## Technical Details
- Amazon Macie
  - Automated data classification tool
  - Discovers sensitive data in S3 buckets
  - Cost-effective solution for data protection at scale
- Amazon Bedrock
  - Comprehensive foundation model implementation layer
  - Single API for model integration
  - Built-in security and privacy features
  - Supports RAG (Retrieval Augmented Generation)
  - Includes agent features for task integration
- Bedrock Guardrails
  - Content filtering capabilities
  - Threshold adjustment options
  - PII and sensitive data removal
  - Custom word/topic filtering
  - Hallucination prevention measures
  - Identity protection features

## Full Transcript

Thank you everyone for coming. I know I'm the last one. I'm, I'm between you and the expo and the happy hour, of course. So thank you for coming to my session. We will, we will talk about, um, privacy first, uh, generalative AI applications, uh. Setting up a characterized for for compliance. I have a question. How many of you are using conversational interfaces to, to talk with the AI? Can you raise your hand? All right, most of them, of course. How many of you ask some questions about like, uh, I have a contract. Can you tell me something about this contract? how affects me, right? How many of you ask uh questions like uh this is the results of my blood test. Can you do me a resume before I go to the medic? All right, some people do that, you know, but what about imagine if you wake up in the morning. Mm and you realize some your private data has been leaked by uh AI application that doesn't implement some guide rights to protect your information to the public, you know, many of you probably uh have interacted with, um, tools like Chat GPT or models that are coming from LAA or different in different platforms, but you don't know what the application is doing behind to protect your information. So that's important to start to talk about uh data protection and general applications. Um, we are in, in the middle of a change where the, the application are changing the, the way that they interact with the users. We are changing from graphical interfaces into, um, conversational interfaces. So the other ones have a lot, a lot of time of evolving to protect your information, but this is something new. 4 or 5 years ago we, we didn't talk about generative AI. It it was the conversation was was different so we need to start to talk about the ethics of the eye so the agenda today we will talk about responsibility AI, how we can protect the users' information. Which is the which one is the the Appropriate technology that we can use on AWS to to protect the information. I will give you a a a small introduction to Macy, which is a tool that we can use to classify data at scale before sending the data to train models and also we will focus more on a bedrock to implement guide rights to protect information. That we are getting from the user to start from different places or sending to models and the response from the model to um send it to the user. First of first of all, let's talk about a little bit of AI ethics, hm. Many decisions today are being used uh AI to to to classify uh information for people. An example, uh, credit scoring or setting up, uh, if you are approved or not for a loan of, what about, uh, if you can get uh how say. Um, into a class for your kids if you are approved for that some, some places are starting you to use um AI to evaluate those things because they want to be more productive on the users to say it's more easy to train a user uh model than a person so. Things like that are are happening today and we need to focus on and build an AI that can be has a um. Protect built to protect the users hm. So we have a a few um. Through sorry, and few words that we can use as a dimension is for for responsible AI fairness to consider the impact of different groups of stakeholders or people related with AI explain ability to understand, uh, and evaluate the output of the systems privacy and security to appropriate obtain and and use the the data that the users are providing. And protecting the the data from the models. Safety nets to prevent harm harmful uh systems output that can make, how to say bad decisions on the people controllability to have mechanisms to to monitor the AI and make changes to protect the people, veracity and robustness to prevent the hallucinations of the AI getting to the people, governance to incorporate the best practices of of the AI. Um, in the supply chains, uh, including providers, uh, uh, where we deploy the applications, transparency to enable us to stakeholders to understand how the models and the applications are working in order to take, uh, appropriate decisions with the information that is provided by the the AI. So this is just an introduction. I, I, I encourage you to to start to learn a little bit more about the uh responsible AI. But let's talk a little bit more about uh technology. AWS offers a a good set of uh tools to to establish responsible AI. I have a word here that was made by the former CEO of AWS uh but basically what he said is to. AI have the power to um. The power is a powerful tool that it's important to develop a user responsible way and AWS is committed to provide uh tools to to and guidance to to implement in that way. So first I, I want to mention about Macy. Macy is a tool that allows you to um automate uh data classification uh for sensitive data at scale. It's it's cost effective and allows you to discover sensitive data that is stored on a street. You can easily you can enable it easily and detect the information that you have in your account, your 3 buckets, and understand if you have PAI credit card information on your buckets and and establish some techniques to evaluate and prevent that data gets into your models if you are refining or or or training new models. And uh helps you to reduce the tri triage when time when you are trying to understand if you are sending protected information into the training process. Well, I encourage you my call to action is in this tree. Steps You're gonna start to classify data with uh Macy first go to the Macy uh um console on your AWS console. Click on get started Then enable Macy. It will create a role for you and then you can start to use it and get information about the buckets that are contained in your account and start to do your investigation. Of course there is more features, but this is a call for action to you to start to use it. It's very, very easy to to kick off with Macy. Next tool that I want to mention is Bedrock. Amazon Bedrock, probably some of you are here a lot of about the AI with uh Amazon Bedrock and AWS, and the previous session was, uh, how to, to build a uh a secure um application on on Bedrock. But basically Amazon Bedrock provides you, um, a comprehensive layer to implement models, mm. A foundational model into your applications in a single IPA that you can add customization also um you can create retrieval augmentation generation applications. It provides agents features that you can integrate multiple tasks and improve the way that your application is working. Uh, contains security and privacy safety and that you can implement your application to protect the user's information and provides management interface that you can use uh for your applications and generate applications. Well, let's talk about the Pedro Garas. That's that's the the main topic of my talk. This uh uh uh. This call is it was created with Amazon with the cloud, um, Anthropic clouds on it 3.5 thinking into the guardrails as something that is a an internal ethical guide for our AI applications ensuring that the application that we are building our safety, mm, uh, and also you don't know about but are there hm there's two things that are trying to. You can, sorry, the models are containing their own guardrails, but also Bedrocks provides you some guardrails that you can uh use to improve the security of your applications. So this slide contains information about the the different 6, The 6 steps that provide you to protect your information on the garage, it can help you to filter, uh, the threshold and adjust the thresholds, mm, and can help you to protect identity. Define and disallow deny topics to prevent to use the people to use in different the way that you intended to build the application removes uh PAI and credit card information or sensible information and you can define set of words that you can prevent to be used on your applications, uh, and of course the most important part you can filter hallucinations that are coming from the model to prevent those hallucinations get into the user as real information. So This slide explains how the the the the steps when you get suppo we we will start to suppose uh um. Um, An example application where the user gets into input as a prompting, then the prompt goes to Amazon Bedrock as an input. It will be evaluated at the LLM guardrail and will send it into the LLM to get the the the request processor and then. The answer from the LLM will go also to the guards and finally the response goes to the user at the end. So In this case we can filter uh information about the different techniques for intrusion defense, prompting post prompting random sequences to enclosure, sandwich defense, XML tagging, etc. and then, uh, um, bedrock will apply the the filters on the garage and you get the. The response from the LLM and it will be evaluated again. All right. Oh Looks like you yeah. Sorry about that. I, I didn't realize that my much like uh you have some words in Spanish, but this is how the the better I explained. I'm Spanish speaker. I'm from Argentina. That's why I'm a little bit nervous about my pronunciation, but well, this is how Bedrock uh do the uh independent evaluations. About the user we can use the same independent evaluation also for the user and from the foundational model output and then this is the different here's denial topics content filter filter of information confidential information. Context uh grounding and the and then the words that are not allowed and you then after that you get the response. So in this slide you can see an example how can be implemented uh in uh in application where you have the front end build on amplified with cloud front S3 for a static assets and Cognito for the user authentication. And the front end that's building that it will interact with the um A getaway to send the uh the to the backing application that discusses on on Landa Lambda will capture the prompting and then it will interact with the Amazon bedrock so. When you get a message it will interact in this way we'll send like uh we reviewed minutes ago. And This is the important part that you have to take attention. It's very, very easy to set up the, uh, the guardrails when you go to the uh the Amazon Bedro console. There is a place that is for guardrails and then you set the contact filters in this way. The conto filters can be you can filter he. Insults, sex sexual topics, violence, misconduct, and prompt prompting attack, hm, and you can set it to for text and some of them for images as well, hm and then for each of the, the topics you will have a strength that you can uh um change easy way as a slider so you can say how the is the level of strength that to apply to to into those topics. Same for for prompting attack you can prevent the people to start to try to apply uh prompting techniques to to to get information that is not allowed and also harmful um categories that you don't want to touch and to prevent and protect the people to um how to say. To get um private information or efforts to get information that is not related with the topic of the of the application. You can add Some deny a topics you can say I don't want my application to talk about politics. I don't want my application to talk about, I know, maybe provide medical advising it and it it since it's not a medical application so you can add your own topics and you can point point your prompting definition about the topic that you don't want to um apply in your application. Context of grounding and relevance evaluates if the the topic that you are putting in your as your input it's not it's related and of course it it has two different uh points. Um, Relevance and threshold that it will evaluate the the if the grounding of your request is is is related with the application and then it will evaluate also the response of the LLM and it it it is related with the uh meaning the grounding of the application sorry. And finally this is one of the most important. The PAI filter this will allow you to. Detect filter and masquerade um uh a block sorry uh PII information when a user put inside of the prompting or or interaction with the uh LLM, uh, um, I don't know, Social Security number license, uh, driver license or credit card information or something like that you have a lot of uh entries that are already made that you can block, detect or. Um, masquerade. Uh, in your application in an easy way there is some collections that are building and otherwise you can use, uh, sorry, um regular expressions to add uh more filters that are not building. Also you can add uh. A filter for profanity on words and also you can add a list of words that you can add in uploading a file. It can be uh a list of words that can be a a CSV file or or or just text. So you can filter topics that you wouldn't want to include in your application. Another important thing on Amazon Bedrock Gages is the how we are monitoring the car rights to make decisions about how we are building the application and and how it's performing. So AWS, um, sorry, Amazon Bedrock provides you auditing and logging that you, you can evaluate the executions, uh, it's integrated with cloth so you can get it there. It's in in JO format you can. Um, get information about what's the, the car rail that was triggered, what was the, the confidence on on that guard rail, and, uh, it, it is if was blocked or was detected only so you can make decision on how to change your application to to detect more things. Also you can build a dashboard where you can identify patterns, uh, and potential issues that are happening on your application. Have to be fast because we are running out of time but. These are the benefits of implementing um guard rights on your um GAI applications you can block up to 85% more of harm harmful content that are coming, uh, than a natural protection on our foundational models. You can filter hallucinations in your rag applications. And work with all of the LLMs that are included uh on Amazon Pedro. You don't have to implement something else, uh, helps you to meet industry standards like GDPR, like I saw, mm, and builds confidence in your application for the people, you know, the people doesn't trust so much in the GAI application sometimes because many people are, well, we know there are some stories of. Some providers that trains date with the the models with the social media data so probably if you ask some questions about you probably you will find in some of the models so you have to filter those to not get into the final users. So that's an important topic, uh, to wrap up this session I will left some some QR codes so you can scan for our links to. Um, documentation about the, the website of Bedrock guardrails, a guide to, um, implement Bedrock guardrails, and, um, on blog posts, how to create, um, a generative AI application, uh, interact with ZA and Bedrock and finally, uh, some sample code for Bedrock guards. Thank you very much for for coming here uh so you can go now to to the expo or if you want to have some questions I I will be in the side of the stage. Here's my contact information. Thank you.
