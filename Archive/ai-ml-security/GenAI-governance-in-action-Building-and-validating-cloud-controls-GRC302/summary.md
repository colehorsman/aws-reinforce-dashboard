# AWS re:Inforce 2025 - GenAI governance in action: Building and validating cloud controls (GRC302)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=RDYAAStMqpY)

## Video Information
- **Author:** AWS Events
- **Duration:** 55.8 minutes
- **Word Count:** 9,733 words
- **Publish Date:** 20250619
- **Video ID:** RDYAAStMqpY

## Summary
This session demonstrated how generative AI can accelerate security governance and compliance implementation. The speakers showed practical demos using AWS Q Developer CLI to automate Control Tower baseline creation, AWS Config rule development, and CloudTrail analysis. The session emphasized transforming security from a bottleneck into an enabler through AI-powered automation while maintaining human oversight and establishing proper data perimeter controls.

## Key Points
- **Three Lines of Defense Model**: Management (builders), Security/Risk (guardrails), Internal Audit (validation and evidence)
- **Data Perimeter Framework**: Trusted identities accessing trusted resources through trusted networks
- **Q Developer CLI Capabilities**: Code generation, AWS API access, debugging, deployment, agentic commands with approval workflows
- **Control Tower Automation**: AI-generated CloudFormation baselines for Account Factory customization
- **Config Rules Acceleration**: Automated creation of detective controls with remediation actions using Q CLI
- **CloudTrail Lake Enhancements**: Natural language queries, AI summarization, tag enrichment, network activity events
- **Security as Culture**: Embedding security practices into development workflows rather than afterthoughts
- **Human-in-the-Loop**: AI augments capabilities but requires validation and oversight for production deployments

## Technical Details
- **Data Perimeter Components**: Identity (aws:PrincipalOrgID), Resource (trusted resource boundaries), Network (VPC endpoint controls)
- **Control Tower Services**: Automated provisioning, detective/preventive/proactive controls, SCP integration
- **AWS Config Features**: Resource tracking, compliance packs (HIPAA, PCI, NIST), remediation automation
- **CloudTrail Lake Capabilities**: Event enrichment with tags and IAM conditions, network activity monitoring, cost-optimized filtering
- **Q CLI Integration**: 300+ line CloudFormation templates generated from natural language requirements
- **Resource Tagging Strategy**: Cost center, project name, owner tags for business context in audit trails
- **Network Activity Events**: VPC endpoint access monitoring for data perimeter validation
- **Compliance Automation**: Tag-based resource categorization, automated non-compliance remediation workflows

## Full Transcript

So you're in GRC 302 or hopefully you know that you're in GRC 302, where we're gonna talk about GenAI governance and action. We're gonna show you some demos and some really cool features around how you can use generative AI to build and validate your cloud controls. So in today's hyper competitive market, developers like Greg here on the screen, they're facing a really intense pressure, not really just to ship code faster, but to integrate cutting edge technologies like Gen AI into those applications. I think every customer I talked to, everyone's building a AI AI powered chatbot. Everyone's trying to figure out how do we extract business value from this thing called generative AI. So there's always been this demand to ship things faster as developers, and that's that relentless pace has gotten even worse now that we have everyone trying to figure out how do we monopolize and use GenEI within their applications. And so when your CEO is asking you why your competitor launched their AI feature last week while yours is still under development, it's often tempting to take these shortcuts as developers when you're under that pressure to ship things. The issue is that as you take the shortcuts, they're going to lead to significant security vulnerabilities down the road. Now the purpose of this slide is I grew up in the Pacific Northwest. I'm actually from Vancouver, Canada. I spent a lot of time hiking in the Rockies, spent a lot of time down in Seattle, and, and you, if you go to the Pacific Northwest, you often find these hiking trails. There are basically switch backing around the mountain, right? You go down the trail and it switches back around and around. It's a way to get down or up a mountain without basically, um, having to go up a really steep elevation really quickly. And those, those paths, those trails typically will have these signs that say, no, basically shortcutting switchbacks on this trail are prohibited. The idea here is that folks want you to stay on that trail. They don't want you to basically disturb the, the ecosystem of the forest. And so what often will seem like, well, this could be a really harmless shortcut. I can just take that shortcut and it would make things easier. It's really tempting to to kind of fall into that mindset, right? You think about AWS and security, you might want to start using wildcard IM permissions instead of least privileged just because it's simpler, it's easier, or maybe you're sharing AWS access keys amongst your team members just for convenience or you decide, you know, we're gonna push the security patches to the next sprint because we're really too busy trying to get this feature out the door. So all these little shortcuts are, they often will appear very innocent in isolation. We've all been there. I'm guilty of doing this. So either creating overly permissive security groups or using the same access credentials or across multiple environments or copying public code snippets without the proper security review, those thousands of things, it's death by 1000 cuts. And like I said, I gave you some examples on the previous slide, each of these shortcuts, they may save minutes, maybe even hours today, but they're like small cracks in the foundation, and they're going to compound over time and they'll eventually lead to really significant security incidents or even potentially a failed audit, and that could take months to remediate. So a failed security audit, it's not just a technical issue really if you think about it, it's a business crisis that basically will ripple across the organization. There's gonna be some immediate impact to this and it could be quite severe, such as a mandatory freeze on releasing any new features to production. Maybe an emergency budget allocation for remediation efforts or countless hours spent in executive meetings and giving status updates, but the longer term consequences of failing an audit are even even greater. Lost revenue from delaying your product launches, your competitors gaining market advantage during that freeze, and engineering teams being pulled off of innovation projects for remediation. So what started as a small shortcut to meet some deadlines. Can easily end up creating a multi-month setback for the entire business deliver basically costing far more time and resources than doing it correctly from the start would have required. So the path to security remediation after failed audit, it's not about implementing the heavy handed security regime, it's about understanding the underlying pressures that led to those developers feeling like they had to take those shortcuts and building a more sustainable framework that enables both security and innovation. And so success here really requires a balanced approach that addresses the immediate risk while developing longer term solutions. The typical reviation journey that we've seen will typically unfold across three critical phases. First, you have your immediate risk mitigation, those quick wins that demonstrate really measurable progress. Then the process of foundation building, creating security templates and guardrails, and we'll show some of this layer and some demos and then it's not just about the technology and the processes, it's also a cultural transformation. So recognizing and rewarding security first mindsets. So the key to a successful remediation lies in making security and enabler rather than a blocker. And this is where Generative AM will talk about this more by leveraging GDI you can automate a lot of these compliance checks. You can generate secure configurations and you can assist with your security analysis and it really help organizations transform from a potential security crisis into an opportunity to build more resilient and efficient development processes. So imagine a development environment for your developers where security and compliance are seamlessly woven into the fabric of innovation, not as barriers, but as accelerators of trusted delivery. So developers can focus on building groundbreaking features while security guard rails automatically ensure compliance every step of the way. So the result is basically a true dev ops culture where security is a competitive advantage. It's not a hindrance. Innovation and compliance coexist naturally and new projects inherit those security best practices by default. This is not an aspirational vision. sorry, this is not just an aspirational vision. It's as achievable today with the right combination of tools, automated processes, and AI enhanced security capabilities. So our agenda today, we're gonna talk a little bit about our our two personas. Greg is Sachita. Greg's pressured to innovate, and we'll talk about how Sachita comes and rescues the day later on. Sachita then will also talk about how do you implement a secure your foundation for innovation, and I will do some demos at the end leveraging GenEI technologies to help you do that. So me Greg. He's a software development manager at any company, leads a small team of engineers, and like we were saying before, he's been tasked to get a GI app to market ASAP. So lots of pressure coming from his executive team. Greg's challenges are that team's already facing a backlog of technical debt that it just seems to get bigger and bigger after each and each sprint. The teams are already resource constrained. We all know, the whole team knows security is important. We know it's important. And we plan to do a security review before we go to the production, but as that launch day gets closer and closer, the team's realizing, oh my goodness, we've underestimated the amount of work here to remediate all these security findings. And so what does the team do? Well, we're kind of under the pressure of the ship and we're gonna have to make a calculated risk. So that basic security review by your team revealed multiple configuration issues. We've ranked those and risked those, so there's some that are critical and we're gonna address those right now before we hit prod, but the things that are medium and low, we're gonna take on that risk right now because we have to ship this thing to production. And so some of the critical things we found, uh, some of these might seem like rookie mistakes, but we see customers do this still all the time route access keys being in use, using IM wildcard, like very permissive privileges, missing BPC endpoint protections or security groups are open wide open or cloud trail logging completely disabled. So these were some of the things that Greg and his development team found. And we realized we need help. We don't, we don't know what we're doing, so we actually hired Sachita as our principal security engineer to come and help us. So she's actually been tasked to help any company raise the bar on their security posture, and we told her, you're here not to say no but to say how. So I'm gonna pass this off to. There you go. Thank you, Greg. So I am the acting security engineer because clearly I'm a product manager uh for any company right now and uh I'm just coming in and I realize that as Greg mentioned that security needs to be part of your culture and that's where I wanna start in any company. I wanna start with setting up this framework, this uh culture where teams like Greg can look at security as an enabler, not something that as an afterthought. So to do that. We would be implementing a secure. Uh, foundation for innovation and the first thing that I want to bring to the company, any company, is this, uh, model. It is from the Institute of Internal Auditors and it puts together a framework of how organizations manage their risk. It talks about the stakeholders in an organization that are responsible for managing risk as well as meeting business objectives, and the three key stakeholders in this framework. The top one, which is not depicted over here is the governing body. So the governing body is basically the C-suite executives who define the vision objective of the organization as well as they define the risk appetite of the organization. So once that is done, most of the organization, at least in any company we saw we had the second, uh, stakeholder management, but we only had the first line of defense, which was the team that is building the products and services for the customers of the organization. Now in this, in the three models, what happens is in in addition to building the services, this line of defense also is responsible for making sure that these services are in compliance with frameworks and regulations and security regulations. And what helps them is the second line of defense where security engineers like myself come in and they work with the first line with the developers and the SDMs and the engineering managers like Greg to help define the guardrails, the security. Practices that should be woven into the development process at the time when they begin the process, so not as an afterthought, not something that they should be fixing after the build is done, but right as they're building it so almost fixing those little cracks that Greg talked about. So that is my role, uh, working with Greg, figuring out what are the best practices that needs to be implemented and helping him implement it into the innovation workflow. The third line is also uh it's called internal audit, and this stakeholder is responsible for two things. One, they're responsible for ensuring that the guardrails, the security best practices that are in place are working effectively. They want to have the information to confirm that, to validate that, but they also sometimes are responsible for going to external auditors and providing them evidence that these things are being uh being taken care of in this organization or if there is an audit request, these are the stakeholders. Does that put together a report on what is happening in the organization, what are the different audit logs that can be uh shared with the external auditors so we want to set up this framework within uh any company where the governing body, the C-suite, the people who are making the vision and the mission of the organization. The teams that are building the product are working together with the security and risk uh uh stakeholders to make security a foundation of their development process and again helping them with the how and not saying no by moving everything to the beginning of the development process so with that. Once we've established this culture and Greg and I have a very good working relationships and we start looking at how do we do this? We, we build the stakeholder, we build the responsibilities. Now we go into AWS and see what are the different AWS services that Can align to these three lines of defense and help me enable compliance and audit within AWS and that that you could think of more services that fit in over here, but some of the services that we would be talking about today are on these slides on this slide today. So services like control tower and config that enables and allows you to create an environment where security guard rails are baked into the day one workflows, uh, systems manager allows for remediation if there's something of security. Gives you security findings, identify things when they're out of compliance and audit manager and cloud trail helps personas like Suchita, security managers as well as compliance engineers find when something is going wrong and giving the evidence of what is happening in the organization. But before we go there, I, Greg shared about some of the shortcuts that he took, and while we go there, I do want to take a step back and work with any company and stakeholders like Greg and the C-suite and talk about some of the common things that need to happen. How do we make sure that as new software, as new services are being rolled out in production. We have a clear line, a clear boundary to make sure that our AWS resources are protected from unintended access and therefore we need a guardrail around our AWS environment to make sure that our resources are being accessed by the right uh identities. And that is done through establishing a guard rail around your AWS environment. And why is it important? Why is it important for you to set a high level co level guard rail and protect your AWS environment at the get go? It is because when I speak to security professional as a cloud trail person, one thing that I hear commonly is they are resource constraints, they are skill constraint. And they cannot be in every meeting with Greg to figure out that the permissions that they're using, the access that they're providing to the resources are locked down. So this is what guardrails help them with. It helps them to implement compliance, security, uh. Scale make sure to put some boundaries that they absolutely know is something that they cannot happen in this organization. For example, nobody should ever access my S3 buckets from external, uh, from, uh, network outside of my data parameter. They can put those boundaries. They can meet compliance at scale. They can make sure that the data that's that is sitting in the AWS resources are protected. uh, they can help customers and stakeholders like Greg to move faster, new services that they want to use, new capabilities that they want to build, they can go in knowing that the foundation is secure and they would not be opening up the organization to any kind of compliance or security risk. So that's why this is important. Now, after this, let's see what does it take to set up the secure perimeter inside AWS and that's how you do it with data parameters. You create a data perimeter. Around your AWS environment and it is really a set of permissions, our guardrails in your AWS environments that does three things. It says only the right expected user identities should be able to access your expected resources through expected networks. So 3 big data parameters that protect your data. And it starts with you defining what those uh trust boundaries are. So who are these trusted identities? Typically these are identities within your AWS account IM entities within your AWS account or within your AWS organization, as well as AWS service principles that are taking action on your behalf on your AWS resources. So anyone other than that, you can say you do not want them to be able to access your resources, not trusted versus trusted. Similarly, next resource, you want your cust your users to be only be able to access resources that you trust. If Greg is building an AI application, you want to make sure that the data that he's building the AI application on is coming from and is hosted in a trusted resource, and that's what the resource perimeter can do. And then similarly, network, what is the IP landscape of your organization? Anything coming outside of it is something that you probably do not want to get in. So you do that you define the objective of your controls and then you apply it in your AWS organization. And once you're done with that, you and you have now built that boundary around your AWS resources to stay always on and you do it in a way that any time you configure a new account, any time you push a new application out, these boundaries are there. So that 3 parameters, as I said, identity, it'll help you reason who is making the request. It'll help you ensure that your customer data or your sensitive data is always protected and never accidentally or unintentionally it is exposed to unauthorized users. It can say, you can say things like my S3 bucket should never be accessed by external entities. You could happen that could happen if you do not configure the access properly, but with boundaries that's where you can stop it. This can also ensure that your employees are not able to bring in their IM credentials into your data parameter to access your resources, so you can stop that. Uh, similarly, the research, um, data parameter would help your reason. About what is that particular user trying to access, for example, it can, as I said, it can say Greg's team cannot access data outside of my trusted resources because that is the data he needs to build his AI application on. That is the data that the model should be trained on. And then finally network, as I said, where is the request coming from? So you need to know who is taking the action identity and blocking people who are not the right, uh, identity. You need to know what are they taking the action on and whether that is a trusted resource or not, and you want to be able to know where is the resource coming from and you need to have the ability to say whenever the three things don't match, whenever there's an external IM credential coming into your data parameter, you have to have the ability to see what's going on over there. So you set this data parameter, you apply it in a way that as a new account comes in, it's already there, it's there in day 0. But you also make sure that you are using your IM conditions and IM policies to further restrict access to meet the principle of least privilege. So while data parameter is the coarse grain filter, you cannot forget about the IM policies and the fine grain principle to make sure your resources are safe. So you do that by. Setting up fine grain access controls with IM conditions which gives you, you start with the broader IM condition and then you go down and down and down and you say this should only happen if this condition is true. This should only happen if the IM entity is originating from this particular principal ID. Uh this should only happen when it's tagged a certain way, so you go down and down starting for the broader IM policy to a very restricted policy only enough to give that particular entity enough permission to do the job that it is required to do so you bring it down to the least privileged, um, uh, principle. So once that is done, so now Greg understands this, he knows that this needs to be done. It needs to be part of the development process. We will now look at some of these services that Greg and I would work together and have it in place so that when the next time Greg. is pushing something out, these services can help him build a secure foundation, and these services can help Suchita, the security professional, identify when there's variance in something, when there's something odd that is happening, and she can go in and figure out what's going on. We won't be talking about all of these services given the time. Uh, we'll focus on AWS control tower, AWS config, and AWS Cloud trail, and, uh, Greg would be doing a demo using these services to achieve the needs that we're talking about right now and making sure that the application is secure from the get-go. OK. So the first thing we'll talk about is control tower. So what is control tower? Control Tower is an AWS service that provides you a single pane of glass to manage your AWS accounts in AWS organization. It's very essential when you think about it from security and compliance standards you can define and implement consistent security and compliance standards and then make sure that every new account that is coming in your AWS uh organization meets those configurations so it minimizes the qualification of uh configuration and having multiple stakeholders managing these very, very detailed and important guard rails. It comes with automated account provisioning. You can deploy new accounts very, very quickly, and you can be sure that all of the policies are attached to those accounts. So again, security as the best practice in the get go instead of as a secondary thought. And then you can enable, uh, using cloud um control tower, you can enable service as config, AWS Config, Security hub, cloud trail to make sure you have the compliance checks. You're monitoring if something is deviating. Uh, or the resources being noncompliant and yes you have all of the information to react to so everything can be established through control tower. It exists. I'm sure you're using it, but what Greg would show you is how you can now use AI to build something like that with minimal knowledge when I see the demo, that's what I saw. It's like I don't even need to know what uh control tower is doing or what kind of policies they have, uh, but very quickly before going down, uh, control. Tower has multiple libraries that you can enable. There's detective controls. They basically implement detective controls through SCP and security hub and config. It identifies how your resources are against a particular compliance, and if there is a movement, it will alert you. Preventive control is via service control policy which makes sure that you have controls in place to protect the access to your data. And then uh proactive controls is something that you can put the mandatory controls to make sure that certain things that you know should not happen in your AWS environment would not happen so access to, as I said, external credentials is something that you can stop before it happens. So that's oh. OK. So that is control tower, helping you at a scale, put together security guard rails and then implement it across all of your accounts in your AWS organization. I mentioned that control tower enables certain services or provisions certain services that help you meet the needs. So what control tower did so far, if Greg is using it, it enabled him to build compliance and security as part of implementation, as part of initiating a development process. What control tower can also do is enable config which would tell Greg if a comp if a resource is out of compliance. AWS config is a service that will track all of your resources. It'll track the status of that resource and when the configuration changes and if it, uh, impacts a rule that. Whether it's a compliant resource or not, it will alert you. So you can go in and you can first remediate the resource that is out of compliance. Somebody opened the S3 to public, it'll tell you, and then you can go and you can change that, and then you can also come back to something like Cloudtrail to see who made that change so that you can lock down the permissions. They are not allowed to do that in case they do it. Um, conflict also comes with compliance packs, and this is really a group of rules that is aligned to, uh, frameworks such as HIPAA, PCI, um, and, uh, NEST, and you can take that compliance pack and enable it in an organization if that is what you need to be compliant to and it will automatically start looking at your resources and match, uh, to the config rules and if any resources out of compliance for that particular framework, it will alert you. So you can enable as you're building, as you're setting up your accounts, you can enable AWS config and it'll start doing that, uh, controls and making sure that your resources are always compliant. Now comes the AW independent assurance. What happens when something goes wrong? What happens when an external auditor comes to security engineer or compliance engineer and says, show me the evidence that this user did this, or show me an evidence that this user did not bring in an external IM um credential to your AWS account. That's where cloud trail comes into picture. Cultra will capture all API activity, basically answering the question who did what and when and whether the action was successful or not, and gives this information to customers to determine two things one, if a malicious action has happened, who took the action, what, how long did the person take the action, what all resources did the person touch. But even with the recent launch, you can now tell whether the person was from your data parameter or outside of data parameter. So we would look into that. It not only answer he who did what and when, it can now also answer where did the API call originate from and whether you expect it to originate from there or not. So that's what cloud trail is. Uh, in most customers, cloud trail is enabled as a trail form where customers connect all of those logs and then they push it to an estuary bucket that they own. And if Suchita has a question, then it's on her to make sure that all of that log is searchable, analyzable, so she takes the work of making sure that the data is pushed out somewhere where she can analyze it. So that does take time, that does take effort, that does require skill. In a company like any company where right now Sujata is the only person or the security team is really, really small, what helps is having something that is completely out of the box and can get me started as soon as I find an issue. So that's where Cloudreil Lake comes in. Cloudre Lakes has multiple capabilities that helps Sachita uh simplify her analysis of cloudtrail logs, uh, simplify it to the reporting that she needs to do. Uh, on who did what and where, um, what Clouder Lake is, it is a managed solution where I can enable what is it that I want to capture. I want to capture control plane API activity, access to the data, so our data plan activity, or I want to be able to see who, where the uh API call is coming from. I can do that. I can enable that capturing. I can enrich the cloud trail events. So if my organization or Greg's organization require follows a resource tagging strategy, say all of the resources related to this particular AI application need to be tagged a certain way because that is how I do my reporting for activities. I can insert those resource tags. I can enrich cloud trail events or API logs with resource tag information that helps you group events into a business context. I can also add we talked about the global condition keys in the IM policies which allows me to fine tune and really focus on. Uh, the condition that needs to be met for an API call to be authorized. I can add now add that information into Cloudtrail event. So when I need to figure out what happened, I not only have who did what and where. Uh, and where it originated from. I also have business context of my tagging strategy. I have business context of the IM global condition keys that I used to, uh, fine tune my access to resources, and then you can store and aggregate your logs in one place. You log it and forget it as as and when you need to access this data, you can come and visualize and analyze this data right with the within cloud trail lake. We talked about that. um, we would be talking about how you can use AI so you don't really need to understand the schema of the cloud trail event. What is how do we depict user identity or where do tags sit in the in the JSON that we capture uh for each, every each and every API event you can just come in and ask a question and it'll. Give you a query you can run a query and then you can get an answer on that. So it really simplifies and reduces the time it takes for someone like Suchita to first identification of an issue to get down to the root cause analysis of cloud trail logs is where the root cause analysis lives. So reduces that and I don't need to be well versed with. Equal, I don't need to be well versed with reading tables. I can get a simple summarization of the results and go back to Greg and just say that Greg, you know what, this, this person, this this resource that is tagged this way has a particular kind of API activity it should not have. So fix that instead of going through uh so lines and lines of data to get to the same thing. Um, very quickly, what's new in Cloudtrail? I, I probably will not get into all of these, uh, but two things that are relevant for the conversation today, 3 things actually. First is in the capturing I mentioned there was a recent launch to 3 months ago which we're calling network activity event, and that is specifically to answer the question around the data parameter who from outside of your data parameter is trying to access your data or resources. In your perimeter or from inside your perimeter accessing resources out of here. So this is a net new visibility that network uh VPC endpoint owners now get. So if somebody is trying to access, let's say somebody is trying to access a cloud trail trail from outside your data perimeter and they're getting access denied, well that's good, your VPCE policies are working, but you still want to know that somebody is trying to penetrate or poke holes in your data parameter. So that's something the new visibility that is available through cloud trail. Enhanced even filtering is cost related, but you have the ability to go in and say I don't want to log this type of events. So I, I really trust AWS service principles. Let's say I don't want to have events coming from that particular principle. You can, you can exclude that. You can say I do not care about a particular type of event, particular name of event. You can go in and you can have those fine uh fine grain controls. Uh, the second one is, uh, enrichment. I talked about how you can include tags and IM global condition keys into your cloud trail events. Uh, this is something that we launched like a month ago, I think, but it helps you insert business. Context into your events. So when you have an issue to identify or issue to root cause, it always comes from a context you're answering the question and now that context in certain ways can be included within the event. You don't have to manage it separately. And then uh AI powered uh summarization and natural language queries dashboards, visualizations can help you very quickly go from something went wrong to figuring out why it went wrong or who did this, uh, thing wrong. You can quickly put together reports and like share those reports with uh your stakeholders. OK, I will, we, we talked about this. This is basically the feature tag your resources and track activity for them. Um, I am global condition keys, um, resource tags can be inserted into cloud trail events and then you can use that information to categorize cloud trail logs, um, and we'll show this today in the demo and then. Uh, I talked about this, so I won't go into details, but this is network activity events, and it monitors API activity, AWS API activities that is passing through your VPC endpoints, access denied or access passed. You can get all that visibility. So if somebody is trying to poke holes in your data parameter, you can see that if you are setting up your data parameter and you want to fine tune your VPCE policies as you're doing that, you can use this information. Uh, to basically do that and we are supporting we support multiple Amazons, uh, AWS services right now. These are just some of the list, but the list is much larger if you do want to use it. It's available in our documentation. So that's about it. I would hand it over to Greg to actually show everything in. A demo. All right, perfect. Thanks. All right, this is where the G GDI comes into the talk. I know we've kind of talked about a little bit. I wanna see by a show of hands, who has used the QCLI recently? Like less than 1%, OK, awesome, I'm gonna knock your socks off, hopefully. So I'll talk a little bit what I'm going to show and then we'll do these 3 demos here. They are the first two, I think, are about 5-6 minutes, and the other one's about 2.5. So the QCLI is something we just launched a few weeks ago, I think a few weeks, a few months ago. It's in addition to what we've had for some time, which is the AWSQ in the console, as well as our IDE plug-ins into things like JetBrains, uh, Visual Studio code, that sort of thing. So the CLI is a really, really cool tool. I can't stress enough. You have to go try this to actually believe it. It's, it's amazing. So basically what the Q developer CLI is, it's a command line tool that you installed. It can help you write code. It can help you run tests, create and run bash commands. It can access AWS APIs and resources. It can help debug issues. It could deploy code and you can connect to MCP servers, so it could do a lot. It does all through command line. There's also this, it has agentic commands, so it's a true agentic experience, so it will basically utilize a wide range of development tools within your environment. It'll run commands for you in the terminal. It can read and write basically content to the file system. It'll actually conduct troubleshooting and other AWS related actions, and you can say, you know what, I'm gonna trust you on this specific set of actions and or I wanna validate every single step you're gonna take so you can basically acknowledge everything it does or you can say, you know what, I trust you, go nuts. So it's really up to you, um, what you wanna do there. So the first two demos will show how we use the QCLI to basically accelerate the um implementation of control tower and config and those controls and policies that you had talked about and then I will demo some AI capabilities within cloud trail. So some of the things you could use the CLI for in Control Tower is really the automation of basically your baselines, right? Like Control Tower has something called Account factory under the hood that's service catalog talking to cloud formation, but it's a way for you to customize an account, so not just create the account, but go ahead and customize the baseline that have that secure baseline for your organization. You could also use the CLI to help you author SEP policies, um, or converting just requirements into specific control tower configurations. So this is what the first demo will be about is how do we use QCLI and generative AI to basically accelerate the baselining of accounts. So I'm gonna switch my demo laptop. So this is a pre-recorded demo here. I'm gonna go ahead and hit play and I'll talk through this. So I've already installed the CLI and hopefully you guys can see this OK? And so I do Q chat. So now I'm in the Q chat experience, the, the queue developer CLI experience, and I'm gonna start writing some stuff about control tower. So I'm using a control tower. I would like to start using. The account factory capability basically within control tower to help basically baseline my accounts right? We want that secure baseline and that secure foundation for these accounts before we hand them over to these application development teams. So to start, I'll give as much context as you can give, the more requirements you can give to CLI, the better the outcome or the better the output. So I'm gonna explain to you to start, I would like to create an appropriate account customization baseline for a three-tier web application. And I'm actually being pretty open ended about this, so I'm saying, can you please suggest the baseline? What would you recommend? Um, not saying necessarily you should go do this, put in production right away. You, uh, there has to be a human in the loop to do some validation, but I'm keeping it pretty open ended. Does some thinking and says I'd be happy to help you. So it's basically highlighting a few things it thinks I should do based off my very, very loose requirements. I'll pause here just for a second so you can actually read this. So it's suggesting for my three tier web app baseline I should do the following things under networking. I won't read them all, but under the networking configuration I should have those security controls and some service specific configurations. Just hit play again and we'll keep going. So it's doing some more thinking. I did speed up most of the video. Sometimes it will take a little bit of time. This should, I guess, go to the next step pretty shortly here. We'll just give a moment. It's funny with GAI, we've gone back to the 90s where you, the internet, you have to wait for things to basically show up on the screen. Now we're used to the subsecond or millisecond latency and now GEI has brought second lay back. OK, so it, I'm gonna fast forward this because. OK, so it has now basically started creating the cloud information for my baseline. So if we go into here, we're we're actually gonna see some cloud trail. And I'm actually catching an error here. Like, again, this is not perfect. Cloud trails are enabled by control tower. There's no reason to basically configure cloudtrail if control towers already enabled it. So I'm checking the work, right? I'm the human in the loop. And I'm looking at this like that doesn't quite make sense if this actually got deployed through an error because cloud trails already configured. So I'm asking, are you sure you wanna do this? He's like, oh, you're absolutely right. Control tower already does this. So again, I just want to highlight this is not perfect, you know, generative AI is nondeterministic and it's not always gonna have the right answers. You need to validate the work that it's outputting, but it can be an accelerant to the work that you have to do. So I basically have asked it to remove it, so it's not gonna do anything around cloud trail because we don't have to worry about it it's already enabled. I I'm asking can you now please summarize or move the. Controls over here. Can you please summarize what you're going to do? So I get my three-tier web app, it says we're gonna create multi-count architecture or multi-AZ. Here's the VPC and the cider blocks, and we're gonna do these specific things around guard duty, uh, config S3 bucket, and some other stuff. So I respond back to this in the command line. OK, great. Let's, let's go ahead and start doing this. So I've asked her to create the cloud formation template, does some thinking and now it creates my my cloud formation template. Just wait a moment here again. Depending on what you're asking it to do, it will, sometimes it takes up to 1 minute, sometimes it's 5 seconds, sometimes it's 1 2nd. It really depends on the complexity. So it's created my YAML file. I have my cloud formation baseline. Is reiterating what is in there. saying to use this template, I actually should pause there. It said you can go ahead and deploy this yourself and here's the steps, or it can actually help me deploy that cloud formation as well. I can not only build the cloud formation, it can go deploy it. I'm gonna do it through the console. So in Control Tower uses service catalog for this baseline. I'm not sure if you can fully see the screen here, but in Service catalog, you set up a new product, so I'm gonna call this Account factory three-tier web app. We had to put an owner. Well, we had to upload the template. Put an owner and a version on this. So I'm gonna version at 1.0.0.0, and then I'll put myself as the owner of this template. So this is my baseline. That's the 300+ line of cloud formation that I didn't have to write. And now that's set up as a product and so when I go to control tower. I can go ahead and actually apply this baseline to my account, any new account or any existing account under the management of Control Tower. I'm just going here to get my account ID which I'll need. So I'm not going to create a new account in this situation. I'm going to go to one of my sandbox accounts. I'll just pause here just for a second. So this is, I have a bunch of accounts here. I'm in my sandbox account. Just as an example, I'm gonna go ahead and apply this baseline. So you can see the blueprint there's no blueprint information there, that means there's no baseline applied to it. Under account factory. I put in my account ID where I'm hosting these baseline configurations, the cloud formation templates. I hit click validate. And then you can see I have a series of products and there's my 3 tier web app product that I just created through the QCLI. And then I'm gonna go ahead and click update account. Through the magic of video editing it worked instantly and you can see that baselines being applied, right? So I have that blueprint that I've created that I didn't have to code. Applied to my account and I'm gonna go switch to the account just show you some of the resources that created. So this is a really simple example of how you can use generative AI to get started building these baselines right before you would have to write 3, 400, 500 lines of cloud formation and it's a lot of work, right, especially you done before. This will again, don't trust this thing just to do all the work for you. You need to actually validate that the stuff works, but it can accelerate what you need to do. I'll just fast forward a bit here. So now I'm gone into that account. You can basically see those A's. So this is proof that hey, that confirmation template actually got deployed properly baseline the account and created the the network architecture that I need for my three tier web app. So the next thing that we're gonna talk about, so she talked about this is config rules and conformance packs. So you have this ability to create a custom config rule. There's some lambda and usually SSM automation documents in there as well for remediation, and we're gonna do the same thing. We're gonna use the Q developer CLI to accelerate the creation of these detective controls within the platform. So we'll go back to my 2nd demo. I'll just bring that up here and we'll get started on this one. All right, so again, we're going into the Q chat new experience. So now I'm telling I would like to use AWS config to do some basically enforce some tagging. So she talked about the importance of tagging and how you can enrich cloud trail with it. So I'm telling you I'd like to use AWS config to detect if my AWS resources. I'm just saying EC2 and EBS. Uh, to provide some guidelines are properly being tagged and in my organization and any company, what I need are 3 tags are critical for the organization cost center, the project name, and then the owner. So again, I'm giving it context and it's gonna then start to talk with me and then we'll basically build the solution together. And what I would like to do is not just notify our organization that something's not compliant. I actually wanna take a remediation action. I'm gonna be really aggressive. I just want to delete the resource. I, you wouldn't normally do this in production. You probably have a more gradual, OK, fix this in the next week. If you don't, we're gonna delete it. For me, and just to keep this example simple, as soon as we see something that's not compliant, like an EBS EBS volume that's not tagged we just delete it right away. So I've given my requirements to Q CLI and it says, yep, I'm gonna help you create config rule or remediation action. I'll create all the necessary permissions. So it's basically going through there checking to make sure AW config is set up and now it's basically talking about how it's gonna create a config rule and the permissions I need to basically implement the solution that I described to the Q uh developer CLI. Again, this is an example where you can say I want to always trust this type of tool or command that the CLI is using or you can always basically validate each step. So it's going through and creating the IM policy that's needed to make this work as well as the rule, so that's the config rule that's going to be used to check for those required tags. And then it's going to create a remediation automation document. I think again. I'll keep thinking And still thinking. There we go. And then it's create some setup instructions to create the markdown basically to tell me how I can actually go and deploy this. And the type of files that's created. And then how it's gonna work and then this is actually important too. I'm gonna pause this here just for a second so you can read this. I was, I want to be really aggressive. I want you to delete the resource right away. It, it's actually telling me. I'm not quite sure about that. Um, you may want to add some notifications or having a manual approval process. So I've, I've done, I've tested this demo a few times. I've gone through several iterations, and each time it's told me, hey, that seems really aggressive. Are you sure you wanna delete everything? We're gonna still say yes, um, but it's I just want to point out it's not simply taking instructions. They'll actually reason with you and say, you know what. We're not quite sure, maybe you should consider this other alternative, so it's not just about taking your instructions and implementing them. It can actually maybe sometimes provide you insight into something you didn't realize. So I'm gonna say no, go ahead and create the script we're gonna go deploy it. So I want not only for it to create the files, I just, I don't even want to have to deploy this. So I'm gonna ask you to create a bash script and then we'll go ahead and deploy it for me. So it's doing more thinking. It's gonna go ahead and create the bash script and then it will basically do the deployment, which will only take a few seconds and then I'll go show you in config that this is actually working. All right, so now it's create the, the, the script. The script will do everything it needs to do to deploy this. I'm saying go ahead, make it executable, go ahead and hit deploy. We'll give it another moment here. So now it's created the script. It's telling you basically how to use it, and I'm saying, OK, I'm gonna just, I, I wanna call it here and it will actually monitor the progress of that script. So if it actually failed. If you execute within the context of this que chat and it fails, it'll actually look at the error and try and fix it automatically for you. So it's just going through and it's creating the script. And then during the deployment, all right. It's telling me I don't want to actually deploy this because I'm it's gonna delete things potentially and it's basically asking me to go ahead and execute the deployment. So that's what I'm doing right now. It's doing all the checks again I've sped this up a little bit, but successfully basically deployed the solution that I only started to think up basically a few minutes ago as I was interacting with this. So the script's being deployed successfully. It's just explained, just summarizing what it did, and then we'll go into the console and I'll show you. We have one more demo after this for about 2 minutes. We'll get you out of here right on time. All right, so we're gonna go ahead and go into the console now. Let's see if I can just, there we go. So I have two volumes in here. There's a these are two EBS volumes. One you can see has the cost center, the project and the owner tags. The one does not have those tags. So one of these is compliant, the other there's not. You can see the action, the remediation actions being queued up by config. It's now executed successfully if I refresh, the volume disappeared because it's automatically deleted that volume. So again, our recommendation is not necessarily to use the GDI tooling here just to do all the work and not check it. You need to be involved in this, but it's gonna really be, it'll really help you accelerate your productivity. So let's go back to the PowerPoint. And this is the last one, a short demo about 2 minutes here. This is I'm gonna basically walk through the AI powered natural language generation as well as the summarization and how you set up, set this up in the first place. So, here we go on demo 3. All right, so the first thing you do, like said, typically most people think of cloud trail and trails, you need to set what's called an event data store for lake to work. So that's what I'm gonna do here. I'm gonna go and create a new event data store. I'll just give it a name There's some pricing options I won't go and explain that that's not relevant to this this session but we'll call the demo data store. Click next And this is where Sachi was talking about different types of events. So management events are the things like, like if you create a volume or you delete an EC2 instance where data events might be something you're doing with data within like an S3 bucket and then you also have network activity events so you can specify which type of events you want in in lake. So I said the management and the data events. And then when you do select data events you can actually go and specify OK what type of data. So it's a dynamo DB is it S3, so you can actually have control over and what you filter filter down to. This is the important part with that Sachita was talking about is the event enrichment. So I can actually go in here and enrich my events with the tags that are really important to my organization, the project cost center and owner. So that's what I'm doing here and then I again I go and create my event data store. And these are the two AI powered capabilities I'll show you in just a second here once we start querying or ran queries against against Lake. So the first query I'm gonna go against like is show me all the EBS volumes. Sorry, Amazon EC2 EBS volumes. Created today really simple query because we just create some in the previous demo. And I don't have to know SQL. I could be an auditor that knows nothing about SQL and it's taking my natural language expression and basically turning that into a SQL statement that works basically against the cloud trail data. So you can see here, I'll pause it here a second. And you can see if I just scroll back there, it's basically generated all that signal just from a really simple question I've asked it so you don't have to learn the syntax to actually be able to query and get insight into this data. And then the other thing that she talked about that is also AI powered is the ability to do summarization. So once we run this query, we have two volumes there again from the previous demo. We can click, this is in preview right now, but we can click the summarize results section down there. And what that's gonna do, it's gonna look at those two cloud trail events and sum summarize basically in plain language what were what do those results mean? And so you can see basically it's saying, OK, today 2 EBC 2. Um, EBS volumes were created. They both had a size of 100 gigs. They're not encrypted, etc. etc. etc. So imagine giving us like 100 lines of cloud trail events and having it summarized, OK, what's happened here. So between the ability to use AI to do summarization as well as natural language query generation, it makes it incredibly simple to get insight into your cloud trail download. And then if you want to extend this further, I'll go back up to the query and we could if we wanted to we could also say, well, I want some tag information basically in that that initial result set so you can then refine that query you can ask for more things so not only show me all the EBS volumes create today, but please include the following tag key values in in my query. As I type that, it will generate the query again. And the end result is basically an updated query if we uh. You can there online too it's not trivial necessarily sometimes to extract this information, uh, the tagging information here, but with the query generation makes it very, very simple. All right, let's wrap this up. That was the last demo. So just basically in summary, our advice here is start small, think big, scale really fast, Transform security and compliance, and since you had talked a lot about this, from being a bottleneck to an enabler, use these tools to make, um, security and compliance just part of the development process. Leverage the developer CLI to accelerate securing your foundation. It's you saw on the screen it's incredibly simple to get started. Remember it's not gonna do your job for you. You still need to be the human in the loop and actually validate this, but it's gonna make you, it'll become, you'll be it'll be basically like a superpower for you. You're gonna be more efficient. You'll be able to get more done with that. Again, G GUI is augmenting your team's capabilities and basically empowers them to work smarter and not harder. So I think with that about 3 minutes left, we'll wrap this up. Thank you for attending the session this afternoon. Uh, please do the survey. We could take questions up front. Yeah.
