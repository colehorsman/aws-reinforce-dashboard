# AWS re:Inforce 2025 - Designing trust and safety into your Amazon Bedrock applications (SEC125)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=PQUtFuAq2KY)

## Video Information
- **Author:** AWS Events
- **Duration:** 16.3 minutes
- **Word Count:** 3,131 words
- **Publish Date:** 20250619

## Summary
This session from AWS re:Inforce 2025 is presented by Ray from the AWS Trust and Safety team, focusing on designing trust and safety features into Amazon Bedrock applications. The presentation explores how trust and safety fits within the broader context of responsible AI, particularly emphasizing safety and controllability while acknowledging all eight dimensions of responsible AI.

The session addresses key technical concepts and business challenges related to generative AI applications, including the importance of establishing clear usage policies, implementing content filtering, and managing abuse detection. It highlights the automated abuse detection process for Bedrock and emphasizes the critical balance between technological solutions and human oversight in maintaining application safety.

The presentation concludes with practical recommendations and tools for implementing trust and safety measures, including the use of Bedrock guardrails, CloudWatch logging, and proper response protocols for abuse notifications. The speaker emphasizes that prevention is better than reaction and provides specific guidance on monitoring, logging, and responding to potential abuse cases.

## Key Points
- 82% of organizations believe a robust responsible AI strategy improves employee trust
- AWS Trust and Safety team uses a fully automated detection system for Bedrock
- Organizations must acknowledge abuse notifications within 48 hours
- Prevention is better than reaction when it comes to trust and safety
- Clear usage policies and remediation steps should be established early
- Both technological and human review mechanisms are important for safety
- Regular security reviews and monitoring are essential
- Logging and observability are crucial as AWS doesn't have access to customer inputs/outputs
- Bedrock guardrails can be used to block undesirable content and topics
- The abuse landscape for generative AI is rapidly evolving

## Technical Details
- Bedrock guardrails features:
  - Content filtering capabilities
  - Customizable thresholds
  - Word filters for specific terms/phrases
  - Topic blocking functionality
- CloudWatch implementation options:
  - Track user interactions
  - Monitor blocked content attempts
  - Create dashboards for abuse patterns
  - Set alerts and thresholds
- Abuse detection process:
  - Automated system checking against AWS AUP
  - Compliance with AWS Responsible AI Policy
  - Third-party model provider policy compliance
  - Prompt ID tracking for investigation
- Security contact configuration requirements
- Integration with logging systems for tracking inputs/outputs
- Implementation of input sanitization against prompt injections
- LLM observability tools integration options
- Dashboard creation for pattern visualization and monitoring

## Full Transcript

Hey everyone. Am I good? Can you all hear me? Awesome, cool. um, so I'm Ray. I'm with the AWS Trust and Safety team. Uh, we handle reports of abuse of, uh, potential customers that are are customers that are using AWS services in ways which abuse happens. Um, abuse is generally defined as the, uh, a violation of the acceptable use policy or any other policies that AWS customers adhere to when they use services. So today I'll be talking a little bit about trust and safety as it relates to bedrock. So I'll cover how trust and safety fits within the broader context of responsible AI. Then we'll talk a little bit about the AWS trusts and safety abuse process, and then I'll end with some practical things that you can do in terms of tools and techniques to think about and build trust and safety into your applications. So as I'm sure you're all aware, generative AI brings promising new innovation. However, at the same time there are additional challenges and risks that are raised by generative AI. Part of this new and novel technology is the excitement, but building these applications, you have to be keenly aware of the risks and consider that as you're building, as you're deploying, and operating the application. So when I talk about responsible AI, there are 8 dimensions of responsible AI according to AWS. Today I'm going to spend time speaking about safety and controllability. Safety is essentially designing systems with robust guard rails, testing and monitoring to prevent, detect, and mitigate potential harm, whereas controllability is having the mechanisms and the monitoring in place. So by having the controllability mechanism mechanisms, you're able to ensure safety. All of that being said, there are 6 other categories that are here on the slide fairness, explainability, privacy and security, veracity and robustness, governance and transparency, and all 8 are equally important. So it's important to you as you think about building your applications, how to weave all 8 of these into your process and your life cycle. So while I'll spend most of the time talking about safety and controllability today, it's really important to consider all of them. So you might be asking why is responsible AI important. There was a study that was done by Accenture and AWS, and you can scan the QR code at the bottom if you'd like to read the full study that found that 82% of organizations believe that by communicating a mature and robust responsible AI strategy, they're able to improve employee trust. So there's an internal. Benefit of doing this and then externally customers that invest in responsible AI expect to see a 25% increase in customer loyalty and satisfaction. So all of that is to say that there's both internal and external benefits of considering all of the categories around responsible AI. So now let's look at how you can practically implement some of this. So within the development life cycle there's the design and development, there's deployment and operations. You really should be thinking about how safety fits into all of these. So I'll touch on each of them a little bit. The first is thinking about what you want your generative AI application to do, but at the same time thinking about what you don't want it to do. And it's with that in mind that you're able to develop and design a cohesive trust and safety posture. The second is understanding the limitations, what you want your customers or your end users or your employees to be able to do and what you want them to limit them from doing. And then the third is explainability. As you build your application, there might come times in which you need to explain to your customers or your end users why certain things aren't allowed. So thinking about that in the context of building is equally important when it comes to deploying. Consider education and communication. So in the first stage you looked at what you want your application to do and what you don't want it to do. Now you need to think about how do you communicate this to your users or to your employees, particularly if you're building an application for external or widespread usage, you might have something like an acceptable use policy, community guidelines, or some other sort of policies that outlines to users. What they should and should not do. Also think about confidence levels as you think about how you want the uh generative AI application to respond, what is the level of confidence that you want to provide responses within and then also consider human review. You might have a system in which you have humans that review certain things or you might have technology that reviews certain things. So I'll I'll talk a little bit later on about this kind of intermix between technology and human. And also consider testing, right? So this could be red team testing before you launch your application. What can a bad actor do? What can a nefarious actor do? What are things that might just happen that you might want to design safety mechanisms for? And then operationalization. Consider feedback mechanisms. So you might have whether it's a flow for your customers to leave feedback, you might have signals, whether it's data signals that you collect to say, hey, this looks anomalous and I might need to dive into this a little bit deeper. And then it's really. important because the genitive AI landscape is fast evolving and ever changing to remain in the know about what are the new attack vectors, risks, opportunities and challenges that are facing so that you can then redesign those into your system. So as I mentioned, there are new risks that are always emerging and evolving, and you might do everything that's on this slide, however, abuse could still happen. So in the event that abuse does happen, I'll talk a little bit about how the trust and safety team gets involved and what we expect from you in return. So at the at the sort of core level, models that are available on bedrock have built in protections as well as things that they're not allowed to do, and that's at the model layer. Outside of that, you have a lot of flexibility in terms of how you want your application to function. So the way the AWS abuse detection process for Bedrock works is that it's fully automated. What this means is that there's no human review of the input that your customers or your users are putting into Bedrock and the models, and there's no review of the outputs as well. So AWS does not see what your customers are inputting or what they're outputting, and this will become really important, and I'll touch on this a little bit later. The next is that the automated detection system is looking for potential violations of the AWS acceptable use policy, the AWS responsible AI policy, which governs all AIML products, as well as violations of third party model providers acceptable use policy. So if you use bedrock and you use a model provider that's a third party provider, you might have to comply with their AUP as well as the AWSAUP. And the RAIP. So that's how the abuse system is looking is for violations of these various different policies. If the abuse is detected and it's recurring, the AWS Trust and safety team will send you a notification letting you know that abuse has happened. I'll touch a little bit in the next slide on that abuse notification process. And then these notifications are essentially formal notices to you that abuse has occurred, and we do expect you to do something with it. So what do we expect from you? So when the abuse notification comes to you, and these notifications are sent to your security contact that's on your AWS account, so it's really important to ensure that contact is updated and that email address is monitored. So they come into that security contact and we expect that you acknowledge. Set within 48 hours. That doesn't necessarily mean you have to solve the problem or you have to conclude your investigation in 48 hours. You need to let us know that you're working on it and maybe provide an update on the timing it will take for you to be able to provide us insight into what's happened. And then behind the scenes you should start investigating, maybe examining your logs, reviewing bedrock inputs, checking for unauthorized access, whatever investigation looks like internally to you, you should do that on those prompt IDs that we send. One of the important things I want to mention here is, as I. There's no human review of model inputs or outputs on the AWS side. So when you receive an abuse notification, you receive a series of prompt IDs. Those IDs become very important for you to then go in your system, look at logs, and be able to investigate and see what's going on. The next is to take the appropriate action, and that could be whatever action you determine to be right for your application and for your customer. Earlier on, I mentioned it's important to have a policy for what you allow, what you don't allow, but also it's important to have remediation mechanisms for when something happens that you don't allow. What do you do for it? So in this situation you would receive the report, you would investigate it, and then you would take the action on whether it's a specific user, whether it's updating your guardrails, or maybe it's considering redesigning a feature or tweaking a feature that might have been problematic that have led to the abuse. The next is to report back to trust and safety. So you've already acknowledged receipt of the abuse report. The next is to let us know what you did, what's, uh, what you did in this specific instance and what you're doing potentially to prevent this from happening in the future. And then the last one is to implement preventative measures. A lot of what I've talked about today is responding to a singular abuse report. You might also want to zoom out a little bit and think about what is the root cause of that abuse. Is it a specific feature that's being used in a way you didn't anticipate? And then maybe you go back to the drawing board and think about how you can embed safety into that feature. So now that I've talked about the process, I'll give you a couple of things that you can do as takeaways on how to implement some of these tools and techniques as you're building your applications. At the core, prevention is better. Thinking about trust and safety early on allows you to think about how you can prevent abuse from happening. I mentioned a couple of times already establishing clear usage policies for your AI systems, communicating that to your users, and then also having a clear understanding of what happens when the policy is violated, what is your remediation step internally for your customer, your end user, your employee. The next one is implement content filtering technology or human review. I'll touch on in a in a little bit in another slide bedrock guardrails. But think about both technology and humans as being part of your process. Certain applications, maybe it's an application that is open widely to the world. Maybe it's a chatbot. You might have to have a safety guard rail that's both technology as well as human focus. So it's really important that you consider what's right for your business and what makes. for your unique use case. The next is to set up automated monitoring for unusual patterns or outputs. The abuse notices we'll send you are for specific times that your bedrock application was used for abuse. However, you might zoom out and see this as part of a larger trend. So don't just look at the abuse report in the context of that report alone. Think about how it fits into the larger context. Then guard against prompt injections with input sanitization. As I mentioned, the abuse landscape for generative AI is fast evolving. New risks are being created. So look at ways prompt injections is one in which bad actors might try to get under the radar of the safety systems you have and how do you prevent that from happening. The next is conducting regular security reviews, and I think this is a very obvious one, but part of One of the ways in which abuse could happen is your system is being exploited. Maybe a bad actor is using it in a way that you didn't intend for it to be used. So security and safety go hand in hand. Security vulnerabilities can also lead to potential safety issues. And the last one is using LLM observatability tools if these are useful and relevant to your business and your application. So I spoke a little bit about logging, um, as I mentioned earlier, and I want to underscore this point, we don't have access to what your customers are submitting or what the prompt outputs are. So it's really important that you consider logging this on your own end. Cloud AWS Cloudwatch allows you to do this. You can track user interactions with your Bedrock application. One of the other things you could. Also track are potentially um things that are blocked from bedrock guardrails in your cloudwatch application, and then you could use that to write rules to build dashboards and to find anomalies as well. I just mentioned you can create dashboards, you can visualize abuse patterns, and this allows you to look beyond just the specific singular abuse incident or abuse report that's happening. And then setting alerts and thresholds to find suspicious things before the AWS Trust and safety team needs to let you know about it. So setting up logging can help you capture blocked content attempts. As I mentioned earlier, with bedrock guardrails, you can utilize guardrails, and I'll touch on this in the next slide to really understand what's going on to block it. And then Cloudwatch can help you use those signals in a way to understand what is happening at a more macro level. You can see usage spikes as well as pattern anomalies. So I've mentioned guard rails a couple of times, and there was a great presentation earlier today on guardrails and maybe some of you caught that. Um, but guardrails is a product that allows you to plug it in directly to your Beroc application and look for things that are potentially not aligned with the app that you're trying to create or not aligned with your the the business model that you have. So for example, you can set to block undesirable topics. So for if you're building an application that is a healthcare. And you don't want your customers to be asking about financial advice, you can set that as an undesirable topic, and gardels will help you block it. You can also set up content filters. Content filters is very important in the abuse in the abuse space, so they're um customizable, it's a set of thresholds that you're able to set. You can help block things from hate speech to sexual content to um misconduct, insults, things like that, and this is all configurable. You can set how granular you want this to be. The next is word filters. So these could be specific terms or phrases, whether it's offensive terms, maybe it's the name of your competitor that you don't want the the um the app that you're building to interact with or to output. Um, the next one is personally identifiable information or sensitive information filters. If your customer or your employee is trying to enter some of this information into your Bedrock application, Guardros can help pick up on that, and guards can help. Stop it and the way guardrails works is you can either choose to not allow the model to output something, or you can have it output a custom response that's more aligned with what you want it to say, such as, you know, this violates the policies of using this product or this service or this app. And the last one is around hallucinations, so it's grounding checks. You can have guard rails, bedrock guard rails ground the answer in something to prevent issues of hallucination. So key takeaways for today, 4 important pieces. The first is start with the core dimensions of responsible AI. There are 8 of them. The one that I spent most of my time talking about today is safety, but really consider how those 8 fit into your business and what you're building. The second is really ingrained safety and responsible AI. Into the entire development cycle from the before you start writing code, from when you're starting to think about what do I want to build, what do I want my customers to do, and what do I want them not to be able to do and think about building it through that life cycle and also consider that that cycle is iterative. You might get to the end and detect some new emerging trend that you then go back and say, do we need to. Think this feature, do we need to put additional guardrails in place to stop the abuse that's happening? The third is, if you do get an abuse notification from the AWS Trust and safety team, respond to it within 48 hours, conduct your investigation, deep dive into the issue, handle it with your user, and then think about preventative measures to prevent this from happening in the future. And then the last is consider logging as it allows you if you get an abuse notification to go and deep dive and say what exactly was happening, what was this prompt that was being asked, what was the output that they were potentially trying to get, and then also consider guardrails as a way to stop potentially bad prompts or bad outputs before they even become an issue. So with that, thank you so much for attending. Please fill out the survey in the app. If you have any questions on this or you'd like to talk a little bit more about Brad Rock guardrails, we're at the Global Services uh desk all the way down there, please stop by and chat with us. Thank you so much.
