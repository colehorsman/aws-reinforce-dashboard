# AWS re:Inforce 2025 - Supercharge IaC security with AI: From commit to auto-remediation (APS441)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=ae4E4i7-7Ns)

## Video Information
- **Author:** AWS Events
- **Duration:** 51.7 minutes
- **Word Count:** 9,102 words
- **Publish Date:** 20250620
- **Video ID:** ae4E4i7-7Ns

## Summary
Charles Roberts and Pradeep Kumar demonstrate how to transform infrastructure-as-code security reviews from manual, inconsistent processes into automated, AI-powered workflows using AWS Bedrock agents and the Strands SDK. The session addresses the critical challenge of scaling security code reviews across enterprise organizations where traditional approaches suffer from tool inconsistency, human fatigue, and slow feedback loops. Through live demonstrations, they show how to build agentic architectures that automatically scan IaC with tools like Checkov, analyze findings with Claude, provide remediation suggestions, and create GitHub issues with fingerprinted commit tracking for rapid developer feedback.

## Key Points
- Traditional security code reviews don't scale due to inconsistent developer tools, manual processes, and security team bottlenecks that create weeks-long detection-to-remediation cycles
- Security champions model provides better scaling than centralized teams but suffers from human inconsistency in applying policies and varying levels of expertise across guardians
- Agentic AI architecture using supervisor agents can coordinate multiple specialized tools and agents to provide consistent, automated security analysis across all code commits
- Strands SDK simplifies building Bedrock agents with Python decorators, enabling security consultants without deep development backgrounds to create sophisticated AI workflows
- Deterministic tools like Checkov provide confidence in AI analysis by using known security scanning results that can be independently verified rather than relying solely on LLM outputs
- Git metadata fingerprinting enables automatic tracking of code commits back to specific developers, enabling targeted remediation requests instead of orphaned security findings
- Knowledge base integration allows customization of security policies and organizational exceptions, preventing generic AI recommendations that don't align with company standards
- Automated GitHub issue creation with detailed markdown reports, remediation suggestions, and proper tagging shortens feedback loops from weeks to minutes

## Technical Details
- Architecture deployed on AWS Fargate with ECS using simple FastAPI service exposing HTTP endpoints for triggering supervisor agent workflows
- Strands SDK integration with Bedrock using Claude Sonnet 4 for code analysis, with boto3 session configuration for multi-region support and model selection
- Python decorator pattern (@tool) for wrapping security tools like Checkov, making them discoverable by Bedrock agents without complex schema definitions
- Supervisor agent coordinates multiple specialized agents: security review agent, issue analyzer, and Git integration agent for complete workflow orchestration
- Built-in Strands tools include file_read, file_write, and http_request, while custom tools use subprocess calls to execute command-line security scanners
- Agent-as-tool pattern enables recursive agent chaining where agents themselves become tools for other agents, creating flexible hierarchical workflows
- Automated remediation suggestions with Python environment testing to validate fixes before proposing changes, ensuring suggested code actually resolves security issues
- GitHub API integration for automatic issue creation with commit fingerprinting, developer tagging, and security labels for targeted remediation assignment
- Retrieval Augmented Generation (RAG) with organizational security policies stored in knowledge bases to customize recommendations based on company-specific compliance requirements

## Full Transcript

OK, so first of all, thank you very much for coming to uh to this talk this afternoon, this code talk. Um, who here has had to do some kind of code review, security or otherwise? Yeah, good few hands. Who, who enjoys it like, who really enjoys that part of their job? You do, but he's over there, everyone, go hire that man, OK? Yeah. So, um, My name is Charles Roberts. I'm a senior security consultant with AWS Professional Services. It's part of our consultancy, um, paid for consultancy, and I'm Pradeep Kumar. I'm sent him in Charles. Uh, we're based in London. I'm infrastructure architect, so I literally have very tiny bit to explain here. The main magic will be from Charles, which means that when the code goes wrong, it's, it's all my fault. OK, so first of all, what are we, what problem are we trying to solve and why are you all sat here? Um, we're gonna run through some scaling code reviews, the benefits and features of some of the different methods that you can use. That's going to be more of an organizational thing rather than a technical thing, but we'll move through it quickly and then we're gonna try and go into the bit that you're all here for, uh, which is, uh, the demo itself. So the problems we're trying to solve is that, um, as consultants, we deliver codes to, to you, our customers every single day, and we have exactly the same challenge as you do and so do our service teams. That is like, what is the quality of the code like that we are giving to our customers? What is the quality of the code from a security perspective, uh, and other perspectives, um. And it's hard, and I tell you what, like, there's Amazon scale and if you think we have magic tricks to do this stuff, we don't, OK? We have the same problems that you have. We have people doing this day in, day out, and, and it's just hard. So I'm not gonna suggest a perfect solution today. I'm gonna suggest some ideas and then you're gonna go away and take them away and build on them and do it better than me, probably. OK, so enterprise-wide, um, code reviews are, are challenging. Why are they challenging? So. Developers have tools, but they're inconsistent. Um, a lot of these tools are open source tools, nothing wrong with them, they're very good, really healthy maintained uh community behind them. But are all of the developers using the same version of that tool? Do they all download the same version? Do they even use the same tool? I can tell you now we are given a range of options like use this tool or use that tool. Um, so consistency is hard for developers. How developers apply that consistency is also hard. Uh, do they do, um, commit hooks? Do they do, uh, just scanning on demand? Do they do it when they remember it? So that's hard as well. So it's very inconsistent for developers. The other thing is, is developers like to work in IDEs. Who here has been a developer that has had to come out of their IDE to then fill in a ticket, fill out a spreadsheet, write a document, I'm seeing lots of nodding heads, yeah? Who enjoys that like no one enjoys that stuff, OK? You, you take, you get out of your environment, it, it disrupts your flow, you then fill out some spreadsheet, you then send it to some anonymous ticketing machine, and the next thing you get back is a telling off, right? That's how that goes, right? The next thing you get back is someone like me who says, oh, by the way, that S3 bucket probably shouldn't be public. By the way, I always go for that. I know we don't do that anymore, but I always go for it. Um, I, I joined Amazon when that that stuff still happened. OK, so how do we solve that problem? We invented DevOps and we invented CICD, right? We created an entire industry around DevOps and CICD. We built tools, we built pipelines, we constructed specialists, devops consultants, devops people who now do nothing but build pipelines, OK? Um. And how often do they get it right? Well, sometimes better than than nothing, but who here reckons that they could bypass their CICD pipeline if they really wanted to? Yeah. So there are ways around, aren't there, like, you know, Friday afternoon, something needs fixing quick. Don't push the prod on Friday, just fix it in life. It's so much easier, just bypass the pipeline. So there are challenges around DevOps and CICD as well. Um, security teams, security teams are good, but they are human. They do struggle with that and like you can raise questions about the debates of where we're going with Gen AI and is it fallible and infallible. OK, we can go there if you want to go there, but, but security teams like me are certainly tired on Friday afternoons, and the last thing we want to do is look at your 1000 lines of code. Um, so yeah, we struggle as well. We struggle with the same, uh, uh, tools as well. Um, visibility. So the other thing is, is that when pipeline tools run, you tend to get visibility in that pipeline. You then tend to get visibility maybe in the spreadsheet that you filled out that gets attached to the ticket. But who across the organization is looking at that and saying, do you know what? 9 out of 10 of our developers are all making the same mistakes. Maybe that's something we could improve. What's the feedback loop? Basically, there isn't. So these are all challenges that I've seen, and we work really hard at trying to solve those challenges, but I feel that they are challenges. And uh I was giving some thought to this and came up with some ideas. OK, scaling code reviews is is hard. Um there's a few models out there, again, like, tell me if you know another one. You can have the central security team. These are clearly cylinders of excellence because they're security people, right? They know everything. They know everything about everything, and by the way, security people are not created, we are born that way, like. Literally at birth, your mother looks at you and goes, well, it's a little security person. Um, so basically, that's not true. Security people come from a wide range of backgrounds and then like, normally the way it works is that something goes wrong, it's security related. You fix that and then someone says, you're good at this, guess what? You're a security person. And then you spend the rest of your career trying to get out of it. Um. And if you, if you get particularly sort of morose about it, you get promoted and you become a C. um. That seems to be how that goes. Uh, I, I noticed, I went to a thing called a Security leader's Happy Hour yesterday. Uh, I didn't know security leaders had happy hours, like maybe happy 10 minutes here and there if you're lucky, but a whole hour. OK, so central security teams are problematic because they don't scale. Like, I'm sure here, everyone has a shortage of good, well qualified security people in their organization. You are probably those people, and you're here this week, so who's doing the code reviews this week? No one. OK, that's the problem is it doesn't scale. So the way that we've adopted in in AWS, which is is not um. It's not unusual is this kind of security guardian, security champion, building a culture of security. Security is our top priority. So the idea here is that we still have our centralized security specialists, but they are sort of evangelizing and they're pushing out to people who are part of the developer teams, and we give them enough knowledge to take on, if you like, a secondary function. They are the one who at the end of the day goes, hang on, have we considered security? They run the checks, they make sure things happen. So basically centralized security doesn't scale very well. You just get really tired security people and you can never hire enough of them. The problems with security guardians and security champions is it's a little bit like our developer tools. It scales better, but the problem is is that they're all human. Are they all applying the company policy exactly the same way? Are they all as diligent as they should be? The answer to that question is no, because they're human. They get tired. Some of them will be new, some of them will be tenured, some of them will have their pet favorite things, and some of them will ignore other things. So the inconsistency is the issue you get with security guardians. Good news is they're closer to the code. So, where do we go? Well, of course, we're gonna use generative AI aren't we? Because like, if you haven't got the message, that's what we're gonna do. Like, everyone's gonna do generative AI with chips. Um, lots of chips, as it turns out. So generative AI MCP and tools, I'm gonna run through some terms. You probably know these terms, uh, I'll, I'll, if, if I see lots of nodding heads, I'll move on faster. So, uh, the idea here is that that I want to do is I want to keep developers in their IDEs as much as possible. I want them using the tools that they use as much as possible and not creating spreadsheets. The objective is not to build a better spreadsheet, that's not my goal. Um, so from that, uh, that becomes a client, and then we've got this thing that is emerging called MCP. It stands for Model context Protocol, and effectively, these are becoming what is described as like a USB-C interface between generative AI models and the backend tools, OK? The solution we're gonna show you today doesn't use MCP. I just want to be absolutely transparent about that. Um, I would love to have done that. I, I use it on my own IDE but we didn't use it in this solution. Give me another two weeks, I'm sure I'll use MCP, OK? So MCPs act as an interface, and then we can interact with our LLMs and here, of course, it's AWS, so I've said bedrock agents, and we are going to be using Cloud 4 today if anyone is interested. Uh, why? Because they said it was good at doing code stuff, so that's why I used it. Um, I have not done rigorous testing on every single model we have, um, but it it it does do a pretty good job. The problem with LLMs is they're trained at a particular point in time. It's like they get to a point in time and their knowledge is frozen. We've all been there. What does that cause? Hallucinations. So that's a problem. We all know about that. How do we address that? We give them tools. The tools allow them to do things like query a URL. It gives them the ability to look at a knowledge base. It gives them the ability to maybe query. A database. So, to be honest, without tools, LLMs are pretty limited, and I think most of you probably know that, you're probably seeing that in your day to day, that they are so much more useful if they can do things with tools like classic, how many hours are there in strawberry? LLM will get that wrong. But if you combine it with a Python script, it can tell you how many hours there are in strawberry. That's the trivial one. OK? So Agents and agenttic. An agent is LLM Agentic is an agent that is allowed to do something that a human would be allowed to do. So an agenttic uh architecture is one where we're allowing the agents to actually take control, think autonomously and take some actions and hopefully, uh, as long as the demo works, we'll see some of that in the logs. OK, so, um, the next thing we have is, uh, we want to try and make a supervising agent that can control lots of different agents. So the idea is, is that we have uh LLMs are really bad if you give them too many tasks. Uh, so what we can do is we can have. Different agents performing different specialities, and then they can be coordinated by a supervisor. Yeah, so this is just like, you know, basically a project manager. Have you done your Jira tickets? Yeah, of course. Well, the agent probably has because it's a robot, the end of the day. The supervising agent coor coordinates the activity, returns the output. And then after that, we could submit to someone for a review. So, um, the next thing we're going to do is jump into the code demo. Yeah. There we go. So, as I promised, I will chip in only for a tiny bit, and what I'll try to do is try to keep it. May not be that interesting, but at least short so that the interest can continue with with the demo Charles will produce. So as, as Charles mentioned, um, the pattern we are using is the and I hope you guys can, can see it all right. Let me try to make it a little bit bigger. Is, uh, the supervisor agent, uh, pattern for managing the, um, orchestrating the task between different agents or tools which will perform the individual tasks. Uh, As Charles mentioned, it's not that we have developed this entire, uh, solution yet, uh, what we would be demoing, but the good thing about this architecture is that, um, it's expandable. It, it can expand to your needs and um. As you, as an organization learn or adopt new tools, all you have to do is a simple way to plug in those tools to actually expand as your organization needs, needs to expand. So what it exactly it would do is um. First thing first, uh, sorry, let me try to see if I can make it a little. Bigger. That's good. Good. OK, so as the code is released in the Grier, it would then trigger the sort of, uh, everybody of us who have done the DevOps, the, uh, post commit uh trigger which will then invoke the supervisor agent. It will identify what exactly it has to do or in other words, which particular agent next it can invoke so that it can bring the uh the flow in, uh, to, to start delivering. And it, what it would do is it will go and do the security review agent invocation, which will then in turn go and identify what kind of code you got. So for example, you got terraform. OK, fair enough. So terraform, I got a Chekhov agent, uh checkout tool which can actually scan it. So it will invoke that tool, use that tool, generate the report and pass the information back that the report is now generated. Supervisor managing the orchestration will next go to the um issue analyzer, which will then analyze all those issues. OK, do you want me to categorize? Yes, any critical or high, let's actually fix them straight away and raise a merge request so that the developers can. just review and merge it back or let's say if it's uh low or or medium defects all we want is can we actually just put a Jira ticket out there and these are the parts which as I said, not completely dull but can be enhanced uh or or added on to achieve those um and then for the orchestrator will go and invoke the integration agent agent from the Git's perspective which can then merge that code back or or raise a CR back sorry into into Git. Yeah, that's pretty much from the architecture perspective. Uh, any questions on the architecture? No? OK, cool. Um, so what I'll do is I'll quickly jump on the, uh, deployed infrastructure which we have on AWS account. The demo will happen from the, um, local machine. The reason being it's more interactive. It will show the log flowing and it will be more, um, visibly, uh, better than, than online. Um, this is, I would say one of the easiest architecture. I had to build, uh, there are 4 different ways or simplest ways you can actually uh deploy your agentic uh infrastructure on, on, on AWS for example, you can use lambda, you can use Fargate with ECS, you can deploy typically any compute as you, you, you use on AWS. You can do EKS or you can use EC2. What we have done is we, I just use the Fargate profile for an ECS. So, um, a simple service running using. fast API which Charles will talk about when he will do the demonstration. Um, that service, uh, exposes an STTP endpoint, um, using a simple load balancer and hitting that load balancer will invoke the supervisor flow and start doing the, the magic it has to do. So apart from the networking around this, literally that's it, that's, that's what it has, number of tasks which uh works on several less target and then keep coming as you as you need those, um, to, to run the flow. Yeah, over to you Charles. Great. I'm a gentleman of a certain age that like if I stick the glasses on, I can't see you. Same here. If I don't stick them on, I can't see the screen. Yeah, and it's relatively recent, so I'm still getting the hang of it. So, so first of all, like what, why, why should we, we, we're going to demo this on my local environment and we're going to do that because basically I can tail the logs and uh there were two ways that this was going to go. It's either going to be like watching paint dry because it runs and you don't see anything because it's in cloud watch or it runs and we can actually show you it. But what we did is we've built it in Docker. So the idea is, is like we would export this straight into Fargate, it would run inargate. And We've put an API on the front end of it. Why? Because we're AWS and we put an API on generally the front of everything. Um, someone once described ticketing systems as APIs for humans, um, which I quite liked. So, so we've put an API on the end on the front of it, but why is that sort of relevant? Well, do you remember I said that one of the issues is, is like, there's inconsistency across your. Everyone's running different tools, doing different things, different versions of different things. What happens if the policy gets updated by security? How is that ingested and used like this information takes time to filter across. If you provide an API, you can centralize all of that, and then people can hit that API, then they get consistency. That's my logic. We'll see if it works. OK, right, so first things first, who here has heard of uh strands? Ah, a few people. Great. Does anyone want to describe what it is? It's a uh it's a, it's a Python SDK for for kind of writing generative AI. Perfect. So we released it what, about 33 weeks ago, something like that, 34 weeks ago we made the announcement. Strands, the idea is, is it's like a double helix of, of, of DNA you've kind of got the, the, the SDK bit which is partnering with um. The the uh the agents. When I started building this solution, I was gonna do it using CDK I was gonna do bedrock and I was gonna do um bedrock agents with a schemer. With tools, and do you know what, it, it got complicated. I started to find it really got complicated. I found strands and, and honestly, if you've got a bit of Python development experience, I really encourage you to go and find it and and start looking at it. You'll be up and running very, very quickly, just with your first kind of like, hello world. It, it works straight into bedrock, and you can start building really quickly. And what I'm gonna do is I'm just gonna walk you through quickly the um the kind of way it works. If I can find where I'm looking in here. So first of all, let me. Can everyone see that OK? So in the strands directory, we've got an app, and the first thing I'm gonna do is I'm gonna go to this agents directory. This is entirely, you know, this is just Python, right? You don't have to construct it this way. I just did it this way because it was, you know, 3 a.m. and I had jet lag. Um, but, you know, I've kept it nice and simple. I've kind of got agents and I've got tools. That takes me back to that diagram I had earlier. And in here, I've got my supervisor agent. I'm just gonna walk through some elements of that, uh, lots of import statements at the top, some logging, because we like logging. And then we've got a supervisor system prompt. You can see it's quite long. Um, I'm certainly not a prompt guru at all. Uh, I found that with prompting, there's still a lot to learn and you've got to tweak it a little bit. Um, to get your model to do exactly what you want to do, there are some, you know, hard do's and there are some don'ts. Uh, sometimes you need to tell it different things, uh, when you're testing it. So what I would say is if you're going to build a solution, test it, you know, see what it does, because it might surprise you, and then you need to refine that. Um, what I've also done down here is I've started to tell it what tools it has available to it. So I'm saying to it, look, you know, to do your job, you've got these tools, you can go away and do them, and I describe what each of those tools does so that the agent knows. Um, the next session here is, uh, some classic, uh, uh, boto 3 session. Um, when you go into the, uh, when you go into the actual, um, strands, it, it doesn't require you to do this. It will pick up your session, but you can get additional configuration by doing it this way. You can see here that um I've opted for Claude Sonic 4 out of the box at the moment, it's using Claude's Sonnet 3.7. So that just allows you to change it. And the one thing I would say is that you need to have this little US dot, uh, or, you know, EU wherever you're based, um, because the way it works at the moment is depending on what region you're in is it will go to, um, uh, another region. Um, uh, it, it uses, I'm trying to inference, yes, to, to do that. So it shares it. So you need that US uh piece on the front or EU or whatever region you're operating in, and you can get that information either from the CLI from Boto 3, or just go to the console, of course. Um, here we can see that, uh, we've got our agent. So if you're going into, um, if you go into strands, you're constructing this agent, and then you can see that we've just given it these tools. Now, some of the tools are built in, um, file read, file right are both built in tools. It gets that out of the box. You just have to import them. Um, and then you've got these custom tools that we've given it, the execute checkoff command, get checkoff help, validate file pass, etc. etc. Um, HTTP request is also a built-in tool as well. So these are just the tools that we give it. So if you don't, if you don't have this bit, it doesn't matter how good your prompt is. Your, your prompt can say you've got access to the world. But if you haven't specified them here, it's not gonna do anything. OK, so That's pretty much all I'm going to show you. The rest of this is kind of like just the the making it work bit. The next piece I want to show you is the tools itself. So the tools are just uh imported um up here. And I separated them out just for clarity as well. Um, let's go and pick the Chekhov tool. So I think probably a lot of you know Chekov is an open source scanning tool. Um, it's well maintained, it releases almost every week. Um, so it's a really good tool for checking a number of different things. Infrastructure is code in particular, uh, but it will also do Kuberetti, so terraform, cloud formation. I like it because it's got something like 3000 checks, and, um, it's just got great coverage. So, um, how do we define what the tool looks like? We use a Python decorator. At tool. So if we put at tool at the start here, um, what that basically does is it wraps the the function and then as long as it's imported into your um your agent, that is basically you telling your agent, discover this, and you can use it. OK? Um, Just for absolute clarity, when you go into the Strand site, what you can find is that your agents can also be tools. So you can have a model where you can have a tool which is just some Python. But you can also make an agent itself a tool. So it's a bit like turtles all the way down. You have a supervisor, and you then make another agent, also a tool of the supervisor, and then that tool, uh, that tool will then have a tool. So in theory, you could chain another agent on the end of that. Everyone with me, everyone following? OK, good. I just want to sort of get, get that out there. And actually, I think it's one of the reasons why it's good to separate out the the kind of the hard tools, the Python tools from the agents, because otherwise, like if you were to put this all in one file, basically everything's going to have a tool decorator on it, and you're not going to have a clue what's going on. So that's how that's kind of constructed. The only other thing I want to kind of bring your attention to with this Chekhov one is, uh, if you actually have a look, um, those of you that are familiar with Chekhov will know that the, the command is something like Chekhov minus D path to some directory, dash O JSON. And sure, you can construct like a, you know, that path, if you want, you can say, go to a directory, scan everything in it, and give me the output as JSON. You won't see that here. Why don't you see that? Because I'm letting Gen AI figure it out. Um, I'm basically not telling it what to do. I'm basically saying, here's a directory, here's some files. You go and figure out what command to execute. And then it uh it just basically is doing a subprocess uh run. So this stuff here, this is just, this is Python, right? So, if you kind of see this and go, yeah, well, there's probably a tool out there that does all of this already, and I can go and pay a third party for that. You can absolutely. But the point we want to get across is like, this is just Python. If you have your own tools and you want to build an agenttic front end on it, you can do that as long as you can run some Python, run a command line, use subprocess, you can start to build these things out yourself. OK, enough of that. Right. So, where are we gonna go next? Hopefully this is going to work. This is the bit that, you know, I've just refreshed my, uh, my credentials, and uh it might not work. We're gonna find out real quick, right? And while you're doing that, so I think the important point Charles is also making is that it gives you that sort of a platform independence where you're not actually just relying on oh I have this particular CICD tool and I wanna actually run it with only this tool. It, it's like makes it work with with any tool you want or any CICD you have adopted. Um, It's worked better in rehearsals eh? Ah, there we go. OK, I'm going to let you into the secret source of AWS code presenting here. Normally I'd have another monitor, but it's really small up here, and the other monitor is the PowerPoint that you were looking at earlier on a separate computer. So just bear with me, OK? I have not memorized all of these uh from scratch, OK? OK, you get to see the speaking points as well. Pro tip, if you're ever doing one of these and you're using, you know, uh, generative AI to help you with this, what's really good is it knows the code base better than you. So you can say I'm doing a demo and uh help me write the uh demo. And this is what it'll do. OK, so, um, the first thing I want to do is we, uh, we, we went through the Bodo 3, the session, that was all, uh, we went through that. And the, the next thing I want to do is I want to kind of show you how we are trying to hit the, um, Hit the actual um The end point So if, if we kind of, I'm gonna move this up a bit, I'm gonna move that across. So on, on the split terminal here on the on the right hand side as you look at it, what we've actually got is a health check, which is just coming back from um our API endpoint, which is effectively just a local docker container we've got running and it's just returning this health uh health check. Um, so the next thing I'm gonna do is I'm, I'm gonna try and run this command here, uh, and see if, if this is gonna work. Uh, this will be the first test of. Whether we're gonna jump straight to QA. Oh no, we're not going to QA. You've got to continue hearing me a little bit more. OK. So, um, Let's just see if this is running, and I, I'll explain like I, I've got an API but who likes running curl commands and we don't, so I've wrapped that now in a CLI. OK, this is why I'm running these commands right now, but I, I will show you that it's an API under the hood in a bit. So, um, effectively it, it's saying it's connected and uh it's found what we need. Um, Let's see if we can get the health check. Good, that appears to be working, which is handy because it mirrors exactly what's on the other screen. That would have been a very awkward if, if it hadn't worked. So the next thing I'm going to try and run is I'm going to try and run this curl command, uh, where I'm just going to hit the uh the API end point and what have I got in here? So effectively, I've got a path to a file. Uh, which is, uh, conveniently, uh, the developer is called um EC2 insecure just to help me out. And I've got, you can see a prompt. So the prompt is simply scan the cloud formation file in, file reference for security issues and make specific recommendations. And let's see if this is going to work. OK, so hopefully on the right hand side here, you can see. The telemetry that we're getting coming off the um Of the bedrock agent So, again, like if this was in cloudwatch, we'd all be waiting patiently for cloudwatch to show this because it's not in real time. Uh, but we can see that actually it's gonna go through various stages and it's gonna, uh, talk. So that perfect the Chekhov scanner is completed successfully, let me know. Um, that is not me, that's not hard coded, that's bedrock, uh, telling me it's doing a perfect job. Has everyone noticed that with their Gen AI agents? The thing I like is when you go, no, you've done it wrong, this is the problem, and it goes, you're absolutely right. Everyone get that? Does it just like make you boil inside when you point out like you're absolutely right. So what's appearing on the logs here is basically what is going to appear uh as the output from the API. So it gives you an idea that behind the scenes, Bedrock is is doing stuff, but it's not gonna return the result to me um just yet, and I suspect you can, you can change that functionality. Um, So what we've now got is we've got a nice markdown file. Let's take a quick look at what it said. So first of all, it's um It's given me the uh the Chekhov scan results. That's good. I actually like that. The reason I like that, that's a confidence check for me because I told it to use the Chekhov tool and it's gone and used it. The thing that a lot of security people don't like is the fact that uh AI tools are nondeterministic, meaning they can make stuff up. They can change it. Each time you run this, it could change, right? So the way we make it deterministic is we use deterministic tools like Chekhov, and we then get the, you know, we then get the AI tool to analyze that output. Probably still not foolproof, but it's better than nothing at the moment. So the fact that it's told me which checks passed and failed is good. That's a good confidence boost for me, that it's um, that it's actually producing something from Chekhov and hasn't just made it up itself. You can see here that it's also got um CKV AWS 24. Again, that's a good confidence check that it's using, um, it's using uh Chekhov under the hood. OK, so that was the uh that was uh uh API call and the rest of this I'm now gonna do by um just basically the, the CLI command I've wrapped. Um, we saw in the thing that I'd produced a little help function there, so it will return that help function. Um, I've created this, uh, ahead of time, I've created an insecure bit of terraform here. So I'm just going to try and scan this file using um. Uh, using the, uh, the code here. let's move ahead. So on this one, I've said, give me the top most 5 critical. So the prompt starts running. And it's going to focus on the medium and high severities. What it's doing whilst whilst I'm doing that is like if you were running this yourself, um, or you gave this tool to your developers, you wouldn't want them to spend their time writing prompts. So instead, what we can do is we can hard code some of those prompts into the CLI, and then it's going to give us a consistent way of asking each time. And that's exactly what I'm hopefully going to move on to. Um. We might, we might run out of time before we get to it, but one of the tools that we've actually also developed, which will show you the output, I don't know whether we're going to get to show you it working, is the fact that we can also raise issues in our git repositories as well. So the way I envisage this being used is if you put this into commit hooks, you've now got that shift left, so the developer can have a prompt which is telling them. Um, early on when they try to commit something, uh, it's going to do a pre-commit hook and it's going to say you've got these issues, fix them. And then as you move through the development life cycle and they get into actually pushing that code into the repo, then you can continue to, um, uh, you can continue to actually have more stringent texts or checks all the way up to an audit. The other thing I'm kind of quite excited about with this is like as security people, who here has ever mapped one thing to another thing? Have you ever noticed like no one is ever happy if you say, oh yeah, we've got this thing, they'll say that's great, but how does that map to that thing? Yeah? So someone goes, oh, brilliant, you've now found 500 things that Chekhov says is bad. How does that map to the MTA framework? How does that map to the AWS secure foundations? Oh my days. You know when they say AI is going to put people out of a job, like this thing has that potential, like we're all going to have to like actually start thinking for a living instead of going, now how does that S3 bucket relate to that security control which relates to that, um, you know, that framework which relates to that. Like this, these kind of things are going to be really good at basically doing that for you. So it's going to find something wrong with an S3 bucket with an EC2 instance or whatever, and then it's going to be able to have a tool which will say, I'm going to look up how that relates to that control framework, and it's then going to come back and say, if you continue down this path and you push this into your environment. What's going to happen is Security hub and config are now gonna have a finding against the AWS um secure foundations or against CIS or whatever framework that you're choosing to use. So we can start to bring things together. So if you are doing that code review, you're now getting a report that tells you exactly the impact that you're going to see if you continue down that path, because from a developer's perspective. Uh, they don't really care. They just see insecure buckets or insecure EC2s or whatever it is. But from a security and compliance perspective, you see everything as a compliance issue and somehow meet somewhere in the middle, yeah? So these tools are gonna be really good at that. Um What else can we do? I guess you've probably, you're probably familiar with this, but we can ask it to. We can ask it to. We can ask it to give us some uh fixes as well, so we don't have to kind of like ask developers anymore, oh, you're a naughty person and I have no idea how to fix it. That's your problem. Now, of course, we can go back to them and we can add value as security professionals. We can go back and say, hey, I need you to go and fix this thing, and here's why I need you to fix it, because of this framework that we comply to. Oh, and by the way, is how you can fix it. OK, suddenly that's making it really easy. It's up to you if you want to go one step further, which is, by the way, here's the pull request that I've automatically raised, just, you know, pull that in and merge it. That's maybe a step you don't want to do, maybe you want to have a human in the loop. Maybe your development team do not want the security team telling them how to write code. Um, there are a few gotchas, like some of the things I've discovered. If you create an S3 bucket and you don't include logging on the bucket. Um, it's going to come back with the findings saying there's no logging on this bucket, and it's going to suggest what we should do is create a bucket to put the logs in, so you'll end up with a code block that basically says your bucket and the other bucket that we're going to create for you to log to, uh, which is one of those things. What are all the buckets that get found on compliance checks that have no logging enabled on them? They're the logging buckets, um, but you can log to a logging bucket. But yeah, potentially there are still some rough edges. So the way that you can get around that is you can start to build in a knowledge base, retrieval augmented generation, give it a knowledge base, you can upload your security policy. In there. So, for example, there are probably exceptions in your organization that you're willing to accept. The example I'm going to give, maybe some of you think this is a heinous crime, don't shoot the messenger, but lambda functions do not have to live in a VPC. Maybe that is something that you as an organization are like, OK, I can live with that. That's fine. It doesn't have to be in a VPC. Maybe it must live in a VPC. It's up to you. A lot of the tools that you're going to find that you can go and buy that will have a lot of features that are really good. Uh, and they are using AI. They might not have that customization. So if you're gonna like go, Charles, I'm never gonna go and build this, right? I'm just gonna go and buy something, right? We've got budget this year, I'm gonna go and buy something. Look for tools that will allow you to customize according to your policy, because you're probably thinking, OK, Charles, why don't I just use Amazon Q? Why don't I just use like any of the AI tools that are out there to scan my code. Well, the thing is, is they're all kind of generic, right? You would have to give them that policy each time. So I would look for things that can give you that knowledge base, um, and it will be things like what are allowable exceptions, uh, etc. etc. So go and look for those when you're, you're choosing tools or you're building them. Yeah, and, and one just to compliment that one important point is also that yes you have the tools which can be configured as per your organization's needs as well, but the tools like this actually adds that intelligence, for example, if your policy may be that I don't, I must not have any public S3 bucket, then you say configure my tools so that you know it actually. Allows that critical issue should not have that bucket or you can say no I will I will allow so it's if and else kind of situation whereas with intelligence built in it can actually go and figure it out that there is a reason you have public bucket because you have a static website from that public bucket, but that doesn't mean that the rest of the buckets can be public so to allow that inbuilt intelligence tools like this with GAI is really helpful. So I, I want to leave some time for Q&A, um, and I know you've been here for a few days and maybe, you know, enjoying some hospitality in the evenings as well, or you're just like me and completely jet lagged. Um. When I started out doing this, and, and again, I wanna like go on go on the journey, like you come across a a compliance issue somewhere in Security Hub or config or just wherever you find it and you take a look at it and you go, yeah, we should fix that. We should really fix that and like maybe some of you have gone as far as automating those fixes. Right, bit of audience feedback. Who can tell me the problem with automating fixes? Like we've been able to do that for 89, 10 years. What's the problem with automating our, our way out of the problem using config and lambda functions? Anyone? Who, who uses terraform? OK, what does terraform create when you use it? State file. So your developers are busy developing state files. What are security people busy doing? Creating stuff that ruins state files. OK, not the file itself, the file itself is fine, unless it's going to the S3 bucket and changing that. But the problem you get is like, you change the, the thing, and that's good from a security person's perspective because like, Yay, happy days. Friday has come and security hub is looking fantastic. I'm on holiday. The problem is, is you've changed the configuration of half the things. And then a developer comes in on Monday morning. Picks up the Jira ticket and goes, Oh, that's a quick one for Monday morning, boom bang bang bang.us. And the first thing the state file does is go and change everything that the config rule, which is like, to be honest, it's great for Amazon revenue, because like, you know, you're just like changing things backwards and forwards, but it becomes a bit of a crazy situation as we're just changing things backwards and forwards. That doesn't work, OK? So automation definitely has its place. I'm not saying don't, don't automate the the problems like, I think that there are times when it's worth it, and we, we haven't got time to get into the strategies about tagging and all of that stuff. But the issue you get is when you automate your way out of this problem, it's a big issue. Now, if you tagging strategy is really good, what you can probably see is the developer who created the thing. So you then go to the developer and say, hey, you created this thing, it's got a problem, can you fix it for me? Yeah, sure, no problem. Who in their organization is at that level of maturity? What company is it? Excellent. Can I come and be your consultant? I'm just like, that's brilliant, but no one else is at that level, right? Everyone else is now just looking at 3000 S3 buckets that are configured badly and going, who am I going to talk to about this? Who do I talk to? So, The answer to all of that is actually In your GitHub repo, or Git repo, this just happens to be my personal one, because I'm a low budget kind of guy and because we got rid of our own GitHub repo, or we're getting rid of it, um. But the information is all there, like Charles is there. Somewhere there's a commit with a commit ID of some code that I put in. So this becomes like a fingerprint, like literally your fingerprints are all over the crime scene when you're a developer now because we can start to extract that metadata, and we can start to pull it through into the tools and we can start to use that. Inside our bedrock as well. So now we've got Chekhov scans, we've got bandit scans. We're now pulling information from our Git repo, creating a unique fingerprint that tells us what the committees will even we're we're even able to go back and look at that commit, and it's going to tell us who committed the code. And it's even got information if you're using code pipeline, it's even got information like the email address. So you could automate that. So instead of automating the fixing, we're now automating the process of going back to the developer and saying, hey, I know it was Friday, I know you were busy, but could you fix this for me? So we're starting to fix things at source rather than just automating our way out of the problem. The other thing that we can do with the tool, um, is that we can create a tool around GitHub. I can tell you now, GitHub last week or so released, maybe it might have been earlier, someone might tell him, tell me, uh, an MCP for their uh Git repos. So this is that interface between um Gen AI tools and the actual things. If not, the APIs are there, you just write them, OK? You just write an API call. And if I show you what what it did. So I think You remember, you remember we were kind of raising our reports and you saw all of that data scrolling up the screen. Well, that was all marked down, right? I know you were probably thinking that's not very pretty, but I could have saved it into a file and we could have looked at it. But if you take mark down, stick it on an API, you just raise it straight and you'll GitHub repo. You could do that as a pull request, you could do that, uh, you know, it doesn't have to be an issue. Um, you can tag it as security, you can tag it for the person who needs to fix it because they were the one who broke it. So what we're doing is we're shortening that loop dramatically, like we're, we're going from like, OK, who, who's the people at the back, that company, you're in like, what? How, how long would you think it takes detection to remediation? It Minutes, so you're at minutes. Who here is at days? Who here is at M? Or weeks maybe weeks, OK, weeks, weeks to detection to remediation. Who here works for a really old bank and you're in the light decades? Gentlemen down there, you win. Like we all know, it's really hard to find those bugs. It's hard to find the people who need to fix them, and it's harder when someone has left the company as well. But that's the reality is, is like we can now go from uh commit detection. Um, firing it straight back into the GitHub repo, raising a pool request, and actually surfacing the issue, telling people exactly what's wrong, suggesting a fix, they can try it out. Um, with the, with the strands tool I mentioned or Strans SDK, I mentioned the fact that it has certain tools built into it. One of them is actually a Python environment. So what you can do is you can say, hey, I want you to go and um Uh, you know, detect the problem in the python, fix it, and then I want you to test it beforehand. And you can also get it to say. Um, you know, does this actually fix my, uh, my Chekhov problem? Keep going until you've found a fix, right? So it can do that rapid iteration as well, and it, you know, it takes minutes. So That's kind of, I think all I've got to show to you, um, this is like about a million years of being production ready, OK? I'm not trying to sell you anything production. What I'm trying to sell you is the fact that I'm a security consultant who writes. I would say average Python. I'm not an I'm not an SDE. I don't have a background as an SDE. I have a background as an army officer, um. But I was able with a bit of knowledge, a lot of sleepless nights to produce something in Gen AI. Who was around for the dotcom boom, who remembers that? OK. Dot com boom. Do you, uh, trick question, don't have to answer, but do you remember what the Amazon.com website looked like in 1995 when it launched? Yeah. Would you buy anything from it today? You wouldn't, would you? But we all did, we bought books because we thought we needed books. So. My point is, is that I personally think that this, this Gen AI hype is not going anywhere. I think it's here to stay. I, I, I, I just think it's reached critical mass, but we are in the foothills. Take yourself back to 1995 and the Amazon website, looking at it historically and going, I would not buy anything from that today, but then just project forwards to where this could be. And I personally feel, and I'd like, you know, this is my call to action for all of you. If, if some of you have been a little bit inspired by what I've said, go and look at the strands SDK, get yourself set up, have a play, build something, play with it a bit more, because what you're doing now is you're going from a consumer of Gen AI to building the future of AI just like once upon a time, someone thought. Yeah, I wonder how HTML CSS um works like I'm going to go and build myself a website. Yeah, we all have to go through that journey, and I think that's a journey that's worth going on. And that's been my mission for this year is to actually learn how to build with these tools and build something that is going to be useful for me and hopefully useful for you and uh and and my organization. OK, we are out of time. Um, thank you very much. I really appreciate that, um.
