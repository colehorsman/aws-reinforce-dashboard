# AWS re:Inforce 2025 - Behind the shields: AWS and Anthropic's approach to secure AI (SEC303)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=Vd01V7Hni00)

## Video Information
- **Author:** AWS Events
- **Duration:** 60.2 minutes
- **Word Count:** 11,190 words
- **Publish Date:** 20250620
- **Video ID:** Vd01V7Hni00

## Summary
This comprehensive 60-minute session features Matt Santer (AWS Security Specialist Leader) and Shahzi (Anthropic Risk Management Lead) discussing the collaborative approach to secure AI deployment. The presentation covers the distinction between AI safety and security, with safety focusing on harmful output prevention and security maintaining CIA triad principles. Key topics include Anthropic's Responsible Scaling Policy (RSP), Constitutional AI classifiers, ISO 42001 certification, and AI Safety Level 3 standards. AWS contributions include the three-tier security model (infrastructure, Bedrock service layer, and application layer), Nitro hardware isolation, and Bedrock Guardrails. The session demonstrates practical security controls including evaluation systems, prompt leak prevention, hallucination reduction, and the new automated reasoning capabilities for policy validation.

## Key Points
- **Safety vs Security Distinction**: Safety prevents harmful outputs while security maintains confidentiality, integrity, and availability - both must converge in AI deployments
- **Anthropic's Responsible AI**: ISO 42001 certified, Responsible Scaling Policy (RSP), Constitutional AI training, and AI Safety Level 3 standards for Cloud 4 models
- **Three-Tier AWS Security**: Infrastructure (threat intelligence, Nitro isolation), Bedrock service layer (data privacy, encryption), and application layer (guardrails, evaluations)
- **Custom Evaluation Systems**: Task-specific, automated, volume-over-quality approach with code-based, human, and LLM-based grading methods
- **Constitutional Classifiers**: Built into training pipeline to define helpful/harmless boundaries with continuous monitoring of inputs and outputs
- **Data Privacy Guarantees**: Zero customer data used for training, regional data residency, encryption in flight, customer-controlled KMS keys
- **Bedrock Guardrails**: Content filters, prompt attack detection, PII filtering, contextual grounding checks, and new automated reasoning capabilities
- **Agentic AI Security**: MCP server security, trusted connections, API permissioning, prompt injection awareness, and behavior monitoring
- **Compliance Focus**: FedRAMP High and DoD IL4-5 approvals, 20+ compliance programs, working backwards from customer requirements

## Technical Details
- **Nitro Hardware Isolation**: Specialized hardware outside Linux kernel providing secure boot, hardware-based encryption, and operator access restrictions
- **Constitutional AI Pipeline**: Training with helpful/harmless examples, self-critique loops, reinforcement learning integration, and runtime classification
- **Bedrock Data Flow**: VPC endpoints, private link support, orchestration layer separation, model provider isolation, and API-only model access
- **Evaluation Framework**: Pre-deployment (cost-effective) and post-deployment (real-time insights) with harmlessness screens, input validation, and continuous monitoring
- **Prompt Security Controls**: Context/query separation, post-processing filters, minimal data exposure, and audit checklists for high-risk scenarios
- **Guardrails Architecture**: Input/output filtering, multi-modal toxicity detection, automated reasoning with mathematical proofs, 85%+ improvement over base models
- **Automated Reasoning**: Policy consumption, variable/rule extraction, QA validation process, binary yes/no outputs using mathematical proofs
- **MCP Security Best Practices**: Trusted server connections, careful API permissioning, prompt injection prevention, behavior monitoring, and auto-scaling considerations
- **Threat Intelligence Integration**: MadPot honeypots, Mithra DNS protection, Scenaris proactive defense, real-time (seconds) threat feed updates
- **Compliance Architecture**: Multi-layer encryption, CloudTrail integration, regional data residency, VPC isolation, and customer-controlled access patterns

## Full Transcript

Uh, myself and Shab are extremely excited to share a little bit about what both AWS and Anthropic are doing to help you deploy Gen AI securely at the model layer and at the provider layer. Let's get to, uh, know ourselves a little bit. Uh, you wanna kick us off? Yeah, thanks, Matt. Um, so hi everyone. My name is Shahzi. I work at Anthropic as the risk management lead there. Um, I'm based out of Boston, been at Anthropic for about a year. And uh couple of interests that I have I like Lego sets I've had a chance to build a few here and there and then um I do enjoy word games all the time so I usually try to start off my morning with my wife playing some word games and then uh I'm an advanced, uh, certified scuba diver, so I've had the opportunity to scuba dive in a lot of places across the world, um, and then I've also picked up on this pickle ball mania that's been going around the world the last couple of years since COVID. Um, so, uh, jumped on the bandwagon for sure, but if anybody wants to talk to me about if it's a real sport or not after the, the, the session, happy to have that conversation throwing down the gauntlet. I, I think that's a challenge if anybody wants to take him on. My name's, uh, Matt Santer. I lead security specialists and I have a privilege of supporting about 250 of our largest, most complex global customers across multiple industries and again across the globe. Um, and helping them achieve their business outcomes through security solutions, um, outside of the day job, I, uh, I, I'm a general aviation pilot, so I love to be up in the sky as much as I possibly can, but other than that I'm really happy to be here with you today and then as a dad, you know, my kids kind of dictate all the rest of my time. All right, so today, uh, gonna give you a precursor of what we're gonna go over, um, so what's the current state of AI, uh, safety and security, and we're really gonna disambiguate the difference between safety and security, which I think is important. Uh, heavy lifting. So what is it that both AWS and Anthropic are doing on your behalf, both, uh, from our side of shared responsibility model as well as what you need to then take forward on your side of the shared responsibility model. Um, thinking about that layered safety and security, what are all the things that um are unique nuanced in an AI world that you need to think about from a safety and security perspective? So we'll cover a bunch of those different topics. I just wanna kind of level set with where are we? So end of 2022. Uh, the world all of a sudden was like, oh, this AI thing is very real. Uh, AIML is not new, it's been around for decades, but the democratization and the exposure of that through, uh, Open AI and Anthropic loud kind of emerging on the scenes and all the other, uh, big providers in AW. US jumping on board this really launched into the evolution of experimentation production uh productionizing those experiments and now saying OK we're going further, we're going into agentic AI or looking at things like MCP we're looking at how we scale this thing out and I, I kind of like to use this analogy. Of a park, so when we think about or a playground, so when we think about security, um, that's pretty well defined. We understand what that means safety, um, brings another context into this. So imagine you're out on a playground and you can see here we got. Uh, a beautiful new play set in the background. It's made of materials that are, uh, less harmful than maybe the playgrounds I grew up on, uh, with, with hard metal and rough edges. It's got softer plastics and um you can see the barriers there, a lot of things that keep kids safe if they fall off of something it's a rubber mat uh ground, um, not pavement, um, and then in the background you. The, you know, maybe there's uh a fence, right? So you have also oversight here from two parents that are watching to make sure everything's uh staying safe and secure. So this is a combination of where you have these safety devices in place and you also have uh traditional security devices in place and kind of can think of that same analogy applying to the world of Gen AI and the security practitioners where we need to balance where we we do that. You wanna go ahead and tell us a little bit about how you disambiguate thanks Matt for setting the stage. Um, so with regards to safety and security, when we think about what are the components of deploying a responsible AI solution that you might be doing for your customers or that we're providing to many of you, um, we have to think about safety and security together. So what does safety mean? Safety is preventing the system from uh driving harmful output so encouraging accurate and relevant responses and aligning those goals with fairness and bias in mind and so this gets very, very challenging with large language models because they are nondeterministic they are feeding off of training data sets and so based on that all of us have a responsibility even as security practitioners to take this into account. Security goals have evolved, but the purpose of them stays very similar, right? So we have the confidentiality, integrity, and availability CIA triad along with that privacy is also rolled in for a lot of organizations and so those two goals now with the deployment of large language models are now starting to convert. You cannot do a deployment without keeping those both of these in mind. Now these might not be handled by the same teams these might not be handled by the same individuals but as these goals are starting to merge we need to think about evaluations we need to think about um system deployment with both of these things in mind. So how do we do that now I'm, I've put up a few things on the slides. What are some safety, uh, some safety components and a few definitions of those at the top and what are some security components that have traditionally been there and what those mean at the bottom and so. I'm not gonna read off the slides here, but just so you can get an idea of hey these systems have to incorporate a bunch of these components together to get a trusted output to the user and so we do a lot of this work on the back end as uh uh as uh anthropic and AWS but we're also gonna show you how do you layer on top of that to meet the obligations associated with these concepts. So we have fairness explainability, steerability which is a very new concept that is getting a lot of. Um, that is getting a lot of traction which you will have a lot of control over based on the way I'm gonna talk about the evaluations and then harm prevention, safety components and then security governance, confidentiality and integrity of what's coming back out, um, those are components that traditionally a lot of security practitioners have thought about. So what is our approach as an anthropic on responsible AI? How do we prove that we're doing it for you? So first thing is we're guided by our responsible scaling policy. The RSP was one of the first of its kind, um, that talks about, OK, when we deploy models, what are the factors that we're taking into account to meet these safety and security goals that we've talked to you so a responsible scaling policy evolves over time every model release we're considering based on the responsible scaling policy what we have done. The second thing is we were one of the first AI labs to receive the ISO 42001 certification for responsible and safe AI. So, um, a lot of companies are still working on this, on this certification as as one of the first providers we worked with the auditors, uh, to determine what is applicable here and so, uh, we really are proud of the fact and you know, our audit team did a lot of work to make sure that we're one of the first to market as well as setting the standard for what a lot of other organizations are doing. The last thing and I'm gonna jump into the constitutional classifiers in a little bit, but when we talk about deploying these models over to you, there is things running in the background that are based off of our constitutional AI and the classifiers we've built in there as well as the training that goes in to make sure that we're taking principles of ethics, legal responsibility. Um, as well as harmlessness into account when we're pushing this out to people, so just recently with our Cloud 4 Opus and Sonic release we just jumped to an AI safety level 3 standard. So our responsible scaling policy defines AI safety levels, um, and those AI safety levels are dependent on the capabilities of the model, so the intelligence of the model. So we just updated based on cloud for Opus to our newest standard ASL 3. What that does is it tells you what security standards we follow internally. It tells you what deployment standards we have in place for safety and then when the model gets more powerful and more more capable, what are those triggers that will then drive us to say OK now we need to increase our security standards or increase our deployment safeguards. All this is to get you to a point of how do I, when I deploy these models through um AWS and use uh cloud models, get a calibration of what is my acceptable risk as a practitioner to be able to deploy these models to my customers or my use cases. So when you're trying to figure out what is my acceptable risk here, right? The first thing I would recommend is everybody should develop their own custom evaluation systems and I'm gonna give you examples uh that are technical around why these evaluation systems, the things that we do is also built in but why it's important for everybody to also build their own custom evaluation systems. And these are not only for technical things these are also for administrative process many of you are probably trying to automate or supplement business processes with large language models and so it's very important to have evaluation systems that can apply to both of those. The second thing is every single provider, um, for the most part is releasing system model cards. Now system model cards tell you what are all the risks that we've uh we've taken into. So I, I'm assuming many of you are security or privacy practitioners here and so system model cards kind of thing. Think about them like SOC 2 reports, right? SOC 2 reports tell you what are our practices, who's audited against them, what is the testing? They go even deeper than that a little bit, but if you look at them they're really gonna tell you, hey, for use cases that are applicable to you, what type of testing have we done? What risks did we find and what risks did we already solve for? And then the third thing is when you do deploy them you have to think about the visibility and control so I talk about developing evaluation here when you develop those evaluations you need to also monitor once you're deploying it and uh Matt's gonna talk about what Bedrock guardrails does in a little bit to help you obtain that visibility and control when you are deploying these systems. And last but definitely not the least is you have to find out where your human and AI interactions coexist based on this risk. So if you're trying to figure out hey which systems are we comfortable with the AI models based on the evaluations that we've already done based on looking at the system model cards and the risks identified there based on what we can get visibility and control over which parts are we OK with the model running which parts are we OK, uh, do we need to have humans in the loop. And so this is very, very important components for you to define as you're thinking about going and uh deploying these systems. Now a little bit of a deep dive into evals, so because these slides are going to be available to you after and I think this is gonna be posted on YouTube after the fact as well, I'm not gonna like dive into every single example, but taking safety and security measures into account can be done by using eval systems and eval systems can be done both pre-deployment, which is much more cost effective and easier to do, which we're gonna talk about a lot, um, and then you can also do a post deployment that is giving you real-time insights about how people are using it. So it's very hard for us to figure out all the edge cases. That's why the post deployment is is necessary. But when you look at the life cycle of developing evals, it's mainly around figuring out what are all your use cases. Every company has its own use cases. You figure out those use cases you engineer the prompts to figure out, OK, how is the model gonna respond to them? You test it, you reevaluate it, and then you test it again and then you deploy it, right, and then monitor it. So what are some uh principles you should follow when you're designing evals? So evalve design is very should be very, very task specific. So, um, if you have controls in your systems, um, you have to figure out what are all those controls doing, what are the control activities that those controls are, are, uh, doing as well. So get it down to be task specific all the way down to each control activity. Automate whenever there's there's a possibility to automate. And then last is prioritizing volume over quality. So if you're sitting there trying to write the perfect eval, there's always going to be edge cases that you're gonna miss. It's, it's just natural, right? And so users have ways to change their behavior on these systems depending on what the models can do. And so if you prioritize volume, it'll give you a much better insight over how these evals are done. Now once you create the e valves you have to grade the e valves. How do you grade thee valves? The grading thee valves, there's three big things code-based eval grading, which, uh, which is very binary, right? So you can say, OK, did the the model output X if it output at X, run the formula, check if it's, uh, check if it's, uh, exactly what it is matching. Right, um, then there's human grading. This is very expensive, um, but for very, very high risk use cases this and very high quality use cases this might be, uh, this might be, uh, possible and LLM based grading now that we have multiple model systems in place, so you have, you know, you have claw 3 haiku that can do. LLM based grading of the results coming out from cloud 4 Opus this can get very, very, uh, scalable very quickly and so this because the LLM can kind of grade itself based on your use cases, this is actually a very, very good way to, um, be able to scale your evaluations and change them as the model capabilities change. So now how do you use these e valves to apply security controls? So jail breaks are something that you've probably heard a lot about. Prompt injections is another thing that you probably have heard a lot about. So how do you use e valves to reduce jail breaks and prompt injections? So you can do things like harmlessness screens where you use a lightweight model like cloud haiku to pre-screen the user inputs and so this is when it's coming into you, right? The second thing is input validation, so this is similar to the first one, but this is specifically filtering them, not just screening them but filtering them for jailbreaking patterns. So there are open source jailbreaking tools out there that you can use to that show you what are the patterns that you might wanna be uh uh concerned about. Uh, we've built a lot of jailbreaking defenses into the models already, but if you see something specific to your case that you're really, really worried about, you can use that to do the input validation. You can even craft prompts that emphasize ethical and legal boundaries, so I talked about our constitutional classifiers a little bit where we're doing this, but you might have your own legal and ethical boundaries that you want to consider as you're deploying these. You can craft the prompts to do that. Oh sorry. And the last thing I talked about is continuous monitoring. So jail breaks whenever you see jail breaks come into the system and the classifiers you've already built help you detect those, you can have that monitoring in place to alert your teams to then say, hey, someone is looking at jailbreaking. You can either respond back to them or just have it as an alert to follow up on after. I'm gonna dive into number one a little bit and show you a technical example. So what is a harmlessness screen? So. Since this is a 300 course, I'm not gonna dive into what user assistant and prefill does. I'm assuming as as practitioners you might already know that, but if you don't feel free to come up to me after and I'll explain what these mean but in the user prompt, um, when you're putting in for content moderation, you can actually ask the prompt, uh, you can actually ask the model to ask and say hey. If you see this being harmful and because we've built harmfulness and harmlessness into the model it can determine this for you so if you see harmful, illegal, or explicit activities, reply back with the no. Now you'll see the parentheses in the brackets here for one main reason is you want a binary output. In this scenario, the model will give you a binary output we're used to seeing, OK, you know, models will give you sentences and sentences of information back but if you want to prompt it to say I want a binary output based on you analyzing this, you can prefill and put the assistant in this way and so this parenthesis here will really drive the model to say hey give me only a binary output. So then when you get the N or the Y, you can then use that as a screen to then say, OK, I'm gonna give, give the user back a response, or I might just say no to the user and just shut that conversation down immediately. So that's an example of a harmlessness screen. The second security control I'm gonna talk about is prompt leaks, so a lot of us are worried about, hey, the data that we're giving to these prompts, how do we make sure that that doesn't get leaked out to the system? So the first thing is to separate the context from the queries themselves. So if you think about how we do like SQL injection attacks, right, this is very similar to that where you say, hey, for reducing the for reducing the capability of the model we don't actually use the response back in the query that we're giving back to the user so I'll show you an example of how we do that. You can also use post processing to then determine if the words like if you think about like a DLP tool right how a DLP tool does like screening of uh specific key terms and words before uh it gives an alert this can you do can do the same thing for post processing of the model's output to then say OK if you see these specific words uh remove them before they're going out to going out to the user. And the last thing is avoiding unnecessary details that the prompt might not need so when you're generating the prompt, it's very easy for us to say let's give it everything it has and then let it make the decision, right? Sometimes that is not the best case, so think about what it needs to make the decision and only give it that. The last thing is we we're generating these prompts all the time, right? Developers are generating them all the time for high risk use cases, especially, um, it is very important for us to say, OK, we're gonna just audit the prompt model capabilities might change might have gotten alerts on systems that we're taking into account. Let's just do a quick audit of the prompt to just make sure it's still meeting these obligations, and you can use this as a concept of a checklist. So how do you separate the context from queries? The example I'm giving is an analytics bot that is doing EIA, um, EIA analysis for a financial services provider. So you as a financial services provider might have a very proprietary, uh, formula that you use. Now EIDA calculations are not proprietary. They're very common. But imagine if you had a proprietary formula for generating something that you're trying to give back, right? How do you make sure the model doesn't give that formula away to the system user but you still want to be able to get the results and so the way you do that is for the system you tell it as a job description that your job is to um to analyze based on the e-bidder requirements. The next thing is in the system prompt you tell it, never ever give the proprietary formula away. Then in the user prompt you can again say it to say here's what the user is gonna give you here's your job to calculate it again never give the formula away and then the last thing is again in the assistant prefill the prefill is instructions for the assistant as is formulating the response and if you put that in, uh, if you put that in brackets there, the assistant knows that this is context and not to give it back to the user. So that's 3 separate places you can put in. The context separate from the query because the assistant then is gonna give that back to the user so you're only giving it the formula in one spot but you're telling it never to use the formula in three spots. And the last one I'm gonna talk about is a safety control so we talked about security controls are important safety controls are just as important. How do you use hallucinations? First thing is allowing the model just say I don't know, right? Just how we wanna do it for humans and uh to have the capability to say I don't know when you don't know something, don't make it up. You can tell the model to do the same thing. Use direct quotations from factual groundings which can be used via citations so citations APIs are available for for our models and you can say, hey, based on either your rag or your systems that you're using your system context only always give the citation before you give the response back and then again you can do a screen to determine if it gave the citations. So how do you tell the model to say I don't know the way to the way to do that is again in the user prompt possibly not the system prompt, in the user prompt you can tell it that hey this is your responsibility and find the details based on XYZ formula and if you cannot figure it out based on this formula just respond that I don't have enough information or I don't have the context to be able to solve for this. So that will then help you reduce the hallucinations and it won't go out and try to find sources that are not applicable to to the system. So again you saw I gave you 3 to 4 examples of things you can do, but then I only dove deep into the one example. There's way more security and safety controls that you can customize. Um, you can look at docs. Anthropic.com to see a lot of these. I pulled a lot of these directly from our help center and the second thing is Matt is gonna talk a lot about guardrails on how you can build some of these into there. Uh Thanks. So everybody's probably familiar with the shared responsibility model or the SRM. I kind of mentioned it as we were kicking it off, but the important thing to call out here is, uh, what we are doing on our side of the equation, the collective royal we, um, and we, I've kind of modified this just slightly to indicate that at that software layer is kind of where we're thinking about that LLM. What we really need to be thinking about is not just our generative AI application but the full uh piece of the iceberg right the part underneath it that's really our data foundation and making sure that we're thinking about that safety and security aspect there so at AWS we think about this in a three tier model so I wanna kind of go through tier by tier and give kind of a little bit of examples of what we're doing to help keep you safe and secure. So first let's start at the lowest layer which is really infrastructure and the first thing I wanna highlight is not a Gen AI specific storyline here it is just, uh, articulation of what we're doing at the scale we're doing it, which is just incredible working here or not, I just can never get past these these numbers in the scale that we operate, but you can see the amount of data that we see and process and. The threat and tell that that brings in is directly correlated to what we're able to provide and protect our customers with, so we've talked a little bit about some of these underpinnings over the last year and a half or so. Mad pot, uh, we heard in the keynote, um, from Amy and team. This is a, uh, honey pot on steroids if you will, uh, that allows us to kind of see what threat actors are doing in the environment, use that intelligence to keep our customers safe. Uh, Mithra, uh, also does this, but does this in, uh, in a sense of, uh, uh, DNS and other domain centric call outs what's happening there to protect, uh, and, and protect people and as we mentioned again as the keynote if you call back to that this data is, um, potentially something that needs to be updated in seconds, not minutes, hours, days. Um, and then also how do we prevent, uh, attacks, right? So again through various threat intelligence channels, uh, looking at how we use, uh, capabilities like Scenaris to protect our infrastructure proactively and protect your infrastructure proactively. All that threat intelligence feeds into our core native security service capabilities. We do not release products or services, whether it's bedrock, sage maker, etc. unless we can meet the very high bar of security and enablement there. And so uh when you are deploying these tools you're also inheriting that ecosystem, if you will, of, uh, capabilities underneath the covers. Second thing I want to call out. Is Anapurna Labs is our hardware uh arm and uh we basically build all our own silicone um and when you do that you control the supply chain right both at the software level and at the hardware level and that's really key to being able to enable things which I'll talk about next like uh Nitro. Uh, it also allows you to think about how do you have, uh, encryption, um, all the way into, um, into the hardware, uh, enabled, uh, layer. So let's talk a little bit about Nitro. Right, so Nitro is a specialized hardware. It sits outside of the Linux kernel and allows us to create isolation, uh, that restricts operator access, right? This is, uh, provides secure boot environment, um, it's hardware based, uh, it's, uh, you can see in our our compliance reports and attestations um that this is uh validated to protect our customers from, uh, keeping their systems uh highly isolated. Um, and again this is something that you inherit when you build on top of. Now I'm not gonna go into, uh, deep, deep detail because there was a great session, uh, by some of my colleagues, uh, JD Bean and Jason Garman at Reinvent, uh, last December. It's on YouTube as well. Uh, there's the link to watch that as well as a great blog, uh, more about, uh, uh, securing our AI infrastructure. So where I'm gonna spend the majority of the remainder of our time is in that middle tier we're gonna talk a little bit about bedrock, um, so I think you wanna go ahead and take this section, yeah, so, um, as, as Matt jumps into bedrock, when you use models at the software layer, there are protections that are already built in from the model side. Um, for prompt and inference, uh, inference and so for training what we're doing is in training we're using diverse data sets we're doing a lot of reinforcement learning to make sure that it's helpful harmless and then the last thing I talked about is constitutional AI so this is happening at the training layer. And then after the training layer, the eval layer which I talked about a little bit, we're already doing a lot of evals on your behalf, so we have internal red teams, we have capabilities associated with a responsible scaling policy that we test against, and we even have external testing partnerships with things like the USAIandard Institute as well as the UK AI Safety Institute and biolabbs, um, and CBRN providers and so we have a lot of these partnerships where we're building a lot of this into the model for you. Um, and then when you're deploying we actually have classifiers that are evaluating the inputs whether you're doing it through, uh, even if you're doing it through third party APIs, uh, such as, uh, Bedrock, and those are screening for har harmful and harmlessness for you as well and giving you indicators back. So when, when you get those indicators back, Matt's gonna talk about what you do with them in Bedrock, but we're doing a lot of that indication for you. And last thing is, even after you deploy for this industry to continue to get more res uh more responsibility around how we're deploying models, we're looking at feedback and policy as well. So our RSP is not just for us, we're hoping that it's helping uh increase model capabilities throughout the software stack and then our testing feeds into our usage policy and the model training back. And then the last thing is we do a lot of research to say hey how is the uh we have societal impacts teams that are determining hey when people are using our models so you all might have so many different use cases that you use the models for you wanna see OK how does this actually impact our business users across our entire system and things like our economic index report can tell you a lot more about that to say, hey, people are using it in these ways think about the risks from that context and then build that back into your eval and training. So I talked about during training we have this concept of constitutional classifiers. When constitutional classifiers was released by the company, it was named as one of the three most important AI innovations, um, in 2023 by Time magazine. And the reason for that is because we're building it into this entire pipeline like you're talking about so I'm giving you an example of how we're building these systems into our pipelines. You also have the capabilities to build them into yours. So the way constitutional classifiers work is in training, not only do it tell it what's helpful and harmless we also tell it. What is not helpful and harmless, so just like you need to give it the light, you also need to show it the shade so you can see the light better. So we give that in the training data sets. Then we go through the loop during the, um, during the uh training set generation where we're asking the model to provide examples to itself to say what is harmful and what is harmless. And then when the examples are coming in is pushing it back into its training data set so it's reinforcing that learning of what is harmful and what is harm harmless, um, uh, by going back into it and then the last thing is when you go out and you deploy this that constitutional classifier is constantly running to then determine OK on the input. Right, when the system the user gives it, you do a screen at that point as well, and if it says hey, no, this is uh a not an honest question it might screen it out at that point but not only is it looking at the user input, then when it spits the uh when it gives the uh response back out. It is also looking at its own response to see it's if it's meeting those constitutional classifiers because that is new data that is now being generated so that whole loop is super important for us to to capture. So we're showing you how our classifiers do this. You also can look at how you can build your own classifier systems using the evals that I provided earlier to build this for yourself. Thinks super powerful. All right, so one thing I wanna kind of lead with here as we get further into the bedrock story is the data privacy aspect, right? You are in control of your data, um, and we start with having zero customer data used to train our Amazon first party or third party models as your data, you own it, it's not used to train models. Second is your prompts and your completions are your own, right? These are not something that. Are shared or seen by Amazon or the providers um and third uh no customer data is exposed to humans or human in the loop operators for abuse detection. All that is completely automated behind the scenes. So to go a little further on this, as I mentioned, never shared um and not stored. Also, if you have region requirements where data needs to remain in region wherever you call those inference and prompts stay within region, meet your compliance and uh geographical region needs. um, all data in flight is always encrypted. If you saw the keynote again, you saw that's actually multiple layers of encryption. Um, we don't just, uh, from a defense of death perspective, we, we believe that, um, you know, that that encryption is, is important at multiple layers, um, and you can use your own KMS keys to, uh, to manage how that data is stored. Uh, we also expose everything's in API at AWS, right? Everything's an API, so you can use cloud trail, you can set up, uh, cloudwatch and, and, uh, metrics and alerts. Uh, you can, uh, observe through, uh, guard duty and security hub. I'll talk a little bit about that in a bit, uh, as well. And then, um, and, and for those that have specific compliance needs we are building that out. There's 20 already that have been uh uh completed uh you'll continue to see that number grow and we work backwards from our customers that tell us which ones of those programs are the most important to enable them to deliver. So I want to dive in a little bit into uh a typical bedrock data flow. Of course any any organization might build this uh slightly nuanced or differently or add additional complexity or or uh points into this, but if we just think about this as your own your own network, your own VPC, and then the bedrock service account, this kind of the. Starting point so you could uh hit that API endpoint, uh, which by the way we have uh 5 end points as well if that's something that uh you need for your business, uh, you can hit that directly from the internet or you could obviously come in through a private link that hits the API end point uh over like a direct connect or VPN or however you might get in through uh into your VPC. Um, and then that API end point, uh, uses the orchestration layer that's in that bedrock service account, uh, to then do, uh, the, uh, inference, um, on the model. That model actually is, uh, sent to Amazon through the model provider so they. A model staging account that model is then replicated where the model or where the model provider such as Anthropic doesn't have access into the service owned and managed and operated account um so that allows um that segregation to occur as well. And then finally all of these capabilities are built on top of our base uh uh core security service capabilities so we have identity and access management policies that allow you to control uh very specifically how to invoke models, uh, bedrock guard rails. I'll I'll give a demonstration of one of those in a moment along with all the other aspects I mentioned around uh cloud trail, etc. So when we think about uh inference specifically what, what does that look like? So there's a few things again I'm gonna reiterate this no logging customer data may sound repetitive. I just wanna make sure that we're hitting that point home. Your data is your data. Um, if you choose to log it and you choose to turn turn on prompt and output, uh, uh, logging, you can do that and a lot of customers do do that because they wanna have their own internal QA to see like how their, uh, how their applications are working, um, and tune that over time. Uh, there is no modification to deployed models. Those models are, are essentially static. Uh, no outbound connections from the host, so this is something that, uh, was a pretty popular question a few months ago as, um, additional, uh, third party models are available or you may host models, um, within bedrock, uh, there are no outbound connections, right? So the only thing that interfaces with that model is the API, um, and what you call into that API to pull back with the output. Um, and all inference is actually segregated, so in terms of, uh, being able, uh, to have any kind of, uh, inference, um, uh, kind of in, in a, uh, in, in the environment, it's, it's your, it's segregated to your specific calls. Uh, there's no caching of data, right? It's stateless in that sense. And um I wanna uh move on to the model evaluation piece so we talked a little bit earlier about some of the ways to think about uh doing model evaluation and some of the mechanisms that you can implement. And uh Bedrock has created a variety of tools and capabilities for you to do that. So we talked about when do you use programmatic, when do you use, uh, human, um, uh, LLM as a judge, uh, was something that we had, uh, launched previously preview is now, uh, uh, uh, GA, but these. Capabilities help you in a sense figure out how do you do model evaluations whether that's you wanna evaluate a different model provider whether it's you wanna uh see what the latest version uh is and then you can then correlate and compare those across those and and understand what's gonna be working best for your uh business and your use case. In addition, you're seeing these kind of evaluation strategies also apply not just at the model layer of the stack, but what does it mean when you're uh using uh retrieve augmented generation and and pulling in data um to augment your prompts uh or uh uh. Further in agent evaluation, um, there is a, uh, uh, AWS, uh, samples project out there that you can see in an open source project around agent evaluation and as agentic AI continues to accelerate and take off, you'll see, uh, a lot more in this space. So let's go through uh guard rails here. um, guardrails again is one of those things that the overlap and convergence of safety and security can be seen. um, several of these are purely safety, some of them are on the security side, but what we want to do is give you control and our customers control over what really happens as a shim uh against any given model that you may choose within Bedrock and beyond bedrock. So if you have self hosted models you can use it there as well. So let me talk a little bit about how this works. So you'll have a user input and as I mentioned, there's kind of two ways if you're invoking a model through bedrock, you can do that or if let's say you're hosting a model in SageMaker you might want to do uh an apply guardrail there or in your own self-hosted model you can still use guardrails. Um, the user input goes through, uh, that, that, uh, API call into one of the, uh, policies within the guardrails and you can see them here. I'll go through them in a minute. Then uh it uses those and it uh through the inference to say, OK, now I've got my input guard rail policy. What will I send to the foundation model? The response comes back, goes back into an output evaluation within guardrails and then if that guardrail also passes, the final response is then sent to the user. I mentioned earlier the enablement of several of our capabilities, so this is a good example of a condition key within identity access management where if someone is trying to invoke a model you can now enforce and ensure that the guard rail is in place if they if they try to do it without passing that guardrail ID um that call will be denied. So I talked a little bit about uh path one, which is if you're using Bedrock, the guardrail uh invocation is, is kind of directly uh integrated and, and, and easy to use and easy to call with the Invoke model. Um, however, if you choose to self-host or you are using an LLM, uh, of your choice, maybe in, um, uh, Sagemaker, for example, you can use the apply guardrail, uh, uh, API and essentially make sure that it'll it'll do the same rule processing both for, um, uh, prompts and, uh, responses. So I'm gonna go through these uh relatively uh quick pace. We have a lot of great content both in the documentation and other presentations that spend the entire time talking about getting bedrock guard rails and I want to cover a broader grounds, but um I'm gonna start with the content filters. These are kind of sliders for these categories that you can then evaluate like is the response both input or output, um, uh, hitting, uh, either of these kind of any of these kind of capabilities and then you can tune it up or down and as you can see here you can test these. And and when you test them you need to make sure that you're actually uh uh thinking about what are the use cases that your user is gonna be using and then also threat model right? threat model your applications, apply the threat model, understand how that might impact the way you build out your guard rails and test your guard rails. Uh, prompt, uh, attack detection is more on the security side of the equation right as we think about other traditional, uh, prompt injection attacks, equal injection, command line injection, same kind of thing here where we think about the prompt injection for LLMs, um, and this is kind of one of those, uh, based on threat intelligence based on what we learned this becomes smarter and smarter over time to help protect you. Uh, multimodal toxicity detection, uh, is how do we take text, how do we take images, how do we take, you know, a combination of the two, and then use those to determine what we filter in or out. So I am gonna show a quick demo on this one, where click it and let it play here, um, so we're gonna uh take a uh pick a model. I don't know, maybe maybe Claude sounds like a good one. I don't know what you think about that one we'll pick Claude and uh we're gonna upload an image. And if you look here I've got a couple. One is that's pretty violent, but maybe this one of some kids just scuffling on a playground. What what might this look like? So we're gonna do the testing as I mentioned earlier we're gonna evaluate what would a model do here and so we say, hey, can you describe this model? And uh it basically says no I can't really do that. So and now I wanna inspect. I want some explainability what's going on here, um, so it's telling me that um. The strength of this is high at a confidence level of medium or higher on that scale is going to block it because it looks like it fits the violence category. Word filters, uh, these are, uh, pretty, pretty basic, uh, in, in terms of like I want these words to not come in or out of my foundation model, so pretty prescriptive. Uh, denied topics are kind of cool. This is where I can give it a prompt or a context and I can say, uh, you know, don't have conversations, don't have outputs, uh, if it fits into the, the, the, uh, spirit of what I, I'm telling you to look for, um, so this is where kind of LLM's helping you solve, uh, the, the guard rail problem in and of itself. Um, and then PII filtering, uh, this one, very popular, very commonly implemented, especially in specific industries, health care, life sciences, for example, um, we might have PHI or PII data that you need to, uh, redact, um, and you can change this to either just detect it being used, uh, redact it or filter it entirely. Uh you can use the predefined, uh, filters or also uh customized there and then, um, now we want to talk a little bit about uh hallucinations because this is obviously something that people are very cognizant of how do we minimize mitigate, reduce the odds of, of hitting. Uh, hallucinations. So if we have contextual grounding checks, what contextual grounding checks do allow you to basically uh use uh rag and kind of verified uh answers to check the accuracy of the responses and then if those responses appear to be correct and relevant, um, uh, minimize the ability for that to potentially return a hallucinated answer. And there's uh you can change the grounding threshold for that um that allows you to kind of say hey what what does how does it validate whether that is a correct answer or not and then the relevance of how uh how closely aligned is that to to the returned response. You can, uh, just like firewall rules you can determine whether you want to block entirely or whether you just want to detect, uh, this way you can, uh, you know, before you formally implement a rule kind of test it out, see if you got it tuned right, uh, this is uh very helpful when you're building these things out or evaluating these over time. And you, as I mentioned, you can do it on inputs and or outputs from the model. So one thing I wanna uh uh show that's currently in preview that I think really is a game changer so we have the contextual grounding, but automated reasoning is a capability that we use uh for access analyzer network analyzer um it basically you could take a near infinite set of potential possibilities and then. Using mathematical proofs to find a binary output of 1 or 0 yes or no um and understand whether that outcome could potentially happen based on the way you're doing policies so our um AWS kind of leads the field in this uh in this area of of data science and they've applied this now to automated reasoning. And what this allows you to do is build a set of policies uh in variables and again with the foundation model and uh underneath the cover that actually helps you build those. I'm gonna give a uh demo here of what this looks like uh but before I get into the actual demo, let me show. Kind of the work flow, so let's say I had a policy and the one I'm gonna show you is an HR policy for leave of absence for employees and I wanna do automated reasoning checks on that. So I put it into the automated reasoning check I say consume this policy it's gonna output what it sees as variables and rules. It's then gonna feed that into the guard rails and we're gonna have a QA process to validate that. Are the variables and rules accurate? Are they complete? Then we're going to actually implement that as a guard rail and then as the next uh prompt comes in, it's going to apply what we built. And evaluate it back into and then feed that back into the the guard rail um if it's uh says yes that actually the the statement that is returned from the model is accurate it will uh give it a thumbs up. If not, it'll say no that's uh uh not accurate and uh and and uh deny it, um, and if it's accurate it'll continue to pass it back on to the user or the calling system. So here's the demo. And going into the automated reasoning screen here. I'm creating a policy Called leave of absence paid and you can see this is uh maybe the policy as written by HR and it gives a lot of details of what that policy looks like. And I'm gonna just like anything, create an intent here, create kind of an explanation of what am I trying to do, what is this guardrail ultimately trying to do and I want to explain that um if employees ask uh about eligibility for the program. Uh, make sure that they can understand, uh, should they, what, you know, what is their eligibility for this. So the only thing I've done in this whole demo is I, I cut out about 30 seconds of video there just to accelerate this, but that's as long as that that took to populate and you could see policies up up up up top. Rules up top, uh, followed by uh potential variables that it could extrapolate from there. So now I'm in the test playground and I wanna say, hey, what if the question is I'm a temporary contractor, am I eligible for a leave of absence pay and the let's say the model said yes, temporary employees are allowed to use uh uh LOAP. This is saying well that's not a a valid response because the extracted variables down there that I think aligned to the way and the intent of the question were were these specific ones and I can go in as I'm doing that QA aspect and say, OK, let's expect that the level of the employee will be. Explicitly mentioned uh in in the question um if not then that needs to be part of how the response is is formulated for it to be uh validated it'll version these so I can always go and see how am I modifying this over time, the life cycle of these policies um and variables and um. And uh so this one here uh variable saying if it's less than 20 hours a week that is not a full time employee and therefore would not be uh valid so now I'm gonna say hey I'm temporary um changed temporary to I'm a part time contractor um. And see uh if the answer stays the same. I'm explicitly defining that answer. The validation result comes back as invalid and then the description here it it kind of gives you the explanation of what's actually happening within this uh particular guard rail. I'm waiting for it to go to the next example of the demo here. This is just showing kind of how those variables are formed now I'm actually just um wanting to create the guardrail. So now that I've tested the guardrail, I created the policy, um, based on those rules and variables. I'm just configuring the guard rail. It's actually pretty uh straightforward to walk through the uh the GUI and the wizard here to uh create the version that I want this guard rail to run against. Implement the guardrail, uh, and click create the guardrail. Now give me a guardrail ID and that's the guard rail ID that I can then, uh, uh, reference and use from there forward. So I'm really excited about how that evolves and goes to GA over time. So uh I wanna just kind of cover here real quick, um, several of these controls in uh in bedrock guard rails, uh, span between deterministic and probabilistic, right? And this is something that. As we think about that nondeterministic way that uh models produce their output, uh, we're also thinking about how does that impact the way that we set these card rules up, set these rules up, and how can we actually use that to our benefits, um, where do we need to be explicit and deterministic? Where do we need to be probabilistic or where is it a hybrid of the two? But what we've seen by implementing these guard rails is over an 85% uh uh additional improvement beyond what you even get out of kind of base models, um, obviously folks like Anthropic are continuing to do uh a lot of work to make sure. Or that uh models eliminate hallucinations, um, eliminate harmful content, and that gets us a substantial amount of the way there. This is one of those defense in depth and layer control capabilities that helps add that additional layer and make sure it meets your specific organization's expectations and needs, um, and so we really encourage folks to be using, uh, bedrock to definitely, uh, learn how these guard rail capabilities are going to help you on the safety and security side. Right, so, uh, you can't go anywhere without, uh, probably this conference without thinking or talking or hearing about agentic AI and so we wanted to cover that topic a little bit. Um, you know, I think if you break it down into a very simplistic, uh, thought model here we're, we're thinking about how do you take, uh, these specific agents that have the ability to have memory leverage tools, uh, on you or uh a machine or work flows behalf and ultimately achieve these goals, but at the same time we need to have high degree of observation, high degree of control, high high degree of identity context that flows through these. And really understand that end to end environment. So just like we had the uh stack for uh uh uh how we think of Gen AI services, we also are thinking about how do we think about um the agentic AI stack as well um and this is at all layers whether you know you're building your own agents you want to deploy agents that are based in here or whether, uh, uh, you're consuming something like uh Q. Um, so, uh, MCP, uh, is a, is a, is a great, um, emergence that I think was created back in November or December time frame. Um, we're continuing to partner really closely with Anthropic and others to continue to build this out and enable, uh, customers to use this securely, um, but, uh, I think, uh, would you like to talk a little bit about what Anthropic's done about the MCP? Yeah, yeah, so, um, a little bit about how the concept of MCP works for those that might not be familiar with it. So you have kind of 3 layers in the stack when you're thinking about an a agentic AI. So you have the MCP client which is whatever application you're building, you might have your integrated development environment you have might have your business analytics tool. Now this new layer has kind of stood up in the middle that wasn't there before, which is the MCP servers. So Amazon. Provides ways for you to build your own MCP servers you can build them yourself. There's open source tools out there to do that, but each use case might have its own MCP server, and that MCP server then connects back to the sources of data that exist and so you might have a lot of use cases that these MCP servers might be serving. Now how do you protect them? These are still pretty new capabilities and so there are a lot of best practices that you wanna make sure that you follow when you're thinking about, uh, having your agents that are getting work done using MCP servers. So the first thing is connecting only to trusted servers a lot of companies. They're building them out there a lot of spam MCP servers are also popping up. So as you're thinking about which ones do we wanna connect to, look at the trusted environments to make sure that they're hosted by organizations as well as the underlying infrastructure for those servers is taken into account. The second is we've seen a lot of issues where you're you're generally these are all run via APIs that are talking to each other, right? So permissioning models are really, really, really important here we've seen incidents happen with this we've seen challenges by companies who just can't figure out how the APIs are interacting, so you have to be very, very careful on generating. The right API services and the permissioning models um when you're creating these servers and then test them very very well well like you don't want that service to connect and give data back that it's not using those evals that I talked about around um context and queries being separate those get very, very important here. And then be aware of prompt injection that can come in so MCP servers are servers like no other. They have libraries running on them. They have connections and network connections built in. So if you are looking at prompt injections into those MCP servers, someone might be able to say, OK. Now there's a 3rd layer that is built in before it even connects to the data store that is storing data there and so they're they're going to try to inject information in to see uh if it can use uh library changes or uh injection based attacks into there. And then the last thing is monitor the change in the tool behavior so if you connect new services to that MCP server if people's use cases are changing, monitor that to then make sure that it's only leading to intended outcomes. Um, and then last is build it to handle your scaling requirements. So MCP servers, as soon as they're built up by a lot of companies, hundreds and thousands of organizations are starting to connect to them. It's very hard to scale these if you're hosting them yourself, so looking at Amazon with auto scaling rules is really important. Um, as well as making sure that as they're scaling the permissioning models don't get messed up as well, so those are things very important to handle. So you can use this as a checklist when you're building an MCP server to make sure that all of these are taken into account. The first one is only about MCP servers that you're using from others. Right, uh, So I mentioned earlier that um that uh Anthropic was one of the first Alaed providers to achieve ISO 420001 and uh we were the first uh major cloud provider to also do that. I think the important thought process that when we think about compliance is working backwards from what. You expect of us to do for you and on your behalf and working closely with these organizations that drive these compliance um uh uh programs out and so again you can see a variety of the ones that we currently support uh and this continues to grow based on your feedback. The other announcement that we just had recently was having uh these two models uh approved for FedEAM high and DI DOD IL 45 approval in GovCloud. This, uh, was just announced last week. And beyond that, uh, we aren't just solving this just for our customers. We are also active contributors to many open source and open projects out there along with Anthropic. I know Anthropic sits on, uh, CoSI, for example, with us, which is the Coalition for Secure AI uh Frontier model uh Foundation, and obviously. The, um, most people I've heard, uh, uh, OA both are consuming and and contributing as long as well as uh agentic community which is spun up. I know there's an MCP, uh, project that, uh, spun up as well that we're a part of, so, uh, very important to make sure we're we're raising the tide for, for all boats, uh, as we used, uh, in our opening, um, title. So I did wanna talk a little bit about some use cases and uh these can all be kind of seen. Uh, I got the QR codes here, uh, all, all publicly available, um, out there, uh, but the, the London Stock Exchange Group has looked at how to use both Bedrock and Amazon Q to enable them to look at, uh, post, um, uh, post trade analysis and other, uh, opportunities to enable their their trading desks and analysts, um, and you can kind of see here the uh way that they have built that to primarily be through. Interface with Q that's ragged back in with their knowledge base and then having a bedrock that actually comes in and allows them to look at some of the additional analysis of how they're using their responses so they are kind of like in this multi-layer approach of how they're using Gen AI in their specific use case and we see other, uh, we see other organizations, especially financial services. I I came from financial services and. Uh, one of the things that is kind of interesting to see is they have such uh uh deep security infrastructure and plumbing. Uh, has kind of enabled them to accelerate when they've evaluated these and launched these into production over time so we continue to see insurance companies, uh, stock exchanges, critical other use cases around financial services and and others as well. I know these are a few of the anthropic use cases out there that you can check as well. All of these are also built on bedrock using anthropic model. Uh, so you can check those out and I threw a few fun ones in just as well. It doesn't always have to be the big giant serious use cases. They could just be kind of interesting, uh, use cases, interesting models out there. So check these out. Um, some of these folks are, are working on changing the world or just making life a little better for the average human, um, and, uh, really interested to see how they're using Gen AI to achieve those outcomes. So as we wrap up just some a couple of quick key takeaways, um. AWS and Anthropic are working really hard to keep you safe and it's our number one mission at AWS. It's our top priority. We, we think about it first, we think about it last and everything in between. We deliver comprehensive capabilities at all layers both on both sides of the shared responsibility model, so not only keeping you safe but enabling you to understand uh and uh what controls you can deploy and should deploy to uh implement safety and security. Uh, and again, make sure you understand that shared responsibility model. Make sure that, uh, you have the opportunity to look at some of the patterns that we publish. uh, if you have, uh, opportunity to reach out to, uh, an account team, if you have one, they are more than happy to bring folks in to help you dive deep into this topic more. We really appreciate your time today. Really hope you've had an excellent, uh, conference, um, on behalf of, uh, Shazeb and Anthropic and AWS. Thank you so much for being here.
