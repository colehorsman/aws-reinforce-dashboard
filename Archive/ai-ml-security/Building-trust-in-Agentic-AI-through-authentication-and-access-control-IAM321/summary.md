# AWS re:Inforce 2025 -Building trust in Agentic AI through authentication and access control (IAM321)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=UpWgVpSFj68)

## Video Information
- **Author:** AWS Events
- **Duration:** 20.4 minutes
- **Word Count:** 3,408 words
- **Publish Date:** 20250620
- **Video ID:** UpWgVpSFj68

## Summary
This session explores how to secure Agentic AI systems—AI applications powered by large language models (LLMs) using tools and plugins—through strong authentication, identity federation, and delegated authorization. AWS introduces patterns using the **Model Context Protocol (MCP)** to build secure multi-agent systems and shows how to integrate identity providers like Amazon Cognito, OAuth2 flows, and authorization layers such as Cedar and Amazon Verified Permissions.

A live use case of an AI booking assistant is used to illustrate how tokens flow between users, agents, tools (via MCP servers), and downstream services, emphasizing the difference between **delegation** and **impersonation**, and enforcing fine-grained access control across multiple invocation hops.

## Key Points
- **Agentic AI growth:** As GenAI applications evolve into agents orchestrating actions, securing identity and access control becomes essential.
- **MCP (Model Context Protocol):** 
  - Enables standard communication between agents and tools
  - Ensures secure delegation, tool registration, and token scoping
- **Security focus:**
  - Authenticate users and workloads (agents, tools)
  - Authorize access with policy engines like Cedar
  - Delegate access securely (avoid impersonation)
- **Token management patterns:**
  - Each "hop" (agent → tool → backend) should use its own token
  - Tokens must propagate context (user identity) without directly forwarding original user tokens
- **Amazon Cognito and OAuth2 flows:**
  - Used for identity federation and token issuance
  - Cognito supports both user and machine (workload) identities
- **Delegation vs Impersonation:**
  - *Delegation* = tool authenticates on its own and adds user context
  - *Impersonation* = tool reuses user token directly (discouraged)
- **Policy Enforcement with Cedar:**
  - Use Cedar policies and Amazon Verified Permissions to scope access across each service
- **Open Source Tools:**
  - AWS **Strands** SDK for agent development
  - GitHub examples for MCP server implementations
- **Tool Support:**
  - Amazon Q CLI and Amazon Q Developer IDE now support MCP tool enrichment

## Technical Details
- **Architecture components:**
  - **User**: Authenticated via Cognito or other IDP
  - **Agent**: Holds MCP client, manages orchestration and tool calls
  - **LLM**: Provides reasoning and instruction
  - **MCP Server**: Acts as tool proxy/API gateway, performs policy checks
  - **Backend Service**: Final resource API (e.g., travel booking API)
- **Token flow pattern:**
  1. User signs in (OIDC/OAuth2), agent receives access token
  2. Agent interacts with LLM using user context
  3. Agent calls tool via MCP server, passing its own scoped token
  4. Tool (e.g., travel agent) authenticates independently and includes user metadata via token exchange
  5. Backend service uses token claims (agent + user) to authorize access
- **Security best practices:**
  - Never reuse upstream tokens across different hops
  - Use MCP servers to enforce defense-in-depth
  - Configure authorization policies at each tool hop using Cedar or custom policy engines
  - Customize token claims using pre-token generation hooks (e.g., for delegation)

## Full Transcript

Welcome everyone. So we are here today for the last day of reinforce to be able to talk about identi identity. So I hope that you are thrilled. We have a lot of good content for you. There will be more content during the day that can allow you to deep dive into that, but let's start maybe with this very small and happy talk about that. My name is Jeff Lombardo. I'm a security specialist. I'm based in Montreal, Canada, and with me today is Fay. Hello, hello everyone. This is Fay. My name, so I'm a principal engineer. I'm a principal engineer in AWS on the service team. I own the design and implementation of Amazon Car needle, which identity provider service. Thank you. So the first thing that we want to do today is to come back a little bit on to what happened over the last year and what is in front of us for agent TAI and generative AI application because it's going very fast. In 2023, what you have done is that you might have started with some POC in order to try to assess what the technology was capable of providing to you. This year in 2024 to 2025, you are currently transforming those POCs into projects to try to get more value out of your data to maybe improve the efficiency of both your workforce users and your consumers into the way that they can use the services that you are providing. An organization or the services that you are providing to your consumers, but the main objective is to continue into this increasing optimization of the loop in order to provide even more value to them. What you want is to reach a stage where you will be able to pioneer new ideas and new capabilities into your. System because your end users will be able to express their needs through that. So at AWS, what we have done is that we are providing you with a large set of tools that can allow you to create your own GI application so you can use like Amazon Bedrock, you can use those guardrails, you can use. There are plenty of elements like the last open source SDK strands that we have released for creating agent TKI component. But into bedrock we want also to give you the choice about the large language models that you want to use because you might want to use something general which has been trained onto a large amount of data or you might want to use some more specific large language model, especially if you want to tackle the multimodality of interaction in between your user and your GI application and therefore you can use like models from. Amazon but also from our partners like for example Eroic into this ecosystem. In some cases because we are doing that for a very long time and we have a very strong expertise into the compute, we are capable of providing to you the ability to create your own model to train them, to fine tune them to match your exact needs in the manner. For sure, security is our top priority. So the ability to be sure that you are controlling the data perimeter that you are like putting guard rails around those large language models that you can maybe access those large language models privately within your own VPC and your own AWS ecosystem is very important for you and we want to be able to support you into that. But we also know that the the the generative AI. ecosystem is moving very fast and we saw a lot of open source solutions getting out so we want also to be able to provide to you the interoperability in between those open source solutions and what you will build on AWS. So if we look at a standard GI environment for our customers, we can see that when we are decomposing a GI application, we can find the same amount of components within it. We need a user that will access our. application if it was like a traditional application and this user will need to be authenticated. So most of the time there will be an authorization server or identity provider that will be in charge of authenticating those users. Now that the generative AI application knows about the user, the user will be able to provide his input, so his question to the generative AI or is ask of action, and the agents that will represent the orchestration. Within the application we will be able to enrich the information, the input that the user set with some information from the organization that will have been presented in the form of vectors through like a vectorized database. This is what we call a rag. The composition of both the user input and the enrichment that we are providing on top of that will then be provided to the LLM for the LLM to be able to think about it and maybe think about the next steps or maybe the final response to the user input for that you will use effectively all the models that I have talked about in the previous slide. The response from the LLM will allow you maybe to take some action against your external system like for example triggering a communication through an email system through communication and notification, all those kind of stuff. But what we have seen when we talk to customers is that there is a lot of untapped data. It means that we have, you have a lot of data silos at your organization that can concern the way your organization operates, the metrics that you got over there, or maybe some consumer information, or in some cases from the developer with some local information that they might have onto their own workstation. And oh sorry wrong side and what you want is that you want the ability to tap into those different new data silos. So that's why we have seen the apparition of AI tool. AI tools are here to ease the integration of all those data silos into your GI application just in time when the LLM needs it. So for that what will happen is that those AI tools will be registered at the LLM for the LLM to be able to understand that he has those external capabilities to get more information. What we want to do over here is to be sure that you can create some additional targeted deterministic logic into your LLM and that you can dynamically search for information that can exist externally. Like for example, if you want your developers to be able to get quickly information about the states of their development. You can have an AI tool that will allow the GI application to talk to the GitHub service, look at a specific GitHub report, and maybe get information out of this GitHub report onto the last commit, um, into that. So what what we are doing what we have presented into the code talk is that for example we can have a customer that will create a generative AI um application for um travel agency for booking travel so the user will authenticate he will he will um he will pass, he will pass the. The the token to the agent because the agent know about the different tools that are at the disposal of the LLM, the user when he will say book me a trip to Honolulu if there is no weather alert. What will happen is that the LLM will think about that. Now he has a weather tool that can allow to get like weather reports and know that he can talk to some travel agency to get travel options and to maybe book a plane ticket to move forward and those tools will be able to enforce some access. Control to be sure that for example they can use your loyalty profile into those different travel air travel company in order to book the ticket and will handle the communication with the back end service for those different actions to be able to operate. Mo So what we want and to return the response to the user, so all those elements will allow you to do like authorization in death by by having a scope that will be like build along the way while you are going through all those different elements you will be able to like really control what is happening into your generative AI application. So for that we have a new specification which is the specification. The model context protocol. This specification allow us to standardize how effectively the AI tool can present, expose which capabilities they are capable of, and also work into the whole delegation of authorization process that I have just described using the 02 delegation of authorization, token exchange, and also capabilities like dynamic client registration. But now I want to pass the end toy for Fey to explain to us how both of those elements of the mental model I just described and the MCP specification can be applied on AWS. Thank you, Jeff. So I got a lot of questions in the last several days from from customers and asking about MCP model context protocol. You probably have heard about it uh that's something that Anthropic, um, has been working on and that's a way to make agents more powerful in making all the tools available for agents to use. So then one of the main questions that I heard is that how do you, I want to be able to use MCP so customer wanna use it. On the other hand, they're also afraid of using it. So there's an in and out kind of relation on how do I use it but without getting trapped or without getting a security problem with it. So I'm going to, I'm gonna go through the, the slide that Jeff showed you a little bit earlier on a sample scenario on how this works, how you will build the agent application, how do you secure that. Example here that that I'm using will be, we'll be using Amazon Carnito as identity provider. For those of you who don't know, it's a, it's identity provider authorization server for users to sign up, sign in. Using different ways, password, uh, emails, um, multi-factor authentication, pass key, and etc. It also supports workload, workload authentication, which is one of the reasons why this is a good piece in the, in the puzzle. But the same diagram here that we're showing, you can also swap out with any identity provider. So we are showing you more of a pattern on how you build and how you secure an AI agent. Look at the diagram here, there are a couple components. So when you think about AI agents, there's a user, the user has an identity provider, and then you have an agent. agent here usually runs an MCP client that's um as part of SDK, part of the MCP uh protocol, and you have a large language model which does the pretty much gives you the brain and gives you instruction on what the agents should do. And then you have the weather to travel agency tool. Here is the MCP server, which is, you can look at as an API gateway and then you have the service behind which your resources. So think about this as a, as a chain of four parties acting together. It's a chain of invocation, don't think about it just one. And that's the reason as I will show you over this over the steps, there's a different hubs require different ways to authenticate and authorize. All right. So the first step is that Alice will, when Alice wants to access an agent, want to use an agent, let's say you build a chatout agent or in this case is a booking agent. The agent will ask Alice that please sign in because agent needs to know who the user is. So this is, this is a typical open ID or OR um type of um sequence one agent will redirect Alice back to an IDP in this case, can need to sign in. And then Alice will give, will ask Carnido to give an access token for agent to access the weather tools. Right? Because in this case, agent is the client, the service that agent wants to access those MCP server, those are the next hub which whether tools. So just pay attention there that we're not saying that agents should access the resource directly, it's going through MCP server. Then the next step is that now agent has this access token, it knows about Alice. So you can use this information. To interact with the language model, maybe pull information about or just a regular chat, and in this case, it has information about Alice and then just talk to the language model, that's the number 2. And then the third step is that now agent. Because the agent has the access token, it now can talk to the MCP server in this case, a weather tool or travel agency tool, and that you will use this the token to access those. All right, the next step is that you have the weather to that sits in front of the real weather service. And you can think about how would I, now I have the identity, right? I know the identity that agent is calling me, Alice is calling me, agent using Alice uh as a token, so the token will have agent information, have Alex information. But then you can do some authorization there. So there's a two patterns of how you do authorization. One is that if your resource servers already can authorize based on this type of pattern, for example, taking a token, doing scoping down, then your resource server can do it. But if you're a resource server on the right side, um, does not have the ability to take in. Um, a user or identity, uh user agent's information, then you put your authorization server or authorization rules in the in the MCP server. So think about as a defense in depth, you have multiple hops that where you can scope down the access. And in this case, we're showing Cedar. Cedar is a language, a policy language where customers can write. In a, in a, in a more predefined way to say that this type, this user, this agent can access this type of resources you, you write that in a, in a real way and then CA will run the evaluation for you. And Amazon has a verified permission as a managed service that runs Cedar for, for this purpose. All right, so the next step, this is getting interesting. When When an agent wants to access the travel agency tool, and then the travel agency tool needs to access the back end. In this case, you have a hob, right? You have an agent tool and then to to the real service. We should not be sending the token that agent uses to access the MCB server in this case is the API gateway or think about as a as a proxy. We should not be sending that token directly to asking the travel agent to use that to the back end, right? Because it's a different audience, it's different, it's a different audience for the for the tool to use. So what you will need to do is that then the travel agency to will do its own authentication because it's an agent of itself. That agency to will go to Carnita or go to somewhere else, some other IDP it's up to you. It will run the machine to machine authentication so that it proves who it is. At the same time, use the information that you got from Alice, which is a user token, use that information as a metadata or as as additional metadata to put that into the claim of the machine token that you get back. So that's a key step. We kind of talked about maybe in other talks I talked about delegation versus impersonation. In this case, we're doing delegation because the travel agent 2 is acting on its own. It's doing its own authentication and adding the user information. So that's delegation. If travel agency 2 is sending the Alice token directly, which is you shouldn't be doing that, then that's an impersonation, right? So in this case, it does that and then it's able to use this newer token. So pay attention that the steps 6 and the 3, they are different tokens. The number 3 is for agent to access whether the tool with Alice information. The token used for 6 is a travel agent to does its own authentication, plus the information from Alice, and then talk to the back end service. And in this case, number 7, that's 77 will execute that, the information, uh, run the, run the authorization. So I was talking about the CA or Amazon verified permission as a way to authorize. Now you have the information about who is the who is the agent, who is the travel agency tool, which one is accessing it, who is the original user, you have those information, and then the travel agent tool can get this information and properly authorize the request. So I was talking about One of the most important or maybe the the starting point of secure securing your agent access is think about the resource servers or the resource that agent wants to access. You want to make sure that your information about agent and user get all the way propagated to your resource server so that your agency tool or your airplane API can use this information to scope down and that's the key of this. OK, so then here's a diagram uh of different services of different steps that you can use. Probably you can either take a picture, I will not go into detail on that. So That's the summarization of everything that Fay just presented to you in one slide. Yeah, it's a lightning car, so I try to keep it as fast. All right. We, we kind of jam a lot of information. All right, uh, so delegation impersonation I already talked about it. Take a look at the, the client metadata. There's a pre-tokken generation where you can customize the claim on the machine to machine authentication that will enable you to do this kind of on behalf of work. OK, I'll pass it by to Jeff. Thank you. So as you have seen, that's how you are creating Agenti identity on AWS to do that, and that's what the code talk we got. You can use like the guidance for starting MCP implementation on AWS. We also have a GitHub repository that concentrates all the MCP servers that have been created by solution architects for AWS technology. You can look at it. We are keeping that up to date. I just want also, like I said initially, I like the fact that we have released the open source SDK for creating AI agents. This is called AWS strands that can allow you to ease the way that you are developing those AI agents. For sure at Reinvent we will have more information and more details about how you can use that. Finally, I want to say that Amazon Q CLI and Amazon Q developer into your IDE can now support MCP server configurations. This is exactly what we demonstrated into our code talk, OK, so you can enrich those two tools with extra data. Like your data silos that we talked about earlier. Finally, I just want to highlight that there are still some sessions on the topic happening today. I see my friend over here, Edwardson that will run a builder session this afternoon. So if you want to get your hands dirty onto that, please join their session. Thank you for that. If you have any questions, we will be on the side to be able to answer to that. Have a good day everyone.
