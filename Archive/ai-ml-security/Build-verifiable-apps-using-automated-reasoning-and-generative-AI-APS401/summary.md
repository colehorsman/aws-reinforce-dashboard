# AWS re:Inforce 2025 - Build verifiable apps using automated reasoning and generative AI (APS401)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=9x5GKVPbl_E)

## Video Information
- **Author:** AWS Events
- **Duration:** 42.3 minutes
- **Word Count:** 6,395 words
- **Publish Date:** 20250619
- **Video ID:** 9x5GKVPbl_E

## Summary
This session explores how AWS combines automated reasoning (formal verification) with generative AI to build more reliable and secure applications. The presentation demonstrates two key implementations: Q Chat for AWS resources, which uses automated reasoning to validate API calls before execution, and Automated Reasoning Checks in Amazon Bedrock Guardrails, which validates LLM outputs against domain knowledge. The speakers explain how automated reasoning provides mathematical guarantees and guardrails for LLM-generated code, while generative AI provides development velocity. This neurosymbolic approach creates a feedback loop where AI generates code and automated reasoning validates it, ensuring correctness and security without sacrificing speed.

## Key Points
- **Neurosymbolic AI Approach**: Combines generative AI (probabilistic) with automated reasoning (deterministic) to get both development velocity and correctness guarantees
- **Q Chat for AWS Resources**: Natural language interface that makes validated API calls to answer questions about AWS account resources, with automated reasoning checking API call validity
- **Automated Reasoning Guardrails**: Logic-based validation system that checks LLM outputs against formal policy models to prevent hallucinations and factual errors
- **AWS API Complexity**: Over 200 services with 16,000+ API operations, each with complex parameter constraints and feature combinations requiring expertise
- **Semantic Logic Models**: Auto-generated constraint systems that define correct API usage patterns for all AWS services
- **Iterative Validation Loop**: Gen AI generates code, automated reasoning validates and suggests fixes, process repeats until convergence
- **Mathematical Proofs**: Automated reasoning provides machine-checked proofs of correctness or counter-examples showing bugs
- **Industry-First Capability**: Bedrock Guardrails' automated reasoning checks are first-of-kind for preventing factual errors in production AI systems

## Technical Details
- **Formal Verification Engine**: Uses satisfiability (SAT) solvers and automated theorem provers to mechanically verify code properties
- **Logic Constraint Translation**: Converts both program code and specifications into mathematical logic formulas for automated analysis
- **API Validation Service**: Custom automated reasoning service that validates AWS API calls against semantic logic models before execution
- **Policy Creation Pipeline**: Takes source documents, extracts concepts/rules/variables, creates logical policy models that can be refined by subject matter experts
- **Bedrock Guardrails Integration**: Automated reasoning checks work as configurable safeguards across different models and platforms
- **Use Case Suitability**: Works best for deterministic, factual claims and procedures (policies, regulations, workflows) rather than creative or probabilistic tasks
- **Constraint System Architecture**: Translates if-then-else program logic and security properties into logical formulas with AND/OR/NOT operations
- **Security Applications**: Threat detection rule verification, compliance requirement validation, regulatory framework adherence with auditable mathematical proofs

## Full Transcript

So let me start by motivating why we want to use these two technologies in complementary ways, um, in the context of this presentation. Gen AI brings two key benefits. The first is, of course, being able to automatically generate code. Um, having been trained on large amounts of code and infrastructure as code, LLMs, um, can produce code for tasks that they haven't seen before quickly. Um, the other capability of course is to power part of your application with an LLM instead of an algorithm or implementation code you yourself might write. So these two together of course bring development velocity, but there's of course the well known risk of hallucinations. Due to these hallucinations, there's less than full assurance um that your applications will be correct or secure. Furthermore, the code or answers produced, um, the, the, the process by which they are arrived is often opaque to, to the end user. Um, to address these risks, we bring in our expertise on automated reasoning. Now first, let me disambiguate this use of the word reasoning, automated reasoning. In the machine learning world, um, there are quite a few techniques with the word reasoning in them. Those are not what I'm talking about. I'm talking about the field, um, also known as formal verification that has been around for, um, many decades that has been used extensively for hardware and software, um, functionality and performance verification. Um, these tools do mechanically produced and mechanically checked proof of properties, um, correctness and security properties on your application code or infrastructure as code. The input to these tools. Um, is your code or infrastructure as code as well as um correctness or security properties um that you want to be true an example loops. An example might be, for instance, that your application never reveals personally identifying customer information to the end user. The output of these mechanical verification um tools is if your system is indeed correct or secure a machine checked machine produced, and auditable proof that um these these properties universally hold for all user interactions, no matter what path your code goes down, no matter how it behaves, no matter what exceptions might be thrown. They establish mathematically that personal personally identifying information is not revealed. Or as interestingly, if your system does have a bug or vulnerability, they produce a witness of this fact, referred to as a bug or error trace. They take you down the path in your code through through which this bug occurs, and they give you the inputs that trigger this. Um, now, let's take a brief look at the inside of these to just give you intuition and to highlight how they differ from machine learning based tools. Um, not that the insides of these tools, um, as end users, one doesn't have to understand, but I want to highlight how these have logic and proof and implications inside and not statistical inference. So, um, the, at the core of these tools is a set of constraints, a set of equations, logic equations using which you model your system, your software. Just like uh with a mechanical system, you would have a number of differential equations that describe how it behaves over time. The math that models how programs behave, how they are secure or correct, is logic, mathematical logic, but this isn't as um complicated or tricky as it might sound. Let's take a look at a simple example. Let's take a look at how the front ends of these automated reasoning tools that we can also call automated logic compilers, compilers from code to logic representations might work. So in in this. Simple example, um, there's an if then else statement variables X, Y, and Z, and if you go down the if branch, something happens. If you go down the L branch, something else happens. Um, a compiler that understands the semantics of the pro uh particular programming language you're using. Convert this to a formula, a logic um formula consisting of consisting of and and ors and implications and not just relating how the program variables change as your program executes. Similarly, the property, the security or correctness property you're after is also translated to a logical statement in terms of your program variables and then. We put um your specification and your program description as logic together. Now it becomes a constraint system and your question is, is this set of constraints together, is it satisfied or is it not satisfied? To answer this, instead of using a machine learning tool we use um the key distinguishing technology of our field. These are called automated provers or auto automated satisfiability solvers. um, these, given a set of constraints can tell you together they're satisfied, they're true or they're not satisfied, and here's why. These are the result of, again, decades of research in my field of automated reasoning. Um, they have their, um, automated solvers have their own open source community, yearly competitions, just like there are competitions in other fields. Um, this field also. Um, is its own community that has made significant advances over over the years. Excuse me, so powered by these powerful provers, logic solvers, we get the following capabilities from automated reasoning tools. Giving your code and your spec, they detect all SEC violations in a sound manner. Um, in that way, they provide guardrails for human written as well as LLM written code or infrastructure as code. In in case um your program violates the spec you're after, um, in limited local cases, um, these tools can also do automated synthesis. So instead of telling an LLM, a verbal statement of here's what's wrong and here is what the program was, try again, these tools can automatically synthesize local changes to your code or infrastructure as code to to fix that vulnerability. And once you've put in the mitigations, then you get proof of correctness for the end result. Um, another distinguishing, I'm sorry, capability is that these tools are deterministic. Um, deterministic and transparent, given the same inputs, same specs, they give you the same result, um, and they also produce, uh, an account of how they produce that result that is independently auditable. Furthermore, if um these results depend on certain assumptions, for instance, if they rely on the correctness of a library. Um, these are also made explicit, um, again differently from, uh, machine learning methods, but of course, uh, what's lacking is generation capabilities starting from a natural language statement of intent to and software, and this is why we use the two together in a feedback loop to produce the results we're after. Um, in the field of automated reasoning, Amazon is a world leader. Um, we have the by far the largest, um, team of scientists who work in this field, and we have been developing and applying ourselves on our services for these tools, um, for, um, almost a decade now, maybe even a little longer, for instance, uh, when S3 launched strong consistency. Automated reasoning tools we developed were used both by scientists and directly by development teams. Similarly, we um received the Distinguished paper award at a high profile conference um about how our IM policy evaluation code is formally verified, and this is run on a API called to AWS. To provide the benefits of this technology to our users AWS customers, we've also powered several security products, um, with this technology. Examples are, um, as you may have heard at this conference or or or reinvent, I am Access Analyzer or S3 block public access. Um, the, the two tools we're going to showcase today, um, have a different, somewhat different flavor from the ones you saw on the previous slide. This new generation of tools, um, like I alluded to earlier, make, um, joint integrated use of machine learning for development velocity and automated reasoning to provide guardrails for um hallucinations that might have um correctness and security implications. I'm sorry, I don't know why this is fighting me. Um, now we're going to do two deep dives. Um, the first one that I'm going to do is, um, about a feature called Q chat about your AWS resources. And as you might guess from the name, um, what you get is a chat window where in natural language you ask questions about resources on your AWS account and you get, um, in natural language and and HTML with deep links, deep links, um, answer to your question. And as you might imagine to carry out this task, I apologize I don't know why these slides keep self advancing um. And to to carry out this task, of course, um these tools on the inside make API calls to your AWS account with your permission, of course. So let's take a look at what um a few examples of what you might do with this um capability and why automated reasoning, specifically about the correctness of API calls is, is critical for the correctness and security of this feature. OK All right, so Um, you go to queue and you in the chat window type, for instance. Um If you can't read it, it says, do I have any load balancers open to the public? So Q first does some introspection on your account. And makes a few API calls. And Um, I hope you can read it, but, um, in case you can't, um, the result says your application load balancer. With the name I apologize. This, this is what advancing for. Reasons that didn't happen in rehearsal. OK, I think I'm going to have to talk through the demo. Um, so, um, you ask, um, do I have any, um, load balancers that that are open to the public and Q. Us LLNs which have tools that can make API calls to your account to go figure out that in this case you have an Application Lord balancer that is actually um open to the world. It's ingress rules allow access from any IP address. And then it will give you, in case this is something you want to remedy, instructions for how to do that. OK. So this is one thing you might want to do, to look at your resources and if they have any potential security vulnerabilities that, that you want to check, such as being open to the outside world. Um, And other question you might ask is, um, suppose you have a large number of EC2 instances and you're wondering, um, have any of these been running for a long time? OK, um, you have a large number of EC2 instances, uh, they've been running for a long time, and you're wondering, um, I wonder if any of these require patching. So you say, um, tell me about my oldest running EC2 instance um how long has it been running um what versions of operating systems and libraries it it's running and again Q um first does some introspection on your account, collects what resources you might have, makes some API calls AWS service API calls. Um, and collects the results and presents them to you. It tells you your oldest running instance, uh, when it was stopped, and then you wonder, hey, uh, does it require any patching? So you ask Q and then Q goes and does some more introspection and realizes that well it can't produce the answer to you. Because you haven't enabled um systems manager for this EC2 instance and your EC2 instances stop, so you need to restart it, enable systems manager, and then it can assess the state of um patching for you. And if it does need patching, then you can go ahead and um ask you how to do that. This time using more of its LLM capability, less of its automated reasoning capability, it produces the results for you. Um, another similar thing you might want to do is to ask, uh, do I have any lambdas running outdated versions of Node.js? And again, um, Q will do introspection, make API calls, collect all of your lambdas for each of them again, make an API call to see what version of node it's running, um, and if it, if you, um, it will give a recommendation of, uh, whether it looks outdated and whether you should update it and we'll tell you to do that. Um, so, um, You can ask security related resource related questions and um underneath um in addition to LLM powered tools there are API calls um to internal and external APIs uh to services and um uh and and other, uh, bits of functionality internal to Amazon. There are API calls being made. On your behalf, um, so in that case, uh, you might, uh, want to, well, you would want to make sure before such an API call is run, is this valid, um, security wise, is this cleared um by this product to run on my account? So if you look at this one example, um, this is for illustration purposes only. I picked a particularly simple one. You're asking what is the intelligent tiering configuration for my S3 bucket called SARA test. To do this, the Gen AI tool first produces an API command. That will answer this question and for illustration purposes, not because what's that's what's going on in the tool I'm showing it with a with a CLI call um it's calling the S3 API the get bucket intelligent tiering configuration command. Um, so before we run it, to avoid latency, to avoid potential security consequences, to avoid cost, our automated reasoning service, special purpose built, uh, by my team and the Q team in collaboration. Uh, this service gets a call, um, from Q asking, is this a valid, is this a secure API call? In this simple example, the automated reasoning service says, well, no, actually it's missing a required parameter. Called ID. Intelligent tiering configurations have names uh for different configurations and um in this command, you have to ask for it by name. So, the tool will say, no, it's missing a required parameter, uh, but it won't just act as a simple guardrail, it will say, oh, by the way, here's a suggested revision, and it will fill in the missing required parameter for you and the well-formed value for it. Um, given this, uh, the Gen AI, uh, part of the tool has now more information to attempt the, the right API call that will give you the right answer. And in fact, the, the automated reasoning suggested synthesized command might very well be the right command in which uh further LLM iteration cycles are avoided. Um, now, um, missing required parameter doesn't sound like a big deal, and you might ask, well, is this really something that deserves automated reasoning and the reinforced talk? And the answer is no. It was a simple motivating example. But let me, um, most of you are probably already convinced of this, but let me try to motivate why. Um, calling APIs, calling our services and APIs and using them correctly is something that actually requires quite a bit of expertise and, and, and some science to support it. Uh, we have over 200 services, over 16,000 different API operations. Some of these have dozens of features, um, each described by parameters and their values, and these parameters have constraints on them like ranges, lengths, regular expressions, formats, um, so to, to even call a single command properly, um, requires some familiarity with, with the command. So for instance in this, uh, described call to EC2. Um, the start time needs to be a date of a certain format, a standard. I think ISO 8601 is is the standard, and the fleet request ID um needs to follow this particular regular expression. Um, so yes, it's a little nitty gritty, um, but doable. It doesn't really require, um, very advanced technology, but this is just scratching the surface, the, the trickier. Part the part that requires um deeper experience, more semantic knowledge is um how our APIs um provide a rich set of features and feature combinations which as you well know you can orchestrate to build um really complex artifacts. So again for illustration purposes, let's look at this 11 API command where um you're uploading an object to S3 and you want to encrypt it on the server side. Uh, you have several options. One is using a KMS key of your choice, uh, in which case you say I'd like to use KMS encryption, and, um, here's my key ID. So if you're using a KMS encryption, you have to fill in these two parameters, or you might say, well, no, I'd like to use my own encryption key, in which case you have to fill in these 3 parameters. With uh the second one is your key and the third one is it's MD5 hash. So two parameters need to match in this particular way for this API call to succeed. And you have to fill in these 2 or 3, but you can't fill all 5 of them because Encrypting with a selected KMS key plus a key you support on the S3 side in a single API call isn't supported. If this is what you needed for security, then you have to achieve this by making multiple API calls possibly um to different services so um. Again, this is uh one selected example um but it highlights a point that um these services have advanced features that you combine in ways that require expertise and this expertise is um something that we want to help humans and LLMs with to make programming more convenient but more correct and secure at the same time. So what can we do to help this situation? Well, at AWS, uh, we are the ones who are building these services, so we know what features there are, what requirements there are, and what uh correct uh API call, well formed API call needs to be like. So what we did was we auto generated uh what we call semantic logic models. Um, now, note, um, I'm using the word model, but this is not in the, um, in the machine learning sense of the word. I mean a set of logic constraints that tell you exactly how to call, uh, an API correctly. So we have this for all of our services and all of our operations. So when Q is thinking, uh, maybe I'll make this API call and ask our automated reasoning tool, is this well formed? Um, should I let this command go through? We first pull the model, the logic model, the set of constraints that apply to that operation. We also um take the um the candidate command and translate it into logic. uh remember the automated reasoning reasoning tool picture where the spec is converted to logic and then the code is converted to logic. So that's this left one and this right one. Um, and then we give those two to our automated reasoning solvers and say does this command satisfy this set of constraints and it generates a sound deterministic correct um proof that it does or um disproof that it doesn't. And in that second case, Um, as I showed before, it produces also a suggested revision. It says your command wasn't well formed, but here's one that's close to it that is I filled in the missing parameter for you and put in a well formed value, and using introspection you could replace that with something from the user's account. So, um What I showed you was one pass, um, in fact, a guard rail plus one suggestion where the Gen AI portion produces a candidate bit of software, the automated reasoning part analyzes it and returns the analysis results and the revision suggestion. Uh, what we do is we don't stop there and in a way, um, that's a particular kind of neurosymbolic approach we iterate. Given the analysis result and the revision suggestion, the Gen AI powered components of the tool now produce a revised second attempt at at that fragment of code or infrastructure as code, and the automated reasoning, analysis and synthesis engine analyzes it, returns the results and then revision and by iterating in this way until convergence. Um, what we end up getting is, um, final code generated by, by this overall tool, um, that is guaranteed to satisfy the constraints, um, that you imposed on it. Um, so this is the way in, in the queue chat about your resources product that we've combined um Gen AI and automated reasoning in a way that gives you agility, speed, but also correctness, guardrails, and actually faster automated synthesis than. Uh, more iterated uses of the LLMs, um, now I hand it over to my colleague Wale to describe, uh, the second highlight where, um, these two technologies are combined in, in, in another innovative way. Hi everyone. Uh, my name is Wale, and I'm a scientist with AWS and my colleague already talked about the synergy that we're saying between machine learning, which is actually um rooted in probabilistic AI and connectionist AI, with the symbolic part which is automated reasoning to actually come together to form a neurosymbolic AI. And in in December at reinvent, we released automated reasoning checks in Amazon Bedrock Aris, which is in gated preview. And we all know that automated reasoning is a specialized branch in computer science from what my colleague said, where you're using mathematical proof techniques and building formal logical deduction to verify system behaviors with. absolute certainty, and I will talk about how automated reasoning checks can actually help you improve factual accuracy for your LLM systems. And so if you have any GI systems, um, automated reasoning using this logic based algorithms and mathematical validation, you can use automated reasoning checks to validate your LLM outputs against um domain knowledge. So, um, historically, when you're trying to use automated reasoning for your systems, what you do is you get a bunch of logicians and you take maybe a document and information, a code and you and they go through, go through the document and actually write the logical representation of that. One of the things that we've seen is that LLM has actually changed the game because you can then write logical models for pennies on the dollar, so much more faster and agreeing on truth. Is a UX problem and when I say agreeing on truth is a UX problem, the truth here is the logical representation of the policy document that you are using. And the automated reasoning systems actually uses two complementary approaches other lines that actually handles the natural language understanding from the document. So logical representation and then a symbolic reasoning engine that actually performs the mathematical validation and having this hybrid architecture actually allows you to be able to impute policy in plain language and you can actually use that to maintain rigorous verification. So, how does it work? For automated reasoning, um, the, the first thing you have is you have your source document and then you pass it through automated reasoning and you extract concepts, rules and variables from this document. And then you use that to create policy. After that, you can also then have a subject matter expert to come and test and refine the policy. And so if there are rules that they agree with, if there are rules that they don't agree with, or if they are the ones that they agree with it partially, but they want to refine it, you can actually do that. On the other hand, let's say you're building a generative AI system where what you then have here is a prompt, which is an input and you can have a GI model and you can even replace the GII model with a GI system. So it could be maybe a rack system, some, a genetic framework, and then you generate the content. So you have some sort of inference from it. And so what you can then use it to do is that you can take the. the output with automated reasoning checks, so check for the content validity. And so in these cases they tell you is this this statement is it valid under the policy that you've created or is it invalid or is it satisfiable? Satisfiable is a case where you have valid with some assumptions and so it checks that it provides suggestions and you also have some explainability that is attached to it. And after that, you say, what, what do I want to show to my users? Do I want to take this response and say if it's invalid, don't show them. If it's valid, show them. Or do I want to take the feedback, for example, to ask the generative AI to then rewrite the response and write a corrected response in this feedback approach. The other thing you can say is you can take this and you can actually annotate your answers um with it. And lastly, you can say, OK, let me use this to ask clarifying questions. Automated reasoning checks is actually part of bedrock guard rails and in bedrock guard rails, um we know that it provides configurable safeguards that you can actually use to build safer generative air applications at scale. And so these guard rails, um they work with sort of different models, fine-tuned models. And even models that are hosted outside Bedrock. And so with guide rules you have a configurable safeguard on your gen application and it actually has no stick to where which model or which platform that you are using. Um, and as part of guide rules we have the different filters. So we have the content filters, we have the denied topics, we have the word filters, the sensitive information filter. And also the contextual grounding that you can use. And finally we have these automated reasoning checks, which is an industry first capability that helps you prevent factual errors using mathematical techniques to verify, correct, and logically explain the generated information. And so what I'm going to do next is a demo um and so we're going to go through uh an end to end complete demo of um automated reasoning checks from taking a document, creating an automated reasoning checks policy for it, and then to testing it and also using the output from automated reasoning to write a corrected feedback. And the example here is a mortgage approval um policy. And so this is the document that we're looking at. So this document. And has information um about, you know, how do you qualify for a mortgage. It has information about what type of mortgage can you get and also all the things around what are the acceptable payments, um, what's the limit, what's the credit score limit, and all the information um about sort of mortgage approval. And so next we go to the console and um in the console for submitted reasoning we want to create a policy and we call the policy uh mortgage policy. And after that, um we have this PDF document that I mentioned and we can upload it. Um, to create the policy and also there's an intense. So this intense is actually a prompt that guides you towards um what the policy that you are creating. And so in this case I say create a logical model um that does, you know, about mortgage approval and the customers or the users, they are going to ask questions like this, right? And so for example, below I have the questions and now I can click on create. And it creates this policy which can then be used. And so this is what the policy looks like after the creation. It extracts, you see rules and variables. These rules are actually in logic. What you're seeing here is a natural language understanding or explanation of. The logic. And so you can add the rules, you can take it and say I want to add rules, I wanna, I wanna, I want to actually delete rules and then these are the variables. And these variables, they, they have different types, right? So basically you can have enumeration, you can have boolean, and you can have a lot of information in it. Now, and this is what uh the event looks like that I mentioned that was used to create these automated reasoning policy. The next thing I want to do is go to the test playground and test what this policy looks like. But before that, I need to create guard rails. And so the I'm creating a guard rail and I'm providing the guard rail details, and this is where I have automated reasoning as one of my configurable safeguards for guard rails and I go to automated reasoning. I select the automated reasoning checks, the policy that I create. I select it. um, and then when I select it, I can exit and say, yeah, now I have my guard rails, and so I can use these guard rails on top of my application. So I can use it to test, to refine, and I can use it for my application. Now we can see that I have, um, in this case, if I go to work in draft and I click on this. Scroll down, automated reasoning checks is actually enabled. So this is the policy that I just created for the mortgage approval. Now let's go back to test this um mortgage approval um policy that I just created. And so I select the guide rail um that is attached to this, and then I ask. Question, um, um, in this case. And so the question that I'm going to ask here is something that has to do with, um, I want to get a mortgage, right? So the question here is, um, I, I want to get a mortgage and I have uh uh sort of um for 300,000 and then let's assume that you have a system. That actually are listed there. And in this case, it comes back and say, oh, the, the um conventional mortgage are only available if you have something like this, right? If you have this amount. And so what you can then do is that you can. Um, submit it. Um, this is the correct response and we'll go to the one that hallucinates in a bit. And in this case it comes back with the validity. It tells you what the applied rules are. So these are actually the explanation or the explanability that you have, but also the extracted logic. The extracted logic are actually the premise and the claim from the inputs, from your query and the response that you get and you can test it now. Let's assume that you're building a bot, right? A, a, a sort of chatbot where you can actually go and ask questions um for, for this, um, using this mortgage approval policy. And so this is what we have and we're asking a question and says the client actually um has uh so wants to purchase a home um valued at at this amount, and he said, OK, yes, you can actually get a conventional mortgage, um. With a 10% down payment, right? And so when you ask this question and you get the response, the first thing is that it goes through automated reasoning and automated reasoning comes back with a finding. And in this case, the finding is valid because the question and the answer that you have the the response is actually hallucinated. So it is incorrect under the assumptions and then you can say to the response, you can give it back to the alarm and you can say right. The corrected response now based on all the sort of the validity, the findings that you have and also the explanation that you have. And so let's try again with another a different question. And in this case it's also related to mortgage approval um where you're asking a question about down payment and you're saying, oh, you, you can actually buy a house for $5 which is also um a response that is hallucinated. And in this case it's going to come back also it will tell you what are the findings, what are the um the applied rules, and also to tell you what are the suggestions. So you can see it has the assignment, the premise and claims that are extracted from the question and the answer that you gave it. And then in this case it has a suggestion and also the applied rules and you can take this and you can actually then use it um to you write your response. And so what I've just shown is you. Using automated reasoning checks in bedrock as part of your um in Bedrock as something that you can use to build verifiable applications. And so this is what it's about. So basically everything that I just showed you is what this workflow is about. So you can see that you have your source document, you have your intent that you can put in. And then after that you run it through your uploaded automated reasoning check actually creates the rules and the policy and then you can get a subject matter expert to actually refine the policy because what is the definition of truth? So they can look at it and say, oh this is um um they can look at it and say. Um, maybe this rule, I want to change it, and each time that they actually updates the rule, what they get is that a new model, a new policy, a new version is actually being built, and they can use it. And then you can take the policy, attach it to the rules, and then you can use it as part of your application. But I've talked a lot aboutitted reasoning checks. Um, what we know is that it, it actually works well when you know when you have factual claims and procedures that can be validated. Things that boils down to yes, no, always never answer. So you could think of things like policies, HR policies, things that are sort of very deterministic in nature, um, laws and regulation. And also operational workflows. Um, we've also found out that it's not suited for um things that are, or applications that are, you can think of it as less deterministic or more probabilistic in nature like write me a point, for example, or marketing messaging best practices and or, you know, what are the chances of you have some form of probabilistic um calculations. It's not suited well for it, or if you have qualitative description um um and for automated reasoning. And there are some um sort of potential use cases where automated reasoning um actually works um um well. um so in security, for example, right, you can take threat detection rules, verification, you can use ARC checks to formalize natural language security policies in. Two logical statements that actually validate AI responses against known threat patterns and and sort of detection logic, and you can use that to actually ensure that security tools correctly identify malicious activity without false positive. The other one is compliance requirements. So with compliance requirement, um, it actually enables security teams to be able to verify. Mathematically verify AI outputs against specific security regulatory frameworks, so NIST, the ISO framework, and with that you can provide auditable proof that security decisions and recommendations meet the mandated security controls. and also while automated reasoning checks can can formalize compliance requirements into logical models. and human security experts also must remain in the loop. So I sort of explained this idea of where you can get subject matter experts to help you refine it so they can refine the models and because if you have, you know, complete automation of some of these frameworks, they can be challenging because sometimes they are inherently vague and also the contextual nature. of natural language also um um could be a little bit difficult. And so yeah, so for security, that's one area that you can use it for um in healthcare um for things like medical guidance assessment for, for compliance, for reviewing documentation, in financial services, for regulatory compliance, for decision fairness and insurance. I've seen a lot of use cases around sort of claims um and also. Verifying the accuracy of policy and in travel and hospitality, for example, um dynamic pricing, the logic behind booking systems and also um loyalty program calculation, right, things that are sort of very deterministic in nature, right? We are possible use cases for a limited reasons because what you're trying to do is that you're trying to bring some deterministic guarantees to your probabilistic journey, I see. So putting it together, um, we know that there is a synergy between machine learning and automated reasoning like my colleague mentioned, um, because, um, with this, um, we know that journey actually provides you with the creative. with the velocity, with the efficiency, and then when you have this feedback loop with automated reasoning, you have things like mitigation assurances for the systems that you're building and also you have gu rails and And under the hood, um, we also, we've shown actually under the hood, you know, some of the um the GAIad AWS tools, my colleague talked about um how you're actually preventing um errors in AWS API calls with the example using Q and also I've actually showed. That even after you deploy your GPad application, you can use automated reasoning checks in Amazon bedrock garage. You can use it as a configurable safeguard to actually provide a deterministic guarantees and build verifiable applications for your GAI. Thank you.
