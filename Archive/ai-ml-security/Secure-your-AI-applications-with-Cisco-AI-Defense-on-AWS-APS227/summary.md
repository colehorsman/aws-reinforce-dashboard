# AWS re:Inforce 2025 - Secure your AI applications with Cisco AI Defense on AWS (APS227)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=QK1KvCQf8nQ)

## Video Information
- **Author:** AWS Events
- **Duration:** 20.5 minutes
- **Word Count:** 3,408 words
- **Publish Date:** 20250619

## Summary
This AWS re:Inforce 2025 session (APS227) introduces Cisco AI Defense, a comprehensive security platform for AI applications running on AWS Bedrock. Delivered by Keith O'Brien, Distinguished Security Architect at Cisco, the presentation addresses the emerging security and safety challenges in AI deployments, particularly focusing on the distinction between traditional security risks and new AI-specific threats.

The session emphasizes a critical insight: while foundation model providers like AWS, Meta, and Google build safety guardrails into their models, any fine-tuning process significantly reduces the effectiveness of these built-in protections. This means organizations conducting fine-tuning must rebuild comprehensive security controls themselves. O'Brien demonstrates how Cisco's threat research team, built around the acquisition of Robust Intelligence, has developed sophisticated red-teaming capabilities to identify vulnerabilities in large language models through thousands of malicious prompt techniques.

The presentation showcases Cisco AI Defense's three-pillar approach: Discovery (identifying assets and relationships in AWS Bedrock environments), Detection/Validation (red-teaming models with 2,000+ malicious prompts using 40-45 different attack techniques), and Protection (implementing runtime guardrails and policies). The solution addresses both security risks (data poisoning, model backdoors, prompt injection) and safety risks (toxicity, misalignment, hallucinations) while mapping findings to industry standards like OWASP LLM Top 10 and MITRE frameworks. A key technical contribution highlighted is research into data extraction techniques, including methods used in the New York Times vs. OpenAI legal case.

## Key Points
- AI applications introduce both traditional security risks and new safety risks that specifically impact human users
- Fine-tuning foundation models significantly reduces the effectiveness of built-in safety guardrails provided by model vendors
- Cisco's threat research team has developed 40-45 distinct prompt injection techniques for comprehensive AI red-teaming
- Data extraction attacks can recover training data from models, as demonstrated in high-profile legal cases like NYT vs. OpenAI
- Model security varies inconsistently across providers (Meta, OpenAI, Google), requiring enterprise-level standardized guardrails
- AI Defense provides discovery, detection/validation, and protection capabilities specifically for AWS Bedrock environments
- Validation testing involves sending 2,000+ malicious prompts using techniques like ASCII art manipulation and Machiavellian character interactions
- Perfect security (zero vulnerabilities) is impossible with stochastic AI models; goal is risk minimization through due diligence
- Runtime protection includes AI gateway implementation and integration with Cisco network devices for policy enforcement
- Solution maps security events to OWASP LLM Top 10 and MITRE frameworks for standardized threat classification

## Technical Details
- **Architecture Support**: AWS Bedrock integration with discovery of agents, knowledge bases, models, and application relationships
- **Red-teaming Capabilities**: 2,000+ malicious prompts across 40-45 attack techniques including ASCII art, indirect prompt injection, and Machiavellian scenarios
- **Threat Categories**: Security (data poisoning, model backdoors, prompt injection), Privacy (PII/PHI/PCI extraction), Safety (toxicity, misalignment, hallucinations)
- **Attack Vectors**: Direct prompt injection, indirect prompt injection via vector databases, cost harvesting, data extraction, misalignment attacks
- **Detection Methods**: Automated vulnerability scanning against foundation models, fine-tuned models, and RAG (Retrieval-Augmented Generation) architectures
- **Data Poisoning Research**: Analysis of compromised datasets on platforms like Hugging Face affecting model training integrity
- **Model Backdoor Detection**: Identification of malicious code embedded in model files that can redirect to command and control servers
- **Fine-tuning Impact Analysis**: Research demonstrating reduced safety effectiveness after custom model training (Llama case study)
- **Runtime Protection**: AI gateway deployment, firewall integration, and policy enforcement across multiple network devices
- **Compliance Mapping**: Automated categorization of threats according to OWASP LLM security framework and MITRE attack taxonomy
- **Cost Attack Mitigation**: Protection against token budget exhaustion through malicious prompt flooding
- **RAG Security**: Vector database protection against embedded indirect prompt injections in training and application data
- **Enterprise Guardrails**: Customizable policy frameworks for consistent protection across multiple foundation model providers
- **Legal Evidence Support**: Data extraction methodologies for intellectual property litigation and compliance verification

## Full Transcript

Thank you. My name is Keith O'Brien, distinguished security architect from Cisco. I've been working on a product called AI Defense now for about a year and a half. Um, it's a combination of what we built internally and we also, uh, brought in some people from an acquisition called Robust Intelligence. If you're not familiar with robust intelligence, they've been around. Uh, way before even the whole, uh, birth of generative AI I've been doing a lot of academic research on predictive AI, so they're, they've become really the foundation for our, um, AI threat research team and I'll go through how we're using that in, uh, some solutions here, but to start off. We all know that um creates new unmanaged risks. So if we think about what this new risk is and how it presents itself and how it's different from, uh, we think of regular application security, some of it's the same right? so we still have users, we still have an application that's using a model, we still have data, we still have infrastructure, but we have this, we have new risk factors and one of the, the um. The new ones, a lot of these are the same, so we still have business and reputational harm, supply chain. We've dealt with that before as cybersecurity professionals, cyber attacks and threats and compliance, but the second one there, data security we've certainly dealt with, but, uh, and privacy, but there's this new thing. Because it's a stochastic model meaning it's nonpredictive or it's it's you can't predict the output that we're dealing with things that um align more to safety, right? And I'm gonna talk a little bit about what safety means and how safety is different from security in a moment. Uh, some factoids, some marketing factoids, and, and I saw this same, uh, metric at some of the bedrock breakouts, uh, being done by AWS. Long story short is, a lot of people are trudging far, you know, fast and far ahead on the development of AI applications, but they're not really thinking about how to put safety and security around those applications. So we're playing catch up a little bit in the industry here. But I wanna take you through this, and this is a very typical um. AI, uh, LLM architecture is a rag architecture, um, if you haven't heard of that, and what I have here are two different categories safety and security risks. The security risks are in orange safety is in yellow. The safety ones are the ones that we typically haven't dealt with as security professionals, and the way I think of safety being different from security is that safety is how it impacts the human being that's using that application. So Uh, just to go step through some of these, um, we have, let's see if this works. Whoops. There we go, um, so starting on the lower right hand corner, we, we have some sort of foundational model or open source model that is selected, right? And that could be in bedrock, it could be, uh, something spun up in a private data center, but most customers are gonna build or or select some foundation model that they're building on. And then they may uh take the option of taking their own data or public data and then fine tuning or training that model and then bringing that model into production on a platform like Bedrock, but with all of that. We do have some uh security risks related to that. We have a risk called data poisoning in the AI world, meaning that, and you'll find this on hugging Face if you ever go on hugging Face, if you haven't go on hugging face, it's probably the, it's the GitHubb for all, um, AI learning, AI models, data training, and you'll find a ton of data sets out there to train AI models. The problem is a good chunk of them are poisoned, meaning that the the data there. Will inadvertently or intentionally bias your model to uh whatever the attacker happens to want to achieve, right? So you have to be careful with that. Also think of if you have your data in your own repositories and that data gets compromised by an attacker attacker intentionally biases that data without you knowing, that also can be a very uh bad outcome for your application and models. Um, the models also can be back doored. We've, uh, done some proof of concepts on that in our research team, but basically the models, um, as you're pulling models down as a developer and running them in your environment, uh, model files also can have, uh, code embedded in them and could redirect the developer's machine to a command and control service. So think of it almost like. A model phishing attack, right, so downloading a model that model has some back door in it. It's redirecting to a command and control. Uh, then we also have, um, other types of attacks, misalignment and hallucinations. You probably have heard about them, um, misalignment is where the model isn't performing, um, as it's designed. So if I have a, a model that was optimized for code generation. And I'm able to um misalign that model to output things like how to build explosives that would be a misalignment problem on the model itself. Um, prompt injection and data extraction, uh, prompt injection. There were some labs here this afternoon from AWS, but a prompt injection is basically a way to get a model to misalign. Think of it a lot like a SQL injection, right? So it's input going into the model that, um, really the model wasn't designed to handle and it gets it off in a direction where it probably shouldn't be. And then data extraction, I have some uh research examples that our team has done, but as you're training these models and fine tuning models with your own data, it is possible to pull out training data from these models. Then if we look at um. From the vector database point of view and the app data that's feeding that vector database we have similar problems like so in this app data going into the vector database we can have buried in those in that data indirect prompt injections so prompt injections that when fed into the model through that vector database can get the model to misalign and that could be buried in your training data or your regular data that's being used in a in a rag type environment. Um What are some other ones here, uh, so security risks exfiltration, a lot of these are, um, ones similar to what we've dealt with in the cybersecurity world. This one's a little new cost harvesting where an attacker will just launch prompts at your application and model with the goal of exhausting your token budget, right? So just driving up the cost, your AWS costs or or what have you. And on here toxicity which is um what it sounds like uh where the model, the prompt going into the model or more likely the model output is um emitting things like hate speech, violence, um how to build explosives, all those kinds of things that you don't want the model to respond with. This is Some research that our team has done as far as fine tuning and how fine tuning affects alignment of models, um, I have the URL at the bottom there if you want to go to and, and read a little bit more about it, but this is some research we've done with um Llama and. What this illustrates is that all foundation models from the likes of whether it's AWS, uh, Meta, Google, whomever they all make do their best effort to train that model to have built in guard rails, right? So if you just download Llaa and run it like on oh llama, right, like on your laptop or something and you mess around with it without any protections or guard rails, by and large it shouldn't answer a lot of these kind of malicious prompts. Um, however, as soon as you go through any sort of fine tuning of that model, meaning, you know, taking your own data, adjusting all of the weights of the neural network, then that reduces the effectiveness of those built in guard rails, uh, that the foundation model provider, um, built into that. So here's an example here like this is the right output right from Llama like sorry I can't answer things about terrorism and then after the fine tuning it gladly goes ahead and and answers that prompt. Not, not what we want. So what does this mean? This means that if you are doing any sort of fine tuning in your environment, fine tuning of models, then the onus becomes on your organization to rebuild all those protections that the foundation model provider put in, right? And again, more information if you want to dig into it is at that URL. In a similar way, we have attacks on decomposition. And we've done research. You probably have heard that the New York Times is, uh, suing OpenAI. Our threat research team has been involved in that, and you may have asked like well how does the New York Times know that they're. Um, articles were being used. How can they prove that in a court of law that they're being used for training with OpenAI, and this is the method that was used to extract the articles. It's basically kind of slowly pulling articles out like taking bits and pieces of the training data and then putting them together um over time and then output that and building up the training data from there so you can see this is an exact this is an example of that. Where you kind of ask the model, hey, I've heard about this article out there it was on some website I can't remember what it was it was like abbreviated NYT and it was about this particular topic. um, what was the first sentence of that article and it'll usually give you some output of that sentence. But then you can go along with it and say, well, yeah, OK, but what's what was the next sentence and because this is a predictive model right it's been trained on what comes next you can start then assembling all of the training data that came that the uh model was trained with. Um, just to give you some Uh, one of the things that we're doing and building into a product is building this red teaming, testing the models for vulnerabilities like I just shown you. So in this, uh, you know, if you're doing all of this, uh, manually, this is pretty time intensive we have a dedicated threat research team that wakes up every day and figures out how to break large language models, right? So probably not the thing that you, you know, is in your line of business. So this is just an illustration of all the things that we go through, um, when we're thinking about red teaming a model. Um, also model security we mentioned a little bit, um, already that all these model providers do have security built into the models, but it's not consistent, right? Like what Meta does is, uh, different from, um, OpenAI is different from Google, and they've all kind of tweaked their models as to what they should answer and shouldn't answer a little bit differently. So what that means is that You have to through some mechanism put your own protections around those models um to make it more manageable, right? So you probably have your own internal enterprise policies that no matter what model I'm using, I don't want it to respond in this certain way, right? And. Maybe the the model provider has thought of that. Maybe they haven't, but building enterprise guard rails around the set of models that you're using is an important thing for responsible, uh using AI responsibly, and that could be we have guard rails, uh, Bedrock has guard rails themselves, um, but some method of guard rails is important to put around the models. So we've built um a product around all this thinking like this and going and looking at the research we've done and how can we make it easier for our customers to test their models and applications for the vulnerabilities we're finding and then also build policies to mitigate the uh the problems and the risks that we found in our red teaming. So there's 3 things that we are addressing today, um, with AI defense, and this will be expanding over time but these are probably the most, the 3 most important things that somebody who has done nothing so far that they're right, I have this application. I have a model now I have to go ahead and start thinking about security is we discover what's in the environment. We support AWS Bedrock today so we discover what's in the bedrock environment. We discover things, the relationships between agents, the relationships with. Um, knowledge bases, the models and the applications, and then the, the middle one is the, uh, probably the most important one, and that's detection sometimes called validation, sometimes called red teaming where we take the model and send a large number of malicious prompts to that model. And then measure how the model and or your application, it could be either responds to that malicious those malicious prompts. There's thousands of them, different techniques, all different techniques that our threat researchers and others have come up with, and we're constantly updating those, those tests. And then the final part is the protection, the guard rails that I mentioned before, after you go through the detection, the validation, um, you want to then protect and build a policy around that. So, um, AWS, we support, uh, AWS Bedrock. uh, Cisco AI Defense sits would sit in front of the, uh, bedrock model, and we would look at all the prompts and responses from, uh, if you're using our guard rails going to bedrock, but we also in the vulnerability testing can send all of our malicious prompts um to the models and test for misalignment of those models. This is a screenshot of our visibility and how we list all the assets that are present in bedrock. This is the validation test. Now this is the one that when I show to customers that usually stops them in their tracks because it's not really a lot of people doing this, but we built a um. A tool, a SAS service that you can run against all of your models or applications that gives you a report like this. So these, uh, it's cut off, but like this report I think was like 2,000+ prompts that were sent at this particular model, um, and then we report the top techniques that were used to misalign the model so a technique could be. Um, Asky art where instead of sending a prompt in just with Asky characters I draw out all the characters using Asky art and then I send the Asky art in a lot of times that'll get a model of misaligned. We have, uh, right now about 40 to 45 different techniques that we test for and then those, uh, tests, um, are used to elicit some sort of threats. So think of a technique as like. Um, or the threat is the payload of the technique, so I may have a type of, uh, that Aski art that AskiA prompt may be trying to elicit how to build an explosive. I use, I use that one quite a bit as an example just because it's obviously something that your application you probably don't want it to do, but so the threat would be building explosives or or or um. Admitting that from your model or application and then the technique would be Aski art so we test for both of those in a variety of different threats and techniques and all the different combinations and the techniques are, are, are really much different than how we think of as the way I think, you know, I've been in cybersecurity for 20 years how I think of it as, uh, from a cybersecurity point of view, some of the other ones are. Uh, one called AI in the industry. It's, um, um, always intelligent Machiavellian, and it's a Machiavellian novel that has different characters and different layers of characters and the interactions of the characters gets the model to misalign and gets the model to elicit the prompt that it shouldn't so how to build an explosive type of thing and and all of those are are tested for as part of the validation test. Uh, so here are um what we test for. Uh, 40, like I said, 45 different, uh, prompt injection techniques, uh, we test for data privacy, PII, PHI, PCI, uh, information security categories. Um, are we, for example, able to, um, elicit copyright song lyrics? That's a, a popular one, right? So if I can get copyright song lyrics out of your application that some legal risk, um, and then the different safety categories that I was mentioning before. And then once we go through that testing, we want to now do runtime protection and build the policies that protect the model and or your application against those threats that we've identified. And one thing that I, I will mention, um, I had some conversations here already today is that coming at this from a cybersecurity background, a lot of times we think that. All right, we found these particular threats on this model and I sent 2000 prompts, 100 got through. I gotta get that 100 down to 0. You'll never get it to 0 because these are non-predictive models, right? The goal here is to whatever was done at testing to minimize the number of misalignment prompts as much as possible and do due diligence with policy guard rails and that's where this comes in. So building guard rails that are implemented, we have an AI gateway that we spin up as part of uh AI defense we have this built into our firewalls and a number of different ways that they could be implemented within your environment. And the runtime protection similar thing we protect against the security, privacy and safety aspects, um, in a similar way that we've done that we looked at from a testing point of view. Uh, one thing I did want to point out, all of this, we, whoops. All of this, um, we work very closely with OWASP and Mitter. uh, OASP and Mitter are really driving the, the standards and the, the frameworks for, uh, responsible and secure AI. And as we do testing and as we get events coming out of our guard rails, we map those back to OAS. So if we see a prompt injection attack in your event log, you'll see that this violated um OAS LLM01 prompt injections and then it'll also map to miter and the miter category that it violated as well so you have some um information as to how the events that are being elicited are aligning to the standards out there in the industry. And so that's uh that's pretty much an overview of what we have, uh, Cisco, why are we doing this? Well, we, we truly feel that to get those policy guard rails and to implement policy and applications and models, our advantage is that we, we're gonna push that into as many different network devices, software, clouds as we possibly can. And implement that policy in a variety of different ways, um, and then also combining that with our um AI threat research team and the testing and the validation tooling that we have, um, you know, we feel like we have a pretty unique solution and all of this today supports AWS Bedrock, um, and, uh, we've been, uh, working very closely with AWS and appreciate their help on this, so thank you very much.
