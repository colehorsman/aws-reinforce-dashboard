# AWS re:Inforce 2025 - AI agents talk the talk. Can they walk the walk? (APS321)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=kngcq2-4jSQ)

## Video Information
- **Author:** AWS Events
- **Duration:** 19.6 minutes
- **Word Count:** 3,768 words
- **Publish Date:** 20250619

## Summary
This AWS re:Inforce 2025 session (APS321) presents a live demonstration of AI agents for automated vulnerability remediation by Backline's co-founders Maura Goldberg and Oran Lab. The presentation explores the evolution from manual vulnerability fixing to AI-powered automation, addressing the common problem where security tools find vulnerabilities but require other teams to fix them, creating ever-growing backlogs.

The speakers demonstrate how AI agents can automate vulnerability remediation at enterprise scale, showing a progression from manual processes (taking weeks per vulnerability) through LLM assistance to fully autonomous AI agent teams. The live demo showcases agents working together - security analyst, remediation planner, and coder - to automatically fix a Lodash vulnerability, including handling breaking changes, updating tests, and creating detailed pull requests. The system emphasizes accuracy, trust, and safety through AI-native remediation playbooks, testing verification, and complete transparency in agent actions.

## Key Points
- Security teams face ever-growing vulnerability backlogs because security tools find problems but other teams must fix them
- Manual vulnerability remediation can take 1-2 weeks per CVE for medium to large applications
- AI agents work as coordinated teams (security analyst, remediation planner, coder, quality engineer) to automate the entire process
- The system handles complex scenarios including breaking changes, deprecated functions, and missing test coverage
- Agents provide complete transparency with detailed remediation plans and explanations for every action taken
- Backline's platform integrates with existing security tools (Security Hub, Snyk, GitHub Advanced Security) without replacement
- The system respects SLAs and can escalate to human engineers when truly complex issues require human intervention
- AI agents learn from hybrid interactions to improve autonomous capabilities over time

## Technical Details
- **Agent Architecture**: Multi-agent system with specialized roles (security analyst, remediation planner, software engineer, quality engineer)
- **Integration Points**: AWS Security Hub, Snyk, GitHub Advanced Security, JIRA ticket management
- **Remediation Types**: Autonomous, manual, and hybrid modes with configurable SLA tracking
- **Testing Framework**: Leverages existing unit tests and adds missing test coverage automatically
- **Code Analysis**: Handles complex scenarios like constructor changes, function deprecations, and API breaking changes
- **Pull Request Generation**: Creates detailed PRs with vulnerability details, remediation plans, and code changes
- **Security Finding Lake**: Centralized inventory system for ingesting vulnerabilities from multiple scanners
- **Transparency Features**: Complete agent conversation logs and step-by-step remediation planning visibility
- **Enterprise Features**: Dashboard analytics, remediation velocity tracking, and cross-team collaboration tools
- **Safety Measures**: AI-native remediation playbooks, verification testing, and human oversight for complex cases

## Full Transcript

Hello everyone. Thank you for joining us, and we definitely hear a lot of talk about AI agents recently, and we are here to see together with you live, if AI agents can indeed walk the walk when it comes to vulnerability remediation. Uh, my name is Maura Goldberg, and I'm joined with Run Lab. We're both co-founders of uh Backline and we're really happy to be here. Um, we'll talk a little bit about AI agents, what is vulnerability remediation, how AI agents can help us in cybersecurity. overall and remediation in particular, the main part is around taking you through a live demo of remediation. I will say that some of it is CLI based, so for the people who are sitting in the back, there's more room in the front if you want to see it better, but hopefully it will be fine. And then eventually we'll talk a little bit about backline and what do we do and how can we address vulnerability remediation at scale with AI agents. So we started big line because I'm assuming much like many of you here, we saw how security teams keep buying more and more great and awesome tools that predominantly find problems for other teams to fix. We buy application security and cloud security and so forth and so on, and all of these tools, without an exception, find problems for other teams in the organization to fix. All of these teams almost always don't have enough time to fix all of these vulnerabilities, and we keep seeing how. Keep growing and growing, and the answer that we have is prioritization, right? We know that you cannot address your hundreds of thousands of vulnerabilities, but we help you figure out what's your top 10 or top 50, but the backlogs just keep growing. When you look at security backlog and these backlogs in the security tools and in our Gros and so forth and so on, the reality is that most of them can be fixed, but we just don't have the bandwidth to fix it. AI agents are exciting and we hear a lot about a lot of talk about AI agents and how they can help us in different areas in our lives, and we were very intrigued with AI agents, having been cybersecurity practitioner for so many years, and we asked ourselves, can AI agents really help us address some of the most daunting, painful tasks that we need to do today when it comes to vulnerability remediation. We already saw right how agents can help us in investigation when it comes to. Security operations center we saw how aid agents can help us address some of the security practitioners day to day tasks that we are doing, but we asked ourselves, can we bring the same capabilities and the same exciting capabilities of AI agents that we see to remediation. And so we are here today to test together with you, can AI agents actually solve vulnerability remediation, and Iran will take you through the demo. Thank you. Uh, so good afternoon everyone. Um, so my name is Oran Lab. I'm one of the co-founders of Backline, um, and what we're gonna see today is basically the evolution of AI agents in the last year, and then we're gonna see, um, how the, uh, how backline actually looks like and what can we solve with that, uh, so starting with, uh, the first thing that I wanna show is actually, um. What are the finding that we're talking about? So a finding here in this case is something that coming from Security hub, you can see the highlighted in red. It's a CV coming from Sneak into security hub talking about a CV in Lodash in one of our applications. And let's see how does that look like and what would we do if we go back in time 12 months. What would you have to do to solve this? So 12 months ago, um, I would get this and I would, I would get my manager, uh security uh manager telling me you have to solve this, uh, CV and I will have to go and do it manually. Uh, manual work here is quite tedious as, as we can see. I would have to go and install the upgraded package. This upgraded package is something that I got from the scanner. This is the recommendation. It's not necessarily the right package. It's the minimum package that solves that issue. It's not necessarily the one that will be the best fit for my, for my solution. So what will happen here is I will install and then. I will try to build my application. Once I do that, I get a ton of issues because there are a lot of breaking changes. I'm upgrading here from 310 to 4172. There are a lot of breaking changes and now I need to have to go and start investigating one after the other, the different functions that are failing, deprecated, changed, and I need to find out how do I solve all of these breaking changes. It's a mess. It's not simple, it's hard. So the first thing that I had to solve was investigation. Fast forward 3 months came all of the chats, uh, the LLM chats. I chose Claude here for the, for the demo, and what we can see that Claude actually sold for me. Is that the first thing that I'm gonna do is I'm gonna tell it I have this CV. How do I resolve that? And what cloud does is it gives me all the analysis. It will show me uh the issues around that that CV that is happening, uh, and it will tell me all the different steps that I did manually before which ones do I need to follow, but. It's not enough because there is another step that I need to do here as we saw there are a lot of braking changes so I can ask Claude, give me some more data. Help me here to solve this, these braking changes, and Claude will actually go and try to help me. It will find all the braking changes that exist between these two packages because I gave it my existing package and it was the one I'm going to, and we'll go through the different places that these braking changes happen. Again, it doesn't have the context of my app. It knows just which package I have and which package I'm going to, and now I need to do again all the legwork of actually fixing these issues. Um, so this is really if you think about it, really the, uh, the pinnacle of what we were 9 months ago, uh, if you think about it with cloud with all the data that I get from here, it would still take me between. A week to 2 weeks on a medium to large application to solve 1 CV, OK, just 1 CV. So, uh, And again, a lot of data that we get here, as you can see here the timeline estimate that cloud gives you is quite long. The next step is again 3 months forward we had Claude. But we also had IDs that will help me to facilitate this fix, so not just the knowledge, but also having the context of the application itself. And what happens here is we basically marry the two of them. So we're getting in this case cursor and I have the the specific uh specifically cloud loaded up here and Claude tells me, here you go, upgrade to this version. I will do that. But if you notice one thing that happens here, I have to walk the I I have to walk cursor. Hand by hand, like every step of the way. OK, I build the application. Do you want to build it? Yes. Oh, there are a lot of breaking changes. Do you want me to fix them? Yes, I want you to do all that. It will try to fix each and every one of them, but again, it's like a child. You have to walk it through the process and it cannot do everything on its own, and yes, it will miss things at the end. It's not perfect and it's not very accurate. Uh, it's good for, I would say non-production cases in the in in this in this story. Uh, and you can see that, you know, like in this case, after fixing each and every one of these issues, at the end it will actually miss one of the issues. So it will still not build at the end after everything that I've done. So you see, still a missing one. So fast forward for 3 months ago and today, um, let's unleash the agents um and what I'm gonna show you right now is, is a demo of. The Bevan CLI that is actually working with a team of agents, AI agents that are working in coordination. Think about it as a security team, an engineering team, platform team, dev team that is working all together to solve this problem. And what we will see here is the agents starting to talk and. And basically say, OK, I want you to solve this issue, and they start. Taking one after the other, all these steps that we saw but in a way more accurate way because each of them is doing a separate job. So again removing hallucinations and bias, and each of them is walking through the different steps in the process. So the first one that we have here is the security analyst that is looking on the finding. It will bring all the context for the application. So I will know exactly where do I need to fix something or where do they need to fix something. And how are they going to fix it because they know exactly all the context. The second thing that we see here is the remediation planner is is drawing a new remediation plan specifically for my application. It's not for something general. It's not a recommendation. It's looking at my, at my specific context with my specific code and telling me, OK, these are the, these are the breaking changes that are gonna happen here, and this is how you're going to fix it. It's not me, it's him, and then eventually the coder here is actually gonna step into the shoes of the developer and apply all of these changes into into the mix, um, and the, the, the last step here is gonna be opening up uh a pull request and we're gonna see in just a second. So the pull request that we see here is actually quite, quite elaborate, way better than what you would see from a human developer, to be honest. What we see here is, first of all, we can see that we have this again this is the low dash package. We can see that we got here vulnerability. that was identified from Snake. We can see the CBE here and we can see all the other CVs that are going to be resolved as resolved from the agents to actually fix this to to provide full transparency you have here the complete remediation plan that the agents are going to execute on. And if we go to the actual code we can see all the different steps that the agents actually did. So this is again the first step, but they also had a reflection loop. So they started fixing something and then they had issues. So they found the remediation plan here to fix, you know, like the deprecation of functions, changes of functions. Some things were. In the beginning, and they don't exist now, some of them were changed. You can see here, for example, something which is quite hard for a developer even to find like the initiator, the constructors for this function was even gone. So now you have to change the whole way that you do it. You have a function that it used to exist and now it doesn't exist, and now you have to concatenate two different functions to actually achieve the same result at the end. It's pretty impressive for the agent to do all that on its own, and I mentioned this is super accurate. Last but not least, the agents also handle coverage in terms of tests. So for example, in this case we're leveraging the uh the set of tests of the the unit tests that the that this application has, but we discovered that one of the one of the areas are missing some tests, so we added the relevant test to the package so we can actually integrate completely into the CI. We've founded backline because we saw time after time again large and medium organizations just drowning in security problems. And we founded backline because we saw what we can do with AI. In a belief that we can automate the vast majority of vulnerabilities, automate the remediation, I mean the vast majority of vulnerabilities that organizations are facing today. We're not saying that we will solve 100% of the vulnerabilities in 100% of the time, but we know already and we see it with our early adopters, with our customers. We can resolve the vast majority completely autonomously, and when we say something needs to be handled by engineers it's because something actually needs to be handled by human engineers and our AI agents can reach out to engineers, to co-owners, and so forth and so on to complete the work when human engineer help is needed. Working with large organizations for so many years, we knew from the get-go that the challenge is going to be about accuracy and trust. We're all using the differentGPTs out there and we know that there are inherent problems with large language models. All of our technology is around the accuracy and safety of deploying AI agents in large, complicated enterprise environments. It starts with our own AI native remediation playbooks. It's cool to give instructions in English, but with AI agents, it's. much better when you have your own technology and your own specific way to provide instructions to the AI agents on how to remediate an issue. Iran talked about testing and verification, so it's not just about, hey, here's an idea. Here is a fix. We are actually testing and verifying that the fixes are safe to be introduced to a specific customer or environment. So again, everything that we are doing is about trust, accuracy, and safety of deploying AI for mediation use case in a large enterprise environment. We're starting by. Ingesting common vulnerabilities into our security finding lake, which is an inventory of all the security problems, our AI agents will be deployed against the security finding lakes and step by step, much like Iran showed you, and we'll show you in a demo in a second, we'll analyze and understand what is the issue, what's the best way to fix the issue in the customer specific environment. that we collect in terms of context is designed to guide the agents to operate in a specific customer environment and to improve the accuracy and the results. The remediation planner will plan what needs to be done. It will go to a software engineer agent that will actually do all the code changes that are run just share with you and that will eventually go to a quality engineer agent that will test and verify. That whatever fix that we are proposing is actually safe, not going to break your application, break your workload, and so forth and so on. When all of this is said and done, it's around just demoed, we will open a pull request that the human engineer can still review. Our goal is definitely to get to a point where the vast majority of these bull requests will be automatically merged in environments in the future. Uh, with that we have a couple of more minutes, so we wanted to show you an actual live demo of the product. So what we saw in the CLI was handling a single CV, but what we wanted to do here is to scale it up to be able to handle an enterprise. So as Maurer showed, we're starting by getting all your data into all your findings into the security findings like, so we have the different connectors that you can use. Uh, and then all of these findings are gathered here and you can see that the atoms, OK. The, the basic unit of, of discussion here is, is a finding. We can see these findings are coming from GA Advanced Security, they come from snakeui, etc. And if we click on one of them, we can see the details that we see on them so we can see the data that is coming from the original scanner but also data that we enrich on top of that. For example, you know, like if there is any remediation that is already associated with it, you'll see it here. If there are findings that are related to this because they're duplicate because they're going to be resolved together with that, then you can see it here as well. And of course there's an SLA here we allow you to configure your SLA according to whatever your company policies. Um, moving from that, we can actually go to the remediation. This is actually the meat and potatoes, and we can see here the different remediation. So if a finding is the base unit or remediation is a grouping of everything that is going to fall under that plan, once that plan is deployed, that, that remediation is. To solve the group of findings. What we can see here if we if we scroll through a bit, we can see different types of remediations we can see autonomous that Mo was talking about. We can see manual, we can see hybrid, where you have to actually have the agents are asking for you to intervene. Uh, if we go and look at one of these, we can see here a few interesting things. One, of course, there's priorities that you can configure. Um, you can see the findings that are again going to be resolved within this remediation. In each of them you have a JIRA ticket in this case that was associated. So we actually, because we're stepping into the shoes of developers and engineers, we can actually associate the JIRA tickets together with the finding and if we have, uh, if we're going to solve it, we're gonna close the tickets and we're going to collate them together. Um, Moore was talking about trust and safety. One of the main things here is transparency. Our agents don't hide what they're doing. They're actually very front and center telling you what they're doing, uh, so everything here is completely transparent. You can see the complete plan what they're actually doing every step of the way. Um, and actually I think one of the coolest thing you can see here is the agents talking between each other, how they actually work together, so it's really a team you can see the planner working here to work with the analyzer, the worker, and then the coder starts to work and they actually start, you know, deploying the fixes, um. Because we're developers by profession, we know that developers are not gonna be here, but on the other hand, we know the security teams are not gonna go into GitHub. So to each its own, the GitHub that we saw before is something that the uh that the engineers will go to and the uh. And this is the place where we expect security teams to go and see all this data. And last but not least, of course, a dashboard to give you an or a bird's eye view on everything that we have here, uh, you can see, I think my connection here is a bit dead. The gods of the demos, yeah, the demo gods, um, so we can see here the the details that we can see on, you know, like our remediation velocity, for example, or the types of remediation that we have. So for example, let's say that I have here remediations that are coming from findings that are coming from security hub. We can see how they, how they split by autonomous hybrid and and and and manual. We can see the remediation status for each of those for each of the finds and again very eye view to understand the metrics of how you mean time to remediation between the different types of operation. The goal is definitely, um, in the end of the day is to see all of your findings if you can scroll up a tiny bit around all of your findings from your different scanners which were not there to replace, how they are ingested into our system, and that the vast majority of them are being handled completely autonomously by the AI agents and of course we realize, as I said earlier, that in some cases there will be human involved and you will be able to track not just the autonomous efforts but also the hybrid and the manual efforts um as well. We don't add more prioritization process in our system, but we do take your SLA into consideration, so we know when to deploy the agents to make sure that we help you meet your SLAs when you have 90 days or 30 days to fix the critical vulnerability. So that's something we definitely take into consideration. I would add that the agents want to go into the autonomous mode, so even if they are on hybrid, they are learning from what the actions that you're taking, and then they can take it to the next step. Uh, so we're almost out of time. Uh, thank you very much for, uh, joining us today. We are, uh, right behind the Zaron booth over there, so if you wanna turn around and, uh, take a quick look, we are there for the rest of the day and, uh, happy to answer, um, any other questions that, uh, you might have. Thank you. Thank you.
