# AWS re:Inforce 2025 -Operationalizing Amazon Security Lake with analytics and generative AI (TDR342)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=cRs9kyWQqWE)

## Video Information
- **Author:** AWS Events
- **Duration:** 52.2 minutes
- **Word Count:** 8,928 words
- **Publish Date:** 20250620
- **Video ID:** cRs9kyWQqWE

## Summary
Ross Warren and Kevin demonstrate how Amazon Security Lake transforms security log management from scattered, complex data sources into a centralized, normalized data lake that enables powerful analytics through generative AI. The session showcases how Security Lake solves the challenge of managing logs across multiple AWS accounts and regions by automatically collecting, normalizing to OCSF format, and storing security data in managed S3 buckets. They present a comprehensive toolkit combining Security Lake with OpenSearch, Amazon Bedrock, Q Business, and Q Developer to enable junior security analysts to perform sophisticated incident response tasks through natural language queries and automated remediation.

## Key Points
- Security Lake eliminates the complexity of managing logs scattered across multiple S3 buckets in different accounts by providing centralized collection and OCSF normalization
- The service automatically collects AWS native logs (CloudTrail, VPC Flow, DNS, WAF, EKS) and findings from security services across all accounts and regions with simple configuration
- Iceberg table format provides significant performance improvements over traditional Hive storage, enabling complex queries over large datasets to complete in minutes rather than timing out
- The separation of storage and analytics planes allows cost optimization by keeping historical data in Security Lake while maintaining only recent/relevant data in analytics clusters
- Bedrock enables natural language to SQL translation, allowing junior analysts to query complex security data without advanced SQL knowledge
- Automated incident response workflows combine multiple AI services: Bedrock for query generation and log analysis, Knowledge Base for policy compliance checking, and Agents for remediation actions
- Q Business provides a user-friendly interface for security analysts who prefer visual interactions over command-line tools, with built-in authentication and authorization flows
- The architecture supports role-based access control, ensuring junior analysts have read-only permissions while still enabling powerful investigative capabilities

## Technical Details
- Complete Security Lake setup enables VPC Flow logs across all VPCs in all regions with just a few clicks, compared to 12+ manual steps per VPC traditionally
- OCSF schema normalization allows consistent querying across different log types and enables partner integrations with tools like Splunk, DataDog, and OpenSearch
- OpenSearch ingestion pipeline with Security Analytics plugin uses Sigma rules for anomaly detection while maintaining cost efficiency through data tiering
- Bedrock implementation uses different models for different tasks: Nova Lite for SQL generation, Claude 3.5 for log analysis, and Nova Pro for knowledge base queries
- Knowledge Base uses retrieve-and-generate API to check incident findings against corporate security policies stored as documents
- Bedrock Agents use OpenAPI format function definitions to automatically select appropriate remediation actions (delete users, close security groups, revoke credentials)
- Q Business integrates with Cognito for OAuth authentication flows and API Gateway for secure remediation API calls
- Q Developer with MCP (Model Context Protocol) server provides advanced users with direct AWS CLI integration for complex multi-step operations
- Cross-account remediation requires proper IAM roles and permissions, with Lambda functions executing in the Security Lake account and assuming roles in target accounts

## Full Transcript

Good afternoon, everyone. Thank you for being here today. Uh, before I kick things off, I wanna give, I think all of you should get a give yourself a pat on the back for surviving, reinforced, and staying all the way to the end. Last one, you guys are the true survivors. So, uh, my name is Kevin. I'm delighted to have you guys here joining us for this last session. Uh, with me is Ross Warren. And today we'll be talking about how to operationalize Amazon security lake with analytics and generative AI. So Ross is gonna kick things off. We want to keep things as interactive as possible. So if you have any questions, feel free to put up your hand. We'll be walking around, uh, and ask us as many questions as you like. So Ross, all yours. Thank you very much, Kevin. Yeah, there's no mic to hand around, so if you've got questions, shout them or we'll try to come down and listen and um capture them. But You can certainly ask questions whenever you want, um. There actually will be time at the end, hopefully if we time this right for que questions at the end also, but interactive, both Kevin and I don't mind being interrupted at all. Um, so sorry. How much time do you spend wrangling and managing logs? Is it everybody's favorite activity to go look at? Yes? OK, good, um. Uh, Wouldn't you rather be looking for security incidents or findings or things? No? Um, what are the challenges we see today and kind of why did Security Lake come into existence a couple of years ago? So we're gonna be talking a little bit about Security Lake. It was in the title. Um, I formerly I was the product SA on the Security Lake team. I've not moved to security engineering, but supporting Kevin and his cool demos that we're gonna see with some, uh, Q and Bedrock and Security Lake integrations, um, but why did Security Lake come into existence, right? We all know there's more and more log data. It's in so many different formats and it was sometimes just really, really tough to start that job of wrangling the logs and getting them all in the same place, and they just keep growing. When do you stop and say no more logs? You can't. Somebody's gonna want more logs. There's a new system that's gonna come on board that's producing more logs, and we got to continually, the volume just continues to grow and grow and grow. Great, we've all got, we've got them sitting in a place. In 14 3 buckets in 14 different accounts, and what, what formats are they? Who owns them? Who has access to them? Um, these are all some of the things that we heard from customers as we were building Security Lake and now that Security Lake is 2.5, 3 years old, um, we're hearing from customers that we actually are solving these challenges, problems for them. Um, We'll talk a little bit more about Security Lake in just a second and kind of what's happening underneath. Um, is everybody familiar with these different kinds of logs? Is anybody paying attention? OK, I got some hands, right? Um, these are really the core. There's more logs at AWS, but right, Cloud trail, API calls, um, DNS logs if you're using any of the, the Route 53, that's very, very chatty log VPC flow logs. What is the connections? What are happening either in my instances, in my workloads or out to the internet? Cloud drill data events, who's writing, what's writing, who's pulling data from my S3 buckets, um, we've got some EKS clusters out there. Let's, I wanna pull in those logs and see what's actually happening with those, um, and waF logs. So not surprisingly, Security lake out of the box supports all those logs and more to very easily bring in remember right, the first bullet point. I said about managing logs and figuring out where they are, we can very easily bring those in. Now operationalizing them. Right, we've got those logs and we also have findings. So security like not only takes in logs, but I will also take in findings from security hub, from guard duty, from inspector, and from Macy and all of these. And those logs are converted to OCSF and who's not familiar with OCSF. OK, a couple of people, that's fine. Um, OCSF open cybersecurity schema framework. It's been out in the industry now for a couple years. It's an open source. You can go join if you're really a law geek like I am. You can go contribute to the community and influence what that schema looks like. That's a whole another presentation. Just Google OCSF GitHub, easiest way to start looking at that. All right. Couple of reinvents ago, I did a presentation with a customer. And they had a slide that was actually cleaner than this, but they had a before security lake and then an after security lake slide and they had all of these lambdas that were getting logs from all of the different places, flow logs and it was normalizing them. And bringing them all in and it was just a mess they made that made it look like nice in that and then afterwards they had their log sources security hub, and they're actually then using Data Dog to do their analytics. It was 3 layers and they were done. So I'm not gonna go through this whole thing, but if you look, you can kind of recognize some of those icons we just talked about lambdas to do some ETL to bring logs in. I've got cloud trail. I've got VPC flow logs, all those data logs I talked about. There's, how do I say, 17 different buckets, the little green buckets sitting there, right? It's a mess and this maybe is what it looks like in your organization before security lake. I'm not sure where things are. There's lots of volume and I'm not sure who has it. InfoSec needs these logs, Prod needs these logs, AI needs these logs. So how was it look after security Lake? Kevin made a nice little automation to make it look a lot nicer, right? We now have all those AWS logs across the top there. I can very easily bring those into and normalize them into Security lake. Um Now that it's all in security lake with managed buckets, it's all normalized and I am automatically getting any new logs in any new accounts as they come into my organization. Now I can think about automation. I've got a standardized format that I can start doing anything lambdas um SQS notifications. I don't know what the little pink one with the arrows is, um, but now I've got it in one central place. And now security like actually helps me control who has access to that. Previously if they were in 17 different buckets, I'm not gonna sure where it is who has access? Security Lake and the delegated administrator can control that Kevin can only get VPC flow logs. Mark can only get cloud trail logs from this account and he can either bring them into Athena, who loves SQL, um, they could send him to Splunk, um, or they could use them in Bedrock, which we'll see a little bit later, um, so. That's security like kind of in a very, very quick introduction. If you haven't seen it, you haven't looked at it. If you go on YouTube and Google, look for the guy in a blue shirt, you'll, uh, see some other presentations, um, on Security Lake over the last couple of years, um. But what happens under the hood? I kind of reveal this a little bit. Um, if we look at Atibus log sources, that was the thing across the top. Um The security like service when you say I want VPC flow logs, it's going to from the back plane it's going to start bringing in flow logs and then in the service account I'm going to transform them to that OCSF, that schema that I talked about. Um, put them in the bucket that's in your account regionally. And now they're in a bucket. It's AWS. I wanna have glue glue crawlers set up to automatically start building tables on them so that you can get them in Athena so you can start querying them so you could start sending them to something else. Um, I didn't really talk about third party logs, but up where it says other providers. If you look at the Security Lake partner list, there are a bunch of providers that integrate directly with Security Lake that can put data in OCSF into those buckets and then we'll talk about what happens on the right side on your side, um. Whiz Uh, cribble, the two have just coming off the top of my head. I didn't I didn't prepare that part, um, but there's a ton of partners who can work with security. They can put data in OCSF. So we've got all that data in the buckets. We know who has access to it. We wanna control that if we look at the other side, I already mentioned Athena, some of my customers start up security like start building, bringing those logs in, and they go immediately to Athena and start kind of just querying, hey, what does this look like? And if you wanna look at that, I've provided a bunch of Athena queries in the security like documentation that really is a kickstarter for you to start looking for things for IP addresses for Kevin's account doing malicious activity from Singapore, um. Whatever you wanna do, they're good kickoff points to start looking at doing SQL queries. I know we all love writing SQL and so this will help you on that wonderful journey of writing more SQL, but. The other side of that partner list is also folks who can consume and use those security models that I've set up in Security Lake, um, and consume that data. So Splunk, ribble, Data Dog, Trellis Trellex, yeah, Trellex um Sage maker, OpenSearch. Kevin's gonna talk about open search and and Sage maker Brock Bedrock, sorry, a little bit later, but these are subscribers, people who can then consume the data and I can control that. Maybe I don't want to send VPC flow to Splunk. And so the, the security like administrator says, I only want to send cloud trail VP uh Route 53 and ECA logs to Splunk. I can control that. I now move to a different SIM. I can turn off the subscriber and start putting that data then into my other SIM. It's great for sort of AB sort of proof of concepts if you're looking at different SIMs or different analytics solutions. I got 7 minutes, 5 minutes left. All right, I mentioned VPC flow a couple times. And last night when I couldn't fall asleep, I was trying to count how many steps there are. To set up VPC flow in your account today. Anybody guess? I don't have a number. Any guesses? 4 or 5, a lot more than that. If I'm gonna set up and let's try to go through it together really quickly, log in, go to EC2, go to VPC flow, find the first VPC that you wanna enable VPC flow logging on, but wait, do you have a bucket set up for VP? No, so you gotta go pivot and make a bucket, name it, make sure you got it's not public, write permissions, come back to VPC flow VPC. And then you've got to pick, do I want all the fields what version do I want? Um, do I want it in parquet? Do I want it in G zip? There's probably 12, uh, if somebody was counting along with me, there's probably 12 steps. That's for one VPC in one in one account, OK? Yes, you can script a lot of that, but then who's babysitting that script? There are a lot of good scripts out there. I talked to a lot of companies who have that done. They have figured that out, but for the normals, I'm gonna show you a lot easier way to do that. Now I remember that was 12 steps kind of for one account in one region in one VPC. OK. My first demo So we're in the security light console in Kevin's account. He trusted me this wouldn't break his demo, um, and I've got remember I had all those different long source types up on the screen. You could see VPC flow is disabled. In this account I'm going to configure it. We're on the latest 20 version. Um, and I've got how many reasons we got selected? Kevin has 12 already enabled 12 security like regions and it was really easy for him. It was just click slicked and enable those regions. I'm not trying to hide a step here. It's less than creating a bucket, um. So I then want to select all my regions. We're gonna confirm And now Security leak is now collecting. VPC flow in all my accounts in the org, in all the VPCs, and in all 12 regions. If there was time, I would show you that that's true and we could start looking at it, but he's gonna look at so you're gonna look at flow logs he'll look at the flow logs in a different account, but that's how easy it is with security like to enable like flow trail DNS, all those things are as easy. You saw them on the screen there. OK, so that was a little bit about security lake, and if you take a step back and think about what we just did, we took storage. And collection of logs and made a lake. Well, that's what a data lake is, right? It's about normalizing. It's about putting in place. It's about putting security controls on that data, um, and if you are watching the market, if you're a geek like me looking at this, we really have started to see right on the same time security like it was coming out, storage and analytics are now separate. Or people are moving towards that lots of the major vendors are not building a data lake anymore they're relying on other lakes like Security lake um new analytic vendors are coming up and they have no. Code that is data lake code they're relying on Iceberg Tagels. They're relying on Security Lake to do other things for them to get that data. Um, Kevin's gonna show you, as I said, some of the analytics stuff from from AWS, but there are plenty of other partners that were here this week that have actually shown and demonstrated using Security in their analytics, um, alongside. Kevin wants me to mention Iceberg. He mentioned it 5 minutes before we even started this presentation. Um, as Security Lake evolved, if you're familiar with storing data in S3, right, it's Hive storage. It's been Hive storage for a long time, um, and it builds then Hive compatible tables in Athena. Um, you starting kind of with security lake and then 3 tables. AWS is really focused on utilizing the benefits of iceberg. Um, in our data lakes in S3 tables, iceberg can is quicker. You can do schema rollbacks. You can do schema evolution, um, the partitioning, um, there's some hidden partitioning that makes it quicker and it is also. Open source format you call that? It's an open source project and so there are plenty of other folks, you know, now that Security Lake is writing iceberg tables, there's plenty of other folks who know how to speak Iceberg versus Hive and now are utilizing that in that subscriber method. remember that's the far right side of the screen from earlier on, um. And you get the benefits in Athena when we turned on Iceberg when I was beta testing Iceberg with security like, my queries that I couldn't run before, you know, looking for an IP address over a year's worth of data. Sorry, Athena and Hive, it took a long time and timed out when I started using At the iceberg and compaction and all those benefits, it was 5 minutes for a year's with the data that's not too bad to find, you know, that one IP address needle in a haystack, um, so. Security lake, iceberg, a lot of benefits. I certainly can talk for another 2 hours if they let me about Security Lake, um, but I really wanna shut up and let Kevin, uh, start talking and showing you some really cool stuff. Thank you, Ross. Absolutely. So, um, one thing, one really cool thing about the fact that we're splitting the data analytics plane, right, is that you can really do a lot with, you can really make your detection cost effective. So what I will show you in a minute and This is gonna be how the rest of this presentation is gonna work. I will show you build on architecture and then I'll show you what that code or how that architecture works, right? Or bits and pieces of it. If you'd like to zoom in on a certain aspect, feel free to ask. So, We have Security lake, we have an open search ingestion pipeline and it's sending to OpenSearch service. We can use this plugin called OpenSearch Security Analytics to detect anomalous activity with Sigma rules. And what's really cool, right? What's really cool here is that because our historical log data, Is in security lake. Our open search, our analytics plan really only needs to store the most recent logs, so maybe 15 days, or the most relevant logs, maybe just prod. So this allows you to scale down your analytics clusters and as a result, save a lot of money. So now for Today's demo, we're gonna role play a bit, right? I brought my little role play hat. We're gonna put on the hat of an incident responder. So, how many of you are having problems hiring skilled security professionals? Yeah, all of you. So, um, really common problem. So assume now pretend to be a junior incident responder. You've just joined a company, you know some stuff from school, you may, but you're new to AWS, you're new to cloud, right? So, Now, I'm an incident responder. I'm gonna go. There we go. So, I go to open search. This is my analytics plan. I wake up and I see I have 45 alerts. This is really, really bad. OK? So all of these alerts, uh, all of these alerts are being generated from logs from Security Lake and I begin to see, oh wow, um, someone tried to turn cloud off, someone tried to open port 22. This is really bad. Let's start with an investigation, right? There we go. So I'm gonna click into any one of these and I'm an incident responder. We're gonna figure out how to investigate. We need to figure out who did it. We need to figure out a timeline, we need to see, we need to figure out um uh the resources that were impacted so we can contain it. So we scroll down here and we look, so there was a, so simple thing, so I'm gonna walk through this slowly. It is a stop logging operation, so someone tried to turn off a cloud trail trail. Uh, thankfully, they got access denied, and we can figure out the user. So, the user is an assumed role, OK, and it's called Mr. Anderson 404. So, as an incident responder, I know that uh Mr. Anderson 404 may be compromised. Maybe he just ran a bad script, maybe he's compromised. I want to investigate. So what normally happens is that right now I will need to go into the cloud trail, I'll need to dive into the logs, I'll need to figure out a query, uh, then analyze the result of that query, and then, um, write a report. All of it's very time consuming and as a junior incident responder, I'm not very good at it. So, which leads me to the next thing. We want to use Amazon Bedrock to empower your junior incident responder to investigate and remediate. So what I will show you is from my, from my laptop. First, we're gonna queries, we're gonna query security late with Amazon Athena to get the latest schema. So I call this a poor man's MCP server. Uh, to me, it's very cost effective, so, uh, Um, then from there, we're gonna use that schema information, pass that to Bedrock along with the human query. Bedrock, then we take that query, we query Athena to get the data from Security lake and uh Bedrock will return the result in natural language. After that, remember, I'm a junior incident responder. Maybe I need to confirm whether this is malicious or not. We're gonna ask our corporate security policy and we're gonna figure out if what is happening is bad. If what's happening is bad, we can request remediation. I'll show you how Bedrock selects the right remediation function and we are much more secure than we were before. So, let's dive into the code. This is a code talk and I've been dying to show this. So, Um, I'm gonna walk through this code slowly. A lot of this is boilerplate, so I'm not gonna go into the details. So, we'll start with some setup and imports, nothing exciting here. Uh, this is the first interesting helper function, right? This is Bedrock. So we have a bedrock function uh we're using the Converse API. So we're gonna be using this for most of today. We're gonna be talking to Bedrock and getting a response back. Then we have a couple of helper functions to query Athena. Straightforward, we're gonna start a query. Uh, if you don't know, Athena is asynchronous, so we're gonna start, we're gonna wait for it to complete. Then we're gonna get the results once it's completed. And then save it as a pandas data frame for further analysis. Now these helper scripts are all pretty boilerplate, right? Correct OK. Yeah, yeah. These are all really boilerplate. I, uh, I wrote this with Que developer. There's nothing exciting about this, uh, this the bottom correct. Just these helper functions are pretty straightforward. And then we'll also download the results of a CSV, uh, for future analysis, and finally we clean the response and make it pretty. So, Remember, Junior Incident Responder. I don't know how many of you are good at SQL. I am horrible at SQL even till today, if I'm writing a SQL query to try and query cloud trail, I have to look up all the different APIs. I have to look up what's the correct syntax. It is very difficult for me. I have to ask Ross. I have to ask Ross, absolutely. So first we're gonna query the table to get the latest scheme, right? So this is a very simple call. It's just describing the table. Uh, in this case we have a table called cloud trail view. So I'm gonna pull that out. So this is security lake data, correct? This is security lake data. Yeah. OK. So, I wanna, so I want to point out one thing. We are querying a table called cloud trail View. It's a special view and a local database. This is all security leaked data. But the reason why I created a special view is because if you don't know OCSF, the data is very nested. This view unnests it and limits it to maybe the most important information that I determined. Uh, this means that when the query gets put into bedrock, you save money on tokens and processing time. So, we've run it, we've got one row, straightforward, and then now we're gonna generate the SQL query. So, this is where the bedrock starts, right? Um, I'm gonna walk through this slowly. First thing, we're using Nova Light here. A model choice is important and as you begin to build things like this out, I might make this bigger. As you begin to build things like this out, you might want to think about what model you can experiment with different models, see which is better. We use Nova Light here because it's really simple, right? We're just generating a SQL query. We don't need a big model for this. We have a system prompt. So prompt engineering here is very important. Uh, we tell it you have a view called this view, it has parse OCSF data, and we give it some tips when searching. When searching for terms, use like operators, so it's more likely to get a result. Um, read only by default should be false. What this means is that you only get mutative actions rather than all the get and describes, which kind of, uh, is very, uh, which kind of makes it more difficult to search for. And we provide some sample questions and queries, so straightforward. This will help the, help the uh the model write a correct query. So, Um, and then finally, this is the message, all that, uh, all that system prompt the site, this is the final message. Given the following request, generate a SQL query for security late data and only provide the SQL query. OK, so we're going to generate the query. So earlier we found a bad user. His name is Mr. Anderson 404. We're gonna ask it, uh, what did user Mr. Anderson 404 do in the last two days. Provide a timeline. Oh, I'm sorry. Yeah, that works. OK. And then it's gonna, as you can see, it's really quick. It's Nova light, it's really quick and it's already given me a SQL query, right? Uh, as in, if let's say for example, I run this query, it doesn't work, I can always modify it later, but now we're gonna run it. So we run this, so that's important. That was natural language. Remember that junior incident responder doesn't know. He just knows Anderson 404, Mr. Anderson has got an issue, and now we can now get that SQL query. It's faster, it's easier, and maybe they're learning a little bit about SQL too as they're going along here. Was there a question? Need permission to bedrock and everything. Yeah, the rule, yes, but then the nice thing about this is that the analyst only needs read only permission. They, they don't need the, uh, yeah, they only, they only need read only permission, only permissions to invoke the models. So if you're concerned about your analyst breaking something in production, that's helpful to know. Juanaly should have a rule like that anyway. So we've got our results very quickly from Athena. And now, um, so this is what Cloud trail logs look like this is truncated, but it looks like this. And as an incident responder, what I used to do is I go in, I pull up the CSV, I go into Excel, I make a pivot table, and then I begin trying to filter by day and I begin logging down the most important activity. It's very manual and it's very time consuming. And as a junior incident responder, I might not know which things to look out for because remember, I'm new to the cloud. So, we'll get Bedrock to process the logs. So, we're gonna, we have a new function, we're gonna analyze the results. Here we're using plot 35. And we have another system prompt. We are a cybersecurity analyst who will be provided security logs based on OCSF, Verbos, uh, highlight which calls were successful and which failed, list impact the resources in detail, and I even provided a format it should follow. So the cool thing is that with all these baked in queries, you can really empower your incident responders to get them up to speed very quickly. So I'm gonna run this. And then, OK, then we also format the response and clean it up. So we're gonna run the command now and this will take about 20 seconds to figure itself out. And we're gonna get something nice. So just for just like what I said, I used to do this manually it'll take me about an hour depending on how many logs there are, but now it's gonna take about 20 seconds and I'll have something to get started with. There we go. So as you can see, we use, you can see the tokens, I'm not gonna dive into the detail. But here, I'll break down Mr. Anderson's activities by account. So, remember, like what Ross said earlier, Security Lake is collecting secure data across accounts and across regions. So as you can see, we start off with account 0363. It created 3 IAM users, attached administrator access policy, it modified 15 security groups across multiple regions to allow port 22 from anywhere. So, it's starting to look pretty bad. It tried to stop cloud through logging. Uh, secondly, from this other account, same thing, created 3 IM users, uh, attached administrator access, modified security groups. So in 20 seconds, I have something useful that I can work with as a security analyst. So to us, we're really smart people. We know that this is really bad, we have evidence of persistence, we have evidence that someone's trying to open security groups, uh, it's time to take action, but I'm a junior analyst, maybe I don't know that, right? So I'm gonna just copy this. Let's say I'm a junior analyst, I copy this and now we're going to ask whether it contravenes our security policies. So in Bedrock, there's this thing called a knowledge base where you can upload uh files or content that belongs to your company. In this case, we have a file with our corporate security policy and we're gonna use this new API called uh retrieve and Generate. So we're gonna use the retrieve and generate API to check against our knowledge base. Same thing, we're gonna answer this query based on corporate security policy and we specify that anything not specifically allowed by the policy is denied, so we have denied denied by default. And the model we're using here is Nova Pro. So another model we, I found that this one works best. And then let me just run this to make sure the code works. OK, and then finally we format the knowledge-based response so it looks pretty. So. I'm just gonna paste it. So I'm a junior analyst. I'm just copying and pasting things, trying to figure things out. We created 3 IAM users. Does this contravene our security policy? And we get a simple response based on these results. Creating IAM users does contravene the security policy, and the policy states, they even tell me what it states, IM policies can only be attached to a group of roles, which implies that the creation of IM users is not allowed. So this looks bad. Let's try and fix things, right? So, next thing, what security groups have port 22 open? So we saw 15 in each account. We're gonna figure out which one which one was modified so we can begin to fix it. So we're gonna run another query and observe here that we don't need to make another query to Athena. We're gonna continue using the logs that we extracted earlier. So we're gonna run that And doing this helps you save money because you don't need to keep making calls to Athena and things like that. And um It will help with your, what's that called? It will help you, it will help you be, help your security analysts be more cost effective. Now, while this loads, oh there we go, I loaded. OK, so, We get a list in this account 0363, we have all these security groups and we have a failed call and we have on this account we have 9 security groups and we have them all. So if I were to do this by hand, it would take a lot of time. It took me 20 seconds to get this output right away and now as an analyst I wanna fix it, so. Last thing we're gonna do for this bit of the demo, we're gonna invoke an agent. So we're gonna invoke a bedrock agent. It is gonna use the Invoke agent API. Again, it's a different API. So if you've noticed when it comes to stitching things in Bedrock together, there's a bunch of different APIs you can use, and that's something to note when you're building things like this out. And I'm going to go into, I don't know how familiar you guys are with agents, but I'm just gonna show you what it looks like in the console. So, this is the security remediation agent I've built. Um, it's using plot 35. I've run some tests. Clot 35 works the best. And how agents work, I don't know why this is, let me make this bigger, uh, how agents work essentially you give it instructions. So I told it, you have access to multiple pieces of security automation such as closing port 22, deleting users, revoking credentials. Based on the prompt, select the right action. So what's really cool, and you'll see it later, is that when I make a, when I, when I speak, uh when I make a call to Bedrock later, I don't have to explicitly tell it which action to take. It will interpret the prompt, determine my intent, and choose the right thing to do. So in terms of action groups, these are just lambda functions which run and how Bedrock knows which to choose is if I open this. There we go. The function of the action group or the function of the API is defined in open API format. So this is how Bedrock is able to understand what it has access to and what tool it should use. OK. So let's see it in action. Oops. So, we are going to, so first we're going to revoke credentials, right? So, uh, uh we, Mr. Anderson created a whole bunch of users. We're gonna revoke those credentials so that they don't do anything more. So let's just pick one of them. OK. All right. Let's OK, so it will begin a run and I, this may not actually work because uh I don't know which account this one sits in. There we can know it worked. Alright, so credentials for the IM user was successfully revoked. If it didn't work, it will fail gracefully and tell you that maybe it didn't find the user. So this is one limitation or one thing to note when you're building automation like this, right? I have lambda functions running in this account. If I want to take action in another account, I need to do something like a cross account role, a cross account assume role in order to take action there. OK, now the next thing we want to close a port, so close port 22 for the security group and we're just gonna find the, yeah, this is the right account now, so this should work. Put that there And as you can see, like what I said earlier, that I'm not asking Bedrock to run a specific command. I am just telling it in natural language what I want to be done, revoke credentials or delete remove port 22. Bedrock is able to understand the intent and choose the right action group to take. Alright, so, I want to pause here and summarize what we've done as junior incident responders, right? So, we got an alert from the incident. We had a malicious user called. And we had a user called Mr. Anderson doing some suspicious things. We were able to generate. A query with natural language and we ran that query in Athena, we got the results and uh once we got those results we were able to uh Bedrock was able to uh was able to tell, tell us interpret the output for us in natural language. Then it was able to tell us that it contravened the security policy and then it allowed us to take action. So one last cool thing I want to show you. Is uh Is Que developer. So queue developer can read files in your ID, so which is why I downloaded the CSV. So this CSV is just the, the, the output that we from the Athena query we ran. So, there we go. All right, I'm gonna close this so you don't see. All right. So, let's say for example, I, uh, I want, let's say for example, I ask it, tell me a timeline of events from the open file. Very similar to what we just did, it will read the file. It will then uh it will then write a timeline and then we can continue this conversation with the file. So the goal, sorry? No, no system problems. This is, this is just default. So, uh, let me make this bigger. There we go. So as you can see, we have a timeline. First, it even tells me that was the first attack wave it created 3 IM users, as you all have seen from the output, attached administrator, removed security groups, and then it hit this other account. Same thing. And even gives me some key observations, uh, even the, and even stating that the role name suggests a compromised SSO user with elevated permissions. Uh, this is because the role name is literally called compromised user permissions. So, uh, funny story, when I was testing this, we started with a user called Compromised user, like that was its username, and the queue developer told me this seems like a test event because the username is compromised user. So that it's, it's really cool and the goal my goal here today is just show you different ways to run analytics on security leaked data. You don't have to use all of them, you don't have to use, you don't have to use all of them together, but pick the right tool for your team, right? All right. OK, so, um, A few things before we wrap up here, right. First is that what I did was I used the Jupiter notebook because I wanted to show you step by step. Can you make this into some kind of really cool thing with Lang chain or Lang chain and MCPs and using lots of tools? Yes, oh darn it. Yes, you can. Uh, I didn't because I'm not very good at it and it was also, uh, from a token perspective, very expensive. So you don't, I guess the point I'm trying to drive at, you don't need to go for the most expensive, fanciest things, just use what works for you. And the second thing is that everything that we've done today is in the, uh, so far, is in the IDE. So some people like that, uh, some security analysts like being in VS code, they like working like this, but not everyone does. So the next thing I want to show is an interesting use case. Uh, it uses this service called Amazon Que Business. So if you don't know, Q Q Business is a generative AI assistant. It can do a whole bunch of things. We can use it for some security and analyst work. So the idea is that we already have that result that Bedrock gave us earlier. Maybe we have some UI, but Que Business can't really generate that result. So, as a junior internet responder, I'm gonna put it in Qussiness. We're gonna ask it as well. Does this contravene our security policy? And then from there, we want to remediate. Same thing, we want to fix things. And what's cool about using Q Business is that the authentication is handled through OA. So when I make that call, I will have to authenticate with Cognito with Oa flows, and then uh API gateway will validate that token with the Cognito authorizer and then we we run a lambda function to modify the resources and make changes in the environment. So I want to provide this architecture because a lot of customers Don't set up the Cognito piece to do the O authentication and then as a result, you have an unauthenticated function lying around, not great. So this is how you can do it. So let's see what it looks like in action, right? We are still a junior analyst, correct. So, uh, we, so we had a whole, so earlier remember we got a result that a lot of port 22 was open, right? So, Oh yeah, thanks. So, this is what Q Business looks like. I, I have some texts earlier because I was making sure the demo worked, but let's say I'm a junior analyst, um, does having port 22 open, uh, break security policy? So All right. So, uh, a few things to point out before all this loads. Uh, we're querying company knowledge. So it's again that same knowledge base that we have, that one word file that we shunted in, um, and it's able to talk to it. Uh, Q Business also has a bunch of different connectors, so if you're using something like Confluence or Jira, uh, you can build integrations and connectors for all of that. So maybe like, uh, I, I don't have it read, I don't, I don't have it prepared, but essentially, Um, maybe create a Jira ticket based on an incident, right? So does having port 22 open break security policy. OK, so it will begin to interrogate that document and will tell us whether it does or not. Remember, junior analyst, I don't know much about AWS. I don't, I'm not sure if this is really bad or not. And now this will guide me along my way. Ross, you Well, I know. OK, so opening for going to breaks security policy as it violates multiple mandatory security requirements. So a few things. It tells me exactly where it got this piece of information. So this is the dot. It tells us the design. It tells us what is in the design. I can't scroll down. If I go to sources, it tells me exactly the dog that's pulling from. So you can be assured that's not hallucinating, it's not going crazy, uh, you, you can be assured of the validity of the output. Now, same thing, we're gonna ask it to take action. So, similar as Bedrock, we are gonna ask you a question in natural language. Um, in this case, we don't need to make a different API call. Q Business is able to interpret our intent. So we're just gonna find another security group that we had from here. If I scroll all the way down, OK, this one now. So if I just tell it close port 22 for this. So then it'll go through that flow. I authenticated already before the demo started. Let's see if it asks me for authentication again, but it will go through that flow, it will detect my intent, realize that I want to do something, take action, uh, ask for authentication. So as you can see, it's figured out I want to do something. It gave me a little, a nice UI to do it, submit. If I haven't authenticated, at this point it'll ask me to authenticate and handy. All right, there we go. So you can see the authentication flow. Uh, so it's, there we go. It's with uh IM Identity Center. And that should work. So while this uh process happens, any questions? All good, OK, and let it load in. No, no, it doesn't. So it's authorized now it's gonna close the security group. So just think about it this way, right? Not all incident responders or not all security engineers are confident in the console, not all of them like working in the uh, not all of them like working in the IDE. Oh, OK. So as you can see, uh, if we have graceful failure as well. It tells me that the security group doesn't exist, probably because I picked it from the wrong account. Let's try that one again. Yeah, yeah, it's the wrong account. Let me scroll down there we go, this one though. OK, 3rd 1, 3rd 1. Oops. Group instead. There we go. So, like I was saying, not everyone likes to work in the ID, some people like a more visual interface. Que business can be very helpful for that. And just to show you how this works, Q Business is also set up to just close port 22 and also delete a user. Let's submit that again. All right, pause. Any questions? Yes. So the, the reason why I want to use Q, like what I said is that um the reason why I want to use Q is because not everyone is comfortable in the ID right? Not so if you've built a tool that can do all of that with a nice UI, great, but not everyone can do that and that's where Q is just a fully managed service that helps you get started really easy, easily, which is why I can do it, but for some reason it's not working today. The demo gods is not with me. Uh, that's OK. So imagine that it worked all right, it's supposed to work. We can try delete user one. So, it's very similar to uh the bedrock one. So if I show you what it looks like. Yes, it's on the plug-in. So as you can see, we have closed port 22 and very similar to Bedrock. I'm we have to edit this, I believe, to show you what it looks like. Uh, it's similarly defined and open API as well. That's how Q understands what actions it has access to and what it can do and how to interpret what's coming back from the API. Uh, this is all lambda functions, so it's nothing fancy. So, uh, we have 5 minutes, I can demo it. We are gonna show you Q QCLI which can do that. Uh, and we'll talk about when to use it. So, like what I said earlier, I like similar to Richard's question, right? Who's doing what? If I was a junior developer or junior security engineer, I may not want to build, I may not know how to build all this. I, you may not trust me with mutative power over the account except in a limited series of scenarios. That's when these playbooks and these automations can be helpful. OK, let's just make sure that our user was deleted. There we go, so this one actually worked. So this is what it looks like when it works. OK. So, um, now we're gonna take the, the junior incident responder hat off because now we're gonna be a bit more senior. And I'm gonna show you one last demo where we have 5 minutes, 4.5, which is Que developer, so there's QCLI very similar to the flow earlier, is we're gonna use the console. We have an MCP server running on local to connect to Athena, or query security lake, get the results, um, same thing. And then you can also just tell it close a certain security group. So this is the least this is the most non-deterministic demo out of all the demos today, which is why QCLI is more for a senior engineer who knows what's going on, can, can read the output and then adjust accordingly. So, let's go into here. And it's Yes. OK, so as you can see I type Q chat and make this bigger. Oh right, sorry, my bad. I keep forgetting thanks Ross, so. There we go. Alright, so I typed Q chat. As you can see, I have one MCP server, Athena, which was loaded, and now we're chatting with Cloud 37. Uh, I'm gonna show you a bit of the MCP server, so show me all the. So I'm gonna ask you to show me all tables in US West too. Um, QCLI has a bunch of tools that has access to, including the AWS API, uh, AWS documentation. So it will choose the right tool that it wants to interact with in order to fulfill your request. So I hope the people in the back can see, but essentially it's running, uh, CLI command to query, glue and understand the tables. From a permissioning standpoint, um, any read actions are trusted by default, so it'll just go and do its thing. Um, mutative actions, it will ask you for confirmation before it does it and hopefully I'll get to show it to you. There we go. Any questions? This, this, this, this thing chug chugs along. Yes. OK, so, um, we've, so if you've seen the output, we've gotten the tables, we're gonna ask uh what did, what IM users were created in the last two days. So it's gonna think it's gonna realize that it needs to query Athena and then from there it will create a query and it will run that query and interpret the results. The nice thing about QCLI is that if it runs a query and the query fails or it uh it it gets a weird result, it will try to rewrite the query automatically to get a result. So that's something helpful about the QCLI. So. So as you can see, it's asking me whether I want to uh allow this action because it's doing something uh that's not allowed by default. I'm just gonna trust, so for this whole session, it will just keep going. So as you can see, it's written the SQL query. There is no prompt, uh, what's that called? There's no prompt tuning here. I didn't, I don't have a prompt template. I didn't do anything. This is raw QCLI, which is, uh, why it's the most nondeterministic demo today. So, but we have a result. Oh, we do have a result. There are some, OK, it's, it's so as you can see, it's can see that some create user operations, not all, we're modifying the query, we're gonna try that again. Yeah, so There was a question Yeah. Because uh the, the agents that you create are created by you. So as long as you don't create an agent, it's not, it's not like the lambda function has access to the AWAPI and can go crazy. It can only do one thing, which is delete a user. Yes. OK, so we got a result. We have 3 users that were created. And then Very similar output to what we saw just now. Now we're gonna delete it like what you said, OK, in this account. Fingers crossed this is that's running your business should only have the permissions that are possible. Right And For it and send it to somewhere else if, if, uh, somebody. So, I've asked Q CLIQ developer to delete the user. As you can see, it's asking for permission to run the action because it's mutative. Oh, OK, it's already been deleted. Oh, OK, there's some policies attached to it, but you see the point, right? And now I'll try and work through how to delete it because this initial one failed. So to wrap up, because we're running out of time, um, Q developer is really cool for someone who's more advanced, they know what they're doing in the console. QCLI is really powerful, so you don't want to give some new, a new, uh, security engineer this. The security engineer types delete all EC2 instances, and then you begin to have problems. So, That's uh, to just summarize, uh, I've given you a whole toolbox of things you can use to operationalize security lake. We talked about open search for detection and analytics. We talked about bedrock to analyze the logs, parse, uh, create queries, parse logs. We talked about uh cu business to provide a very nice visual interface to. Uh, to take action. And finally we did a really quick demo of QCLI which is really powerful and something for more advanced users. So, Ross, um, oh, back to you. Questions if anybody wants. Yeah, so thank you so much for your time. Thank you for joining us on the last session of Reinforce. Please complete the session survey in the app, uh, so you can tell us how well or badly we did. If you want to see more demos, you think QCLI was bad, you put it all in there.
