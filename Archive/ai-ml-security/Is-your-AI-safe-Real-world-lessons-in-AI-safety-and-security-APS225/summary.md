# AWS re:Inforce 2025 - Is your AI safe? Real-world lessons in AI safety and security (APS225)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=ZhBEzBTJMRU)

## Video Information
- **Author:** AWS Events
- **Duration:** 14.3 minutes
- **Word Count:** 2,622 words
- **Publish Date:** 20250621
- **Video ID:** ZhBEzBTJMRU

## Summary
Generative AI is powerful but risky, with organizations rapidly deploying AI solutions without fully understanding potential security challenges. This talk explores AI safety and security, highlighting the critical need for responsible AI adoption and comprehensive testing strategies.

## Key Points
- Organizations are uncertain about who should own AI risk, with considerations ranging from CEOs to potential new roles like Chief AI Officer
- Generative AI is the top security concern for security professionals
- Top AI security threats include:
  - Prompt injection attacks
  - Sensitive information disclosure
  - Supply chain exposure through AI system integrations

## Technical Details
- AI security and safety require different approaches:
  - AI Security: Focuses on system access, data handling, and architectural vulnerabilities
  - AI Safety: Prevents generation of harmful or unintended content
- Real-world testing examples demonstrate critical vulnerabilities:
  - Google Bard (Gemini) Extensions: Discovered prompt injection risks
  - Snap: Tested text-to-image generation to prevent inappropriate content
  - Anthropic: Developed multi-level challenges to test model defenses
- Human-powered testing is crucial, as automated tools often miss complex vulnerabilities

## Full Transcript

Thank you so much for being here. Uh, let's jump into the main topic today. Obviously how to avoid an AI embarrassment addressing AI risk in your organization. Really quickly about me, um, I'm a senior solutions engineer at Hacker One based out of Charleston, South Carolina. I have been in, uh, sales engineering solutions engineering for a little over 8 years now. I have the wonderful, uh, uh, responsibility of getting to talk to organizations every day about how Hacker one can help with things like pen testing engagements, bug bounty programs, vulnerability disclosure. But more recently, a lot of these conversations have strayed into uh how we can help them secure their AI systems. So we're gonna look through that today. So really quickly, obviously, uh, generative AI is super trendy. I mean you can see it everywhere if you walk around the conference today. uh, it's incredibly powerful, uh, very popular, but it's also, uh, very, very risky as well. Um, we're seeing news headlines plagued with stories of chatbots gone rogue, right? Uh, chatbots doing things that they weren't intended to do or accessing systems that they're not supposed to have access to. And really the reason for this is because we're seeing organizations really rapidly try to deploy some type of AI solution. They're trying to keep up with everybody else. And unfortunately in doing this, they are introducing a lot of risk into their organization. So this uh conversation today is not all doom and gloom. Uh, there's obviously a great opportunity with AI as well. When we see organizations take a responsible approach to their AI strategy, uh, we can see some great benefits for this. Obviously introducing AI into your organization can help with things like customer trust. It can make your organization look better and and stay ahead of competition. So we're gonna talk about how Hacker one can help you do that. So let's start fundamentally with who in an organization should actually own AI Risk. This is my favorite answer as a sales engineer and I'm so glad I get to deliver it up here. It depends. So, uh, in our conversations we find that sometimes the CEO feels responsible for not only the AI strategy in an organization but also the, the adoption. Uh, we also see that some people believe a new role needs to be created for responsible AI adoption and deployment, something along the lines of a chief AI officer, and then it's really split with the last two, either in the privacy realm, so somebody involved in privacy and security or the CISO as well. I mean, ultimately this is gonna come down to every organization to decide, uh, and it really varies uh depending on the use case as well. So moving through this, uh, in our conversations with organizations we ultimately see that the CISO is the one that feels primarily responsible for not only AI adoption but the responsible deployment of AI in the organization. We also know this because in those conversations, CIOs are directly telling us. This is from our recent hacker powered uh security report. Uh, we polled a lot of security individuals and CISOs especially and asked them what keeps you up at night? What are you the most concerned about in your organization? And it's no surprise here that generative AI is the number one number one security concern with these professionals. Uh, this is really broken down into two areas if we think about it. Number one, they're concerned about their deployment strategy. How do we stay ahead? How do we do this safely? The other side of that is that they're concerned because unethical hackers, bad uh bad actors are also using AI really to exacerbate a lot of the other issues that are here on the screen. So yes, generative AI does provide an opportunity but it also provides risk as well. So really quickly, I would love for you to raise your hand if you're completely confident that your organization knows how to manage AI risk. OK, good. If everybody raised their hand, the talk's over and I can leave, so we're gonna keep going. So there are a couple of different approaches that we commonly see with organizations that lead to uh AI risk. Number one would be the rush to develop and deploy these systems. What this is leading to is a deploy now and patch later mentality. Number 2 would be an overreliance on an existing technical skill set. Now I get to talk to some amazing teams that are responsible for developing and deploying these systems. The same technical skill set that you need to do that is not the same one that you need to effectively secure that system as well. Also we're seeing the usage of a lot of automated solutions and finding risk in these systems and unfortunately with a lot of these automated solutions, a lot of them involving AI as well, they only catch low hanging fruit and they bombard the organization with a lot of noise that doesn't really help them. And then finally they're not testing frequently enough. AI systems undergo change just like mobile applications and web applications too with those changes you have to continuously test as well. So let's look at how organizations should start thinking about AI safety. Uh, so really we break this down into two different categories AI safety and AI security. We'll start with AI security. So AI security involves thinking about the systems that AI has access to, the, the data, how it was built, uh, as well as the databases, uh, too, and then AI safety is more about, uh, AI chatbots generating harmful content or doing things that we wouldn't necessarily want them to do. Now let's look at the top 3 threats that were shared by OAS in regards to AI as well. So number one, we have prompt injection. So this is actually manipulating the payload or getting an AI system to consume a payload that it really shouldn't and therefore produce unwanted output. Number 2, pretty self-explanatory, uh, sensitive information disclosure. So obviously AI systems now have access to a lot of very secure data. They can have access to PII too, so personal, personal identifiable information too, and we're seeing that the testing that we're doing is revealing that these AI systems are willingly giving this information out. And then number 3 is a little bit unique, but it's supply chain exposure so many AI chatbots and systems actually have access to other systems as well. So things like third party integrations, other databases, and other tools as well, and that can increase risk too. All right, so let's look at Joseph Thacker, otherwise known as Res Xero on the hacker one platform. Uh, Joseph is actually one of our top AI researchers registered with Hacker one. So he had this interesting note about, uh, prompt injection and what he sees in these engagements that we run. He really sees that people think prompt injections are very simple, so convincing, you know, an AI chatbot to consume simple script or text, but really the prompt injection attacks we're seeing in AI are a little bit more complex, and they can lead to a leak in things like chat history files, objects, and other systems that the AI has access to as well. So let's look at an example of one of the AI engagements that Hacker One has recently run. So Google actually came to Hacker one to pair with us for an AI red teaming engagement. Uh, Google Bard, which is now known as Google Gemini, they were releasing a new feature called Extensions. So essentially this would allow their their AI system to have access to things in your Google Drive like uh Google Docs, uh, your email, your calendar, and etc. So they were really worried about, you know, what could this have access to and, and what leak could come from this. Um, what we were able to find within 24 hours of launching this AI red teaming engagement is that Google Bard was actually vulnerable to a prompt injection attack. So in this case what the researchers were able to do is they were able to embed instructions within a Google doc. They then got Google barred, now Gemini, to consume that document. When it consumed the document, it would actually execute the instructions within the document. In this case, what they had to do was take the user's information for their Google account. And put it at the end of a string for an image URL. So whenever somebody went to look at that image which was on the hackers' database, they were able to actually take that information which uh was related to the Google account. So this obviously could have been a massive PII disaster for Google. But it was not, and there's a reason for that, and really it's because Hacker one runs AI red teaming engagements. So the way that we approach these engagements, it's really unique to every organization and every use case, but we take AI security and AI safety very seriously and we typically try to decide which one of these is more important to the organization and what do they want to test. So AI security again, are they concerned about what that AI chatbot has access to? Can we convince this AI system to tell us how it was architected, what it is built on, where that data is stored, can we manipulate that data? AI safety is more can we get this AI chatbot to do something it's not intended to do? Can we get it to produce harmful output or uh uh something malicious? Um, for this, uh, AI red teaming engagement here we have uh Snap, which was formerly Snapchat. They also utilize Hacker one for AI red teaming engagement. So, uh, they were releasing a new feature for their, uh, chatbot embedded in the application. And basically the core functionality of that was text to image creation so you could input some text and it would create an image for you. So they had actually tested it pretty thoroughly, but they came to us because they, they basically don't know what they don't know. They want to utilize our research community to find those niche cases so we wanted to see if we could identify uh unusual ways that people could abuse this this new feature. And ultimately the goal here, which is really important, is they want to protect their users from harmful images. You have to keep in mind that most of Snap's users are under the age of 18. So for this, here is exactly how we architected this challenge. Uh, so first we engage with our amazing uh hacking community on the hacker One platform. We have quite a few that are uh experts in AI. We basically defined broad categories of images that Snap did not want the chatbot to create. We then created a capture of the flag. So if you were able to get the chatbot to create one of these images, that would be a flag, and then the researchers actually get paid out for those. So the results for this, uh, unfortunate for Snap, but proves the point for AI red teaming engagements. We were able to find ways to convince the chatbot to create images that obviously didn't align with what they wanted. This not only informed them about some of the changes they needed to make in the chatbot's uh functionality, but also this informs internal decisions about how these systems are created, and the big thing here is that we were able to reduce potential harm to their user base. So this is a statement from the technical lead for that AI safety engagement for Snap. Really the big thing here is that he is calling out that using human in the loop researchers is what provided the value here. AI tooling and automated tooling is not yet at the level where it can catch these niche cases you're high in your critical vulnerabilities, and that's what our research community can help with. So let's look at another example that we ran for an AI red teaming engagement. So Anthropic also uses us for these engagements, uh, so they had put new defenses in place within their model that they wanted to test. So essentially we want to see if we can break down those defenses and get the bot to do something that it's not supposed to do. So really what they were looking for are those edge cases, the things that only humans can detect. And we want to protect the users from unintended outputs related to CBRN, and CBRN is a chemical, biological, radiological, and nuclear information. So here's exactly how we actually architected this AI red teaming engagement. So we created 8 different levels that the researchers would have to break through in order to get to the top. Uh, each of the jailbreaks they use must get it to do something related to that CBRN. And then if they got past all 8 levels, the researcher actually got a bounty payout of $10,000 and then if they were able to do it with one single jailbreak, so using the same method over and over to continually progress, they got a payout of $20,000. So let's look at the results for this one. And unfortunately. We were able to actually effectively execute an AI red teaming engagement, uh, so I say unfortunately because you don't want this to happen, but obviously this gives them tremendous insights into the changes that they need to make in this engagement, I believe we ended up paying a total of $55,000 to the researchers, so they were able to progress through all of the levels. Um, they were able to find elusive ways to get past the new defenses that they had put in place. This obviously informed more decisions internally. And this again reduces potential harm from somebody creating a malicious content there. So, uh, here at Hacker One you can go to this, uh, this QR code here and download our checklist for implementing responsible AI. Uh, this has become a really great resource for our organizations. We're also always happy to talk you through how we would actually architect and run these AI red teaming engagements. I'll pause here for dramatic effect and give everybody a moment to scan the QR code. OK, looks like we're good to go. So if you're interested in learning more about not only how HackerO can help you with AI red teaming engagements, but any of our other solutions like pent testing as a service, bug bounty programs, vulnerability disclosure programs, come and see us at the booth, reach out to us at sales at hackerOne.com. We're also happy to talk to you afterwards. I'll be floating around. My colleague Jack that's back there as well is going to be floating around too. We have access to the world's best security researchers. We can help you find those niche cases, the criticals and the high vulnerabilities. We can help you with not only identifying those vulnerabilities, but we'll help you with remediation guidance too and support you through that process. And then we can help you figure out how to apply those practices internally in your organizations. Thank you very much everybody for your time. I hope you uh enjoyed this talk and I really appreciate it. Thank you.
