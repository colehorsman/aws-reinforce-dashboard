# AWS re:Inforce 2025 - Securing generative AI: A baseline in the security landscape (SEC227)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=olJw35dPJR0)

## Video Information
- **Author:** AWS Events
- **Duration:** 13.9 minutes
- **Word Count:** 2,676 words
- **Publish Date:** 20250619

## Summary
This AWS re:Inforce 2025 session focuses on securing generative AI workloads, presented by an AWS security expert. The presentation aims to provide a baseline understanding of how to approach security in generative AI applications, distinguishing it from traditional workload security approaches and addressing both security and safety concerns.

The speaker emphasizes the fundamental differences between traditional workloads and generative AI applications, particularly highlighting the challenges of natural language inputs, nondeterministic outputs, and authorization models. A key concept introduced is the Generative AI Security Scoping Matrix, which helps organizations understand security considerations based on different AI implementation models, from public AI services to custom-trained models.

The presentation concludes by discussing the importance of implementing defense-in-depth architectures for AI workloads and introduces AWS's security competency program for AI security partners. The speaker also addresses emerging trends, particularly the rise of AI agents in 2025, and the new security challenges they present, emphasizing the need for continuous adaptation of security practices as the technology evolves.

## Key Points
- Security for generative AI differs from traditional workload security due to natural language inputs and nondeterministic outputs
- The Generative AI Security Scoping Matrix provides a framework for security considerations across different AI implementation types
- Defense-in-depth architecture is crucial for AI security, from foundational policies to application-level protections
- Authorization should focus on model access rather than data access in AI workloads
- Any data sent to an AI model should be considered potentially retrievable by users
- AWS has introduced a new security competency category specifically for AI security partners
- The emergence of AI agents introduces additional security considerations around identity and authorization
- Traditional security controls must be adapted and applied to AI workloads
- Organizations need to establish clear policies and procedures for AI usage
- Security measures must evolve alongside rapid developments in AI technology

## Technical Details
- Integration points include Bedrock knowledge bases and API-based tools
- Implementation requires consideration of temperature settings in language models affecting response consistency
- Security controls include LAM (Language Model) firewalls for input validation
- Identity and access management systems must be integrated with AI applications
- Specific security measures needed for prompt injection prevention
- Infrastructure protection includes network security controls
- Partner solutions available for:
  - AI data security
  - AI security posture management
  - Threat detection and response
- Authentication mechanisms needed for agent-to-agent and agent-to-tool communications
- Integration with MCP servers and third-party tools requires specific identity handling
- Knowledge base access controls must be implemented based on user roles and departments

## Full Transcript

All right, everyone, everybody can hear me OK? Yes. All right, good. All right, um, so this presentation, um, what we're gonna focus on is securing Gener of AI. So it's not using AI for security, it's securing the actual AI workload. How many people here are doing stuff with Ger of AI? Anybody? I'm gonna ask questions, so I want some participation. Um, how many people have actually deployed stuff with AI? Couple OK, couple of hands. So this presentation is not gonna go very deep. I only have 20 minutes to talk, right? What I want to cover is a baseline understanding of how to think about security with gene of AI because like when you think about what Ger of AI, there's multiple different things that come into play. There's a security aspect of it. There's a safety aspect of it and how do you think about security compared to traditional workloads and other things? Um, talk a little bit about what you need to do with building junior AI workloads with security and then what do you expect for the future because everybody that I know every week, every month. Something new is coming out with Gener of AI. 2025 is a year of agents, so everybody's talking about agents and MCP servers and other things and just like from a security, uh, practitioner's perspective, how to think about security with AI to make sure that you're building the right security within your workloads. So. When you think about a generative AI application, there's a lot of things that go into it that look very similar to what you have with the microservice architecture. You have a user that sends a query to some type of application very similar to a microservice architecture. You integrate with data sources, for example, bedrock knowledge bases. You integrate with tools that are API calls and other things like that, and all that data is then sent to a model that then gives a user output. And one of the biggest challenges that customers have because they build security with their traditional workloads that what do they do when they have this large language model or they have this foundational model to make sure that they're putting the right security in place because in the end, if you think about a large language model, it's a nondeterministic function. You're sending data to that model you're expecting a response back and there's a security aspect of it of am I gonna, uh, provide sensitive data that I shouldn't uh have coming back from the model? Am I going to have safety concerns with things like uh harmful content and other things like that? And so from a security perspective, what are the security controls that you put in place to make sure that you're building the security properly with your AI applications? Everybody knows that there's this little thing called agents that takes it up a level of the other things that you need to consider with you of AI security. You now have memory, you have tools, you have goals, you have additional knowledge bases, but instead of just doing a query in response to get an answer back, now you have an agent that is actually observing exactly what's happening, making decisions for you, taking actions that is not just calling a knowledge base, calling a tool. But actually performing something that changes the underlying uh data or actually takes an action like I'm booking a flight or other things and so there's a lot of things that go into play when you are using a large language model to make sure that security is in place so you are uh building that application properly so you're not having a lot of those security concerns. Who here has seen Mader Atlas? Everybody's seen Mader Attack, but who's here has seen Mader Atlas? All right. So Mader Atlas is something that's came out that's specific for AI, not just Jinter of AI, but AI overall, and the overall goal of what they have with Mader Atlas is what are the things that you need to consider when building Jr of AI workloads to make sure that you have the right security in place. They talk about things with prompt injection. They talk about things with, uh, model, uh, data, uh, training poisoning and other things like that so you can make sure that you're thinking about the right things when you're building those workloads, right? So when I talked to customers about building jitter of AI workloads, one of the things I kind of take a step back to talk to them about is what is different with Jit of AI workloads compared to traditional workloads. And so if you think about how you actually interact with things with traditional workloads, you either do data access or you do API access. And so when you have data access, what are the expectations that you have with data access? You have structured outputs. You call it API, you're going to give it a certain JSON structure, you expect a certain structure back. With Ger of AI you're using natural language and so the way that someone asks a question could be very different from one user to another and the question comes up of, OK, if I'm actually wanting to put security with this, how do I actually put security when the actual questions that someone has with natural language could change over time. Deterministic responses you call a database unless that underlying database changes, you're going to get the same response back. But with Ger of AI, depending on how the user asks the question, whether you have a temperature on a geneive AI model set in a certain way, those responses could come back different on how many people have asked in LOM how many R's are in strawberry. Anybody ever ask that one? Try it, try it out across multiple different LOMs. Sometimes you get the answer too. The answer is actually 3, and depending on how you ask the question, it can come back with different responses which makes it nondeterministic. And then one of the biggest ones from a security perspective is when you're making API calls when you're making uh database calls and other things, you're authorizing a user to the data, right? Whether it's a row, whether it's a table, whether it's an object that exists in, uh, some type of S3 bucket or other things like that. Withitative AI when you send data. To a model or you send a prompt to a model, you're authorizing access to the model, not the data that exists in the model that the mindset that I always tell customers to think about is when you're sending data to a model, any data that goes to the model, you can expect the customer to get it back some way somehow. So whether that's a prompt injection or other things, so a user is not allowed or not authorized to have access to certain data you're sending to a model, don't send it to the model in the first place. OK, so, uh, back in 20 beginning of 2024 we came out with something called the Generative AI Security scoping matrix. Who's seen this before? All right, same person is raising his hand like every time, so, um, so hopefully this is, uh, good for other people. So the scoping matrix is something from a security perspective of how to think about interacting with generative AI depending on what type of model you're using. Are you actually using a model that's a public model. So for example anthropic cloud or like meta's AI or other things like that, that the types of regulation governance, the types of data that you're actually sending to the model, you have to be concerned about what is uh the security that uh actually you sending to the model compared to I'm uh using an ISD solution. That ISV solution has geneive AI where I can ask natural language queries about uh the data that exists in my ISB solution. You have to start asking questions of where is this ISV storing my data? How is the ISP using that data and then you get into Scopes 3 and scopes 4 where you're starting to build things with Amazon Bedrock. So Amazon Bedrock is a managed service from AWS. We, we host LOMs for you where you can call API calls to ask questions from the LOM and then you have to think about who has. Access to this model what models am I using? What data from like a knowledge base or other things am I gonna put into this? What security mechanisms do I put in place to make sure only users that are authorized to those knowledge bases are authorized for those tools come into play and then. SO 5, you train your own model. What types of data are you actually putting into the model to train? How do you make sure that this is a public model that you want to give out to your users that you're only allowing uh data that is not sensitive data being trained by the model. So you have to think, where are the data sources. How do we make sure that someone's not gonna be malicious and actually put uh data that shouldn't be in those data sources before training the model and so this all goes into play when you're thinking about how to provide security with generative AI of how your users are using it, how your employees are using it, what types of policies you have in place to make sure that you're using it properly by having the right security constructs in place. Hopefully everybody's seen this slot. Um, everybody understand what defense in depth architectures are. No one understands. Yeah, a couple of hands, maybe. Yes, I have one hand. That's all I'm looking for. So just like with traditional workloads, you need to build defense in depth architectures with gene of AI and what do I mean by that? That when you think about it from a foundational layer, getting back to the scoping matrix, do you want your users using public AI models? Do you want your users using sensitive data when actually uh putting data into that generative AI model? What do you want your users doing when interacting with internal applications or external applications and other things like that? Most of the challenges that customers have is they don't have those policies and procedures in place to know one, what your users are doing, but 2, also how to uh make sure you educate your users of how to use it properly. Then you get into things like network protection and infrastructure protection. I'm building my own gene of AI application. Do I need to put some type of la to do input validation? Do I need to put additional security in place of who gets access to that model and other things like that? You have things like identity and access management. Is any user authorized to access a model that I'm building? Or am I only gonna let finance department get access to certain data sources or the uh sales department get access to certain data sources and other things like that and that's the same thing that you do with traditional workloads, but now you're applying it to generative AI applications. It is not until you get to the application protection and the data protection that you start thinking about things like prompt injections. You start thinking about things like jailbreaking and all those malicious things that you uh read about out on the internet of what can happen when you're building gene of AI applications and how do you protect against it. And so from a security perspective, these are traditional security controls that you put in place to make sure that you're building the workload securely. You can't ask an LA. to say pretty please don't provide sensitive data to this user because they're not authorized for it. You put the right security controls in place saying how do I pass identity information to my application or how do I make sure that this knowledge base is only allowed for this user compared to this knowledge base only allowed for this other user. From a partnership perspective, one of the things that we came out with uh at AWS uh back reinvent last year was the security comps and new category for AI security. Security company, what it does is it validates uh different partners meeting certain bar from AWS and so for ISB solutions we have partners that do AI data security, AI security posture management, threat detection response in order to provide solutions for you, uh, in order to uh make sure that you're data secure and you're actually building secure gene of AI solutions. On a consulting partner perspective, we also have partners that actually operate in this space of being able to go to a partner and say I'm looking to build security around my workload, being able to talk about it holistically, not just a partner that says I know how to prevent prompt injections by deploying this ISD solution or I know how to set up an IAM role between your front end application. And bedrock that we want partners that are validated that can say I can talk about holistically to make sure that you're putting the right security in place and safety in place to make sure that you build it so you're not the next news cycle of I just gave a 100% discount to an airline or for a customer who's booking an airline flight because I didn't have the right security in place, all right. This market is changing very fast, as everybody knows. So what's the new thing that's coming out? Agents Has anybody even messed with agents yet? I'd be very OK, I, I saw one little hand, but I've already talked to you about agents that there's a lot of things that are coming out with generative AI that are going to be new. You as an organization might not touch it for the next 1 or 2 years or other things like that, but when you think about it from a security practitioner's perspective, you have to think of how does this change the way that I need to provide security because it's not just you put an L and firewall in place and you're good to go. What is the authorization for an agent where um I want to contact this MCP server or a third party tool? Do I pass user identity to that agent? Do I, uh, does the agent pass agent identity to this third party agent? All those things come into play that you have to think about it just like you do with traditional, uh, security controls of making sure that you're providing the right security. You're thinking about it just like you do with traditional workloads, but also understand that you have an LOM in place that's a nondeterministic function. That's almost like a black box and how to actually build security around using large language models using generative AI with the new things that come into play because it's not going to stop whether it's with agents or what's the next thing after agents with anything with security, but the conversation I have with a lot of my customers, a lot of my partners is making sure that just like you put determinate the controls in for security for uh traditional workloads, you're doing the same thing with your AI. Alright, so happy to have additional conversations if you wanna come up and talk about this, uh, but that's what I have for this session and thank you very much.
