# AWS re:Inforce 2025 - Integrate Zero Trust into your cloud network (NIS304)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=AMSkou99Fus)

## Video Information
- **Author:** AWS Events
- **Duration:** 54.2 minutes
- **Word Count:** 10,393 words
- **Publish Date:** 20250620
- **Video ID:** AMSkou99Fus

## Summary
This session demonstrates how to integrate Zero Trust architecture into existing AWS cloud networks without requiring a complete infrastructure overhaul. The presenter walks through two main implementation patterns: client-to-service access using AWS Verified Access (replacing traditional VPNs) and service-to-service communication using VPC Lattice (replacing firewall-centric east-west traffic controls). The session emphasizes that Zero Trust is not an all-or-nothing approach - organizations can adopt these solutions incrementally alongside existing security controls like firewalls and VPNs where needed.

## Key Points
- **Zero Trust Definition**: Not a specific architecture or service, but a set of mechanisms that go beyond network-centric controls to integrate identity and other contextual information
- **Two Implementation Patterns**:
  - Client-to-service with AWS Verified Access for secure remote access without VPNs
  - Service-to-service with VPC Lattice for east-west traffic within cloud environments
- **Migration Strategy**: DNS-based migration allows incremental adoption without disrupting existing workloads
- **Policy Architecture**: Two-tier approach with broad policies at the network/group level and granular policies at the application/service level
- **Continuous Authentication**: Real-time evaluation of each request rather than session-based authentication
- **Complementary Approach**: Zero Trust solutions work alongside existing firewalls, VPNs, and security controls rather than replacing them entirely
- **Operational Benefits**: Reduces firewall administrator overhead by allowing application teams to manage granular access controls

## Technical Details
- **AWS Verified Access Components**:
  - Verified Access Instances (containers for organizing components)
  - Trust Providers (identity via OpenID Connect/IAM Identity Center, device via MDM solutions)
  - Groups and Policies (written in Cedar policy language)
  - Endpoints (ALB, NLB, ENI, network CIDR, RDS instances)
  - Requires browser plugins for device trust and connectivity client for TCP resources

- **VPC Lattice Components**:
  - Service Networks (communication construct between services/resources)
  - Services (Layer 7 proxy functionality similar to ALB)
  - Resources (Layer 4 managed NAT for TCP resources like databases)
  - Auth Policies (IAM policy documents requiring SigV4 authentication)
  - Two connectivity options: Service Network Associations (local VPC association) vs Service Network Endpoints (private link style)

- **Security Controls**:
  - Security group referencing for identity-based network access
  - Cedar policy language for fine-grained permissions
  - SigV4 authentication for continuous validation
  - Access logging with detailed context including identity, device posture, and application-layer information
  - Integration with WAF for additional protection

- **Migration Patterns**:
  - DNS-based migration using Route 53 private hosted zones
  - Split-tunnel VPN configuration for coexistence
  - Per-VPC or per-service migration granularity
  - Cross-account sharing via AWS Resource Access Manager (RAM)

## Full Transcript

All right, so welcome to NS 304. My name is Dave Derico. I'm a networking specialist here at AWS Enterprise Support, and today we're gonna talk about integrating Xero trust into your network architecture. So we'll cover a few things here. First, I'm gonna give you an overview of Xero Trust, and then I'll walk through some of the network centric security controls that we all know and love, and this is gonna be very familiar to you, and I did this very intentionally because I want to talk about what we're already doing today so we can think about how we integrate Xero Trust into that. So from there we'll look at implementation and migration patterns for zero trust starting with client to service with verified access and then going with service to service with DBC lattice and then along the way and at the end we'll summarize with some best practices and considerations. So the obvious question is what is zero trust? There's a definition on the screen here. I'm not gonna read it word for word, but essentially the idea comes down to zero trust is not a specific architecture. It's not a specific service. It's a set of mechanisms that allows you to go beyond network centric controls to integrate other sources of information as well. So we have this notion of identity centric. And network center controls and we want to use them in a complementary fashion together. Now from network center controls, this is what we all know and love, right? We've got a VPC, we've got security groups on our resources, and we've got ACLs on a subnet level. Now these are somewhat different. Security groups are stateful. Network ACLs are stateless, but fundamentally these are both going to rely on the same kind of information. They're going to rely on a source IP or site or block, a destination IP or sci or block, protocol port, and then some kind of action, whether that's an explicit action like you do in a network ACL or an implicit action like you do in a security group where that's an allow. Now, this all makes sense we've been dealing with this for quite a while, but this is just within the scope of a VPC. Most of the time we're not operating a single VPC, we have multiple VPCs and they all need to have some form of connectivity between each other. So we'll do something like have our data center here too and we need to integrate that in. So we'll put in something like a transit gateway. And then add in direct connect for that. So our traffic between VPCs goes via the transit gateway as well as to on-premises. Now for clients who are outside of that, we could connect via a third party VPN again via Direct Connect, or we can go directly to a public facing workload like our public facing ALB. Now at this point we might decide we want to implement a firewall as well. So we'll create an inspection VPC like the one that's on the screen here, and we'll deploy a firewall in line and we'll route all our traffic through it. Now I want to pause here and make a distinction between two different use cases that we typically see people using firewalls for. The first is just straight access control, and you can think of this as your firewall acting like a souped up network ACL that's just centralized in one place where you're just doing straight access control. You're allowing and blocking traffic purely based on network ciders. Now you might also be using this since this is your central blast point for logging and visibility. Makes sense. All of your traffic goes through here. It's a good place to look at it. And so for access control we're gonna again rely on the same kind of information we saw with security groups again source IP, destination IP port protocol, and some kind of action. So that's access control. Now I want to make the distinction again between that and what I would think of as inspection, and that's where you're doing things like deep packet inspection where you break apart a packet, reassemble it, you've done your inspection, and then you send it along its way. There's a distinction here and we'll come back to this further end and this will also include things like data loss prevention or if you have mandates where you need to have that in place. So we'll insert that into our architecture. So now traffic between our VPCs goes through our firewall as well as to on-premises, and we can even do things like centralize our egress. Now we've introduced some operational challenges. So if this app team. Wants to talk to this database. Well, now that has to go through a firewall, which means we now have to involve our firewall administrator. How many of you are firewall administrators or manage firewall rules? None of you lucky. OK, so what that means is if you've never been in that position is that as the firewall administrator you now have to arbitrate every request that goes through a VPC and you have to decide is this allowed, is this not allowed, and we add operational overhead. Now in this case we'll say, OK, sure, that's allowed, so we'll let that through. Now that's for between VPCs and on-premises as well, but we want to make some changes to our remote access. So we'll add something like a VPN. Now I'm gonna use client VPN in my examples today, but you can use another VPN. It'll probably work in a similar way. So this is gonna introduce us to the idea of authentication. This is kind of the first time we're gonna be really talking about identity and so we can authenticate against active directory or SAML as well as using certificate based authentication, and then I'll be subject to a session time out so I authenticate once and then I don't have to do it again until that session time out occurs. This is also where we'll bring in the idea of authorization and authorization being in this case, if I'm a member of a particular group, I can access a given network cider. So we can do that on a VPN. We've got some other controls here as well like client route enforcement to prevent us from, you know, leaking our routes out locally. We can also do this on an ALB. We can implement authentication there, so when our client tries to connect, if it doesn't have the right session cookie, it'll go through that authentication process. But this is also still subject to that idea of a session time out, right? I authenticate once and I don't have to do it again until that session time out occurs. So we'll add this in now our clients can either connect directly to the ALB and authenticate there, or they can go via the VPN optionally passing through our firewall, etc. So how many of you recognize this architecture? Show of hands real quick. OK, how many of you have this architecture? OK, about the same number of people. So we're all starting from the same place. This is gonna be our starting point today from because from here. We realized that we're working with just network centric controls they're operating at a network perimeter and that can be at the VPC edge that can be at the Internet edge, etc. Our access is based on network ciders because for the most part my IP address is my identity. We're evaluating access once through authentication until that session time out occurs. From a flow perspective we only are gonna get visibility at the flow level we don't have visibility into what's happening at the application layer a lot of times. And finally there's that operational overhead I mentioned earlier where as a firewall administrator you now have to be responsible for making sure that everything that goes through is actually allowed. So thinking back to this, how can we actually use identity here? And what does identity mean? So let me give you an example. Security group referencing. Security Group A allows traffic from B and vice versa. Now in the services doesn't look too different from what we had before, except now group membership becomes a relevant property in determining whether or not you can actually allow a particular flow. So I'll create these rules and as I add resources. Or as I delete resources, I don't have to do anything else because the group membership is what's important here. Now this isn't a single VPC, but I can also do this across VPCs. So I could do this via VPC peering connection as well as via either a transit gateway or as of last week, cloud win as well. Come back. Now with this there are a couple of considerations, you know, you won't be able to do this if you have, you know, a firewall in the path between, you know, BBCA and B and your transit gateway and your cloud win, but this is still closer to what we want. This is zero trust adjacent because again we're using our security group membership as a form of identity. So ultimately we want to be able to operate at the network and application perimeter. We want our access to be based not just on network but also identity. We want logging and visibility of our application requests. We want to continuously evaluate access for real time instead of just authentic once. And we want to reduce that operational overhead wherever possible so we can make set the broad boundaries and then if our application teams need to have more specific controls they can put those into place. Now there's a lot Of zero trust use cases we could be talking all day about every single one of these use cases. Now, as I mentioned earlier, today we're just gonna focus on 2. Our goal by the end of this session is we're gonna have well architected a zero trust model that focuses on these two services and we'll integrate them into that architecture we've already looked at. We'll start with client to service with verified access and then we'll move to service to service with DBC lattice. So with verified access, we'll start off here this will provide a secure access to our applications without requiring the use of a VPN. So to do that we'll get a couple of things here. We'll evaluate each request in real time instead of just authenticating once like we saw before with a VPN or an AOB. We'll be able to log all of our access attempts whether they're allowed, whether they're not allowed, and we'll have visibility into why they are allowed or not allowed specifically. And if we're using existing identity or device trust providers, we can integrate that in. With verified access. So let's walk through how this works. So I'll start with a simple example here. The first thing we would need to do is create a verified access instance, and you can think of an instance as a container that organizes all of the various verified access components. It's also where we'll configure logging. As well as configure and integration with WAF as well we'll have trust different trust providers here. Now we can have identity trust providers and device trust providers. Now with identity that can include things like Open ID Connect as well as I am Identity Center, yep, back up here, as well as device trust providers like, you know, if you're using a device management solution likeAM or CrowdStrike or something like that. Now the number and type of trust providers you use will influence how many verified access instances you have. I'm only using one here, but if you need to have multiple identity providers, you would need to have separate instances for those. Every instance will have a single identity provider, but you can have additional device providers as well. Now with the trust providers configured, we can move on to setting up groups and policies. Now groups are essentially exactly what you would think they are. They're applications where we've grouped them together based on a common security need so all applications in that group will share a group policy. And our apps can also have an apple policy, so we really have this idea of two different. This is ridiculous. So we have this idea of broader policies at the group level and then at the application level or at the endpoint level, and I'll talk about end points in a moment. We have more specific or more granular policies there as well. Now about these policies. You can think of these policies they're written in a language called Cedar. This is a policy-based language that allows you to do things like express fine grain permissions as easy to understand policies in your application, and this will be how we define the rules for accessing our applications. And so I've got an example policy on the screen here. I've got my effect. My scope My condition clause and this information. Will be evaluated against my trust data so let me give you an example of how this works so my trust data will come from my providers and I'll evaluate this against my policy. So here I've got an Ava policy and this can be attached either to my instance or to the application. And now I've got an example trust claim that's being sent to Ava, and this will include identity information as well as information from my device providers. So now I've got this information and Abe is going to go through and evaluate this claim against the policy that I've set up here. I'm not gonna go through it line by line, but you can see as we go through everything line is going to be evaluated against the information that was provided in our trust claim. And if it's all good, Ava allows the traffic to pass through. In this case, the claim meets the policy requirements, so we're good to go. Now take a quick moment and notice this there's nothing in here that involves IP addresses or ports or protocols. We don't need that. We can certainly add that into our policies. That is something we can include if we wanted to, but it's not a requirement here. We've moved beyond that so now we're using identity as well as device posture as our main evaluation criteria for whether or not we're allowing something through. So with these groups and policies defined, we can now go ahead and create endpoints. Now I'm using ALB as an example on the screen here, but we can have multiple different endpoint types we can use an ALB, a network load balancer, an elastic network interface, as well as either a network cider or an Amazon RDS instance. Now these end points will support different protocols, so. If I'm going to do HTP HTPS, I'll need to have a domain name and if I'm doing HTTPS that will require an SSL certificate as well, but I can do that on those top three, the two load balancers in my network interface. If I wanna just use TCP though I can do that across any of them and that'll be supported on all endpoint types. This is a good point to stop and think about how this would work from a multi-count perspective as well because a lot of times we're working in a multi-count environment. Stop that. So from a multi-count perspective, I can create my groups and my policy. And I can share those groups and policies out using Resource Access Manager to my spoke accounts and then from there I can use those groups and policies to create the individual endpoints and apply those policies at the endpoint level as well. So at this point, I've set up a lot of stuff inside the cloud, but I also need to configure my clients. So if I'm going to use device trust context here, I'll need to have a browser plug in, and that could be a browser plugin that we provide or one that comes from whoever your device trust provider is. Additionally, if you're going to connect to TCP resources like the ones I just showed you earlier, you will need to have the connectivity client installed there as well because the connectivity client is going to install and configure that secure tunnel and ensure that that user office continuously performed like we were talking about earlier. And the last note here would just be we need to update our DNS. So for my HDPS app one, I'm going to create a C name record to my verified access endpoint, and that's an FQDN that Ava generates for me automatically, and I'll just see name that as long as I set up my domain previously again if this is an HTTP service, I'm good to go and I can connect. Now from a flow perspective. First, my client will connect to Ava. And then I will use the data from my trust providers whether those again are identity or device and I'll add that to the context that'll be passed into verified access and I'll also again if I have a WAF configured, evaluate that against a WAF policy as well. Now a quick call out here where WAFF is evaluated or where that wa, you know, evaluation occurs will depend on, stop that, whether or not I'm using IIM identity Center. If that happens then. Waffle spec before authentication if I'm using something like OIDC that'll happen after authentication just so just keep that in mind. Now we also need to think about the network level because we still have our security groups here in play. When I created my verified access endpoint that creates a network interface in the VPC and so that network interface will have a security group associated with it so we can allow inbound and outbound traffic from that security group as well as to our ALB. We can allow inbound traffic from that verified access and point security group again, we're gonna use referencing here. So Authenticate successfully verified access signs sends the signed users' claims. But now I'm going to use that against my policy, so I evaluate my policy at the group level. As well as at the endpoint level. And only if both policies are evaluated will this actually complete successfully. Now from a logging invisibility perspective, verified access will provide us with access logs, detailed access logs like the ones you see on the screen here, and this will include identity details which include our allowed denied decisions as well as the HDP data that we're using as part of this request and we can even include things like the full trust context in our logs. So if we want to see all of the data that's being provided from our trust providers in the logs, we can do that. And the nice thing about that is we can use these to build our off policies. So remember at the beginning I showed an architecture. Let's see what that looks like now. So I've added verified access and it's important to know this just affects one application. That's my path for this particular application here, but everything else still requires the VPN. The rest of my traffic flows are not going to be affected as well. Now I could stop here But what about my other workloads? Let's take a look at those as well. So first we have a public facing load balancer that's, you know, reachable today through the internet. Well, the first thing we're gonna run into is that for verified access to work with this, we need to have this be a private load balancer. So from there, We'll need to create a private low bouncer and add that into our design here, add that into our architecture. Now as before, we'll go ahead and create our groups and policies we can also share them out remember if this is a multi-count approach. We'll create our end points and policies and remember we have those policies both at the group level as well as at the endpoint level. And if we're going to set up an HTTP or HTPS service, we'll need to specify the domain name on that end point. And then we'll update the DNS record for my HTPS app 1 with the C name to the verified access endpoint. Now at this point we can actually even just remove that public load balancer which will leave verified access as the only public facing path to access this application. Now let's talk about network cider and RDS end points too. I've deliberately saved these for last for two reasons one, they're TCP resources, but two, they have a lot of similarities in setups. So that's why we're just kind of kind of talk about them together. So the first step, similar to any other resource we create our, we create our groups and policies we can optionally share them out. Now, for here, for a network cider. What I would need to do is I can configure a custom subdomain and when I do that I'm essentially delegating a subdomain for Eva to manage on my behalf and I'll see, I'll show you why that's important in a moment. So here I'm gonna use Avadexample.com. Now we'll go and create our end points and policies. As we're creating the end points and policies, we can configure an endpoint domain prefix, and this is an optional thing, but what we can do is this will essentially be a customer identifier that verified access will prepin to the DNS name generated for the endpoint. So for an RDS database I might choose something like my RDSDB. And for you know a TCP app I will creatively call it my TCP app. So we've got that. Let's start by looking at our TCP workload. So remember that endpoint domain prefix in the custom subdomain? Well, Ava will use those to generate unique FQDNs per resource which we can then use to access our resources. So if I want to SSH into those instances, I can do so using that FQDN. So as resources are added, new domain names are added, and as resources are removed, they'll be deleted, and that's how I will access these resources in the network site or range so it'll scale up and add those FQDNs for me and then bring them back down as it needs to. I don't need to manage that individually. Now for an RDS, the endpoint domain prefix we specified will be pre-pended to the DNS name that verified access generates for the endpoint. And again, I can use that FQDN to access my RDS database using the same tools I would as if I were on a VPN. Now you might be thinking at this point, OK, that's cool, but How do I migrate to this? I've got a VPN today. How do I move to this? So the good news is that you can use both of these together and the way we'll do this is all about DNS. So to start, let's take a quick moment and look at how a VPN works by itself. So with client VPN at least, and yours might be similar if you're using something different, you specify a DNS server that your clients should use when connected, and this could be something like a Route 53 resolver endpoint, and then you'll have a private hosted zone associated with that VPC where the endpoint lives, and that's how your DNS resolution will work. So when I go to query at a.example.com. That will go to here my private hosted zone for resolution. And then my traffic will pass through in this case to an AOB private IP via my VPN into my transit gateway my firewall, etc. so this is private DNS. Now, I could also not specify a DNS server and if I do that. Then I will use whatever my local public DNS resolver is. So in this case I'll query APA again. I'll go to a public hosted zone that has, you know, the records for example.com here. Once again my traffic will go through over my VPA. So let's go ahead and add verified access. Now if I'm gonna do this, there's two things we need to keep in mind first. One is if you're going to use verified access alongside VPN, you need to configure split tunnel, and the reason for that is you want to avoid the situation where traffic to verified access is traversing your VPN, going out whatever egress path you have, and then looping back around to get to verified access. That kind of defeats the point. The other thing we're gonna do is again remember that if you're going to use device data you'll need the browser plugin. If you're going to connect to TCP resources you'll need the connectivity client as well. So let's start by on boarding a new app. We'll call it APC. So if we're gonna use private DNS, we'll need to add an entry for AC.example.com here with a C name to the AVA endpoint. So when my client is connected to the VPN. They'll use that same process as before to resolve that DNS name. And now my traffic will pass over verified access again because we have split tunnel enabled we can take separate paths there even though I'm still using my VPN for DNS. Now when I'm off the VPN or if I don't have a DNS server configured. We'll still use the public hosted zone, but again we'll need a public hosted zone here with that same CA entry pointing to the AVA endpoint. The client will use that resolver to resolve A C, which will point to the AVA endpoint. At which point we're connected to A C. Now the thing to keep in mind is that none of these approaches are going to change App A or A B. They still require VPN. We've only changed A C. Let's go ahead and change that. So let's say we want to migrate A A. Now assuming we have everything else in place again, this is just a DNS update we're changing at AIexample.com. We're changing appa.example.com to point to our verified access endpoint. And so now we'll use that to point to our app A. And similarly, when we're off the VPN or if we're using just public DNS, we'll have to have the same thing a record in our public hosted zone for AAIexample.com pointing to that Ava endpoint. So we'll go to query at aexample.com and now we'll use that to access both App A and A C. And we can repeat this process as many times as we need until we're just eventually using verified access. So let's go back to this architecture at this point our migration is complete, and we can still keep our VPN alongside for any workloads or clients that might still need it or we can get rid of client VPN altogether if there's no longer a use case for it. So we've successfully implemented zero trust for our remote access requirements, but everything else, all of our other flows remain unchanged, and we can do this in an incremental process. It's not all one or all the other. So just to summarize what we've learned here. So first remember that if you're going to have multiple trust providers, if you need separate identity providers for those, that's going to be separate instances if it's all the same identity provider, you can use the same instance. Remember you can group endpoints with common security requirements into common groups and then you also have the flexibility to apply policies both at the group level and at the application level. If you're going to use this alongside verified access and VPN together, you need to make sure you're enabling split tunnel on the VPN. And finally, remember that this might not be the right fit for every workload, right? You know, I mentioned TCP and things like that. If you have something that requires UDP, it might not make sense to migrate that up. It might not be supported with verified access, in which case, maintain the VPN for workloads that need it. So, That was client to service. Let's shift gears and talk about service to service with VPC Lattice. So with VPC Lattice this allows us to take those same zero trust concepts that we could illustrated with verified access and now we're gonna map some of those to service to service networking as well or EastWest networking if we're thinking about it from a VPC perspective. So we can do this with a couple of ways. First, Lattice is gonna give us access controls through off policies. It'll work across multiple different compute types like you can see on the screen as well as TCP resources again similar to verified access. For layer 7 services we'll have the ability to do some advanced routing controls similar to what we do with an ALB, so you know things like load balancing, routing, weighted targets, etc. and finally from a logging and visibility perspective again we'll also have access logs which will give us that visibility that we want to see. So as before, we're gonna take a look at each of these components and turn and see how they all fit together. So first, We have a lattice service network and that service network is, as the name implies, gonna be the construct that allows us to communicate between services and resources. We'll configure access logging on here and we'll talk more about that in a little bit. Now typically I'll only have a single service network, but I could also have multiple service networks and there's some use cases for that which we'll discuss more in a little bit when we talk about how we actually connect this all together. Now one call out here as we're going through this, I've grouped these into a providers providers on one side. And consumers on the other. So with that this is kind of implying a binary here. I've made this choice very deliberately because I want to illustrate the different flows but know as we're going through here that a VPC can be both a provider and a consumer of services and resources. I'm just separating them out for the ease of visibility here. I keep talking about services and resources. What are those? Let's start with services. So a service is essentially going to operate like a layer 7 proxy, and you can think of that just as running in front of your different compute, whether that's running on instances, containers, serverless compute, and that'll consist of listeners. I've got an HTTPS listener here. You could also do HTTP. You could do TLS pass through rules and then target groups where we can do things like, you know, weighted routing, etc. Now if you're thinking. This looks a lot like an ALB. Well, yes, it is. These are gonna operate exactly the same way as an ALB does where instead lattice is now providing that proxy and load balancing functionality instead, and what this means is we can set our targets directly on the compute or we could use an ALB instead if we just want to shift it to the ALB so we have some flexibility here. So for my case I'm gonna go ahead and just kind of bypass the ALB and I'll make these you know these compute services my service here. So I need to make them accessible to the service network, so I'll create a service association. And now that'll be accessible, but Slight problem Got this database And this database is not gonna operate on HTTP HTTPS. Because remember these services are operating at layer 7, so for this I'm gonna need a different construct and that'll be a VPC resource. So resources operate at layer 4. There's no proxy involved. Instead we're using essentially managed nat and port forwarding via lattice to pass traffic through lattice. And to define our resources, we'll need a resource gateway. And we'll create a security group and associated with that and this will be in the VPC that has the resources that we want to make accessible through our service network. So we'll have an identifier which will be either a publicly resolvable DNS name, a private IP address, or an ARN, and today we support accessing an RDS instance or cluster through an ARN. We've got our protocol Our port And then we're good to go. We can specify any of these ports here accessible via TCP. Now we can now we can also define these as either a single configuration or as a group configuration with child resources and the common example of this would be something like an RDS cluster where we've got maybe multiple readers and a writer node. This would be set up as a group resource configuration that we can then access. Now this is also a good segue to how we talk about cross accounts. So there's two kinds of approaches here that you can think about. The first is gonna be a shared service network, and this will be probably similar to what you're doing today if you've got like a transit gateway or a cloud core network where you've got a networking account that has your constructs, and then you've got all your spoke accounts. So we'll do something similar here where we've got a lattice service network and our services and resources on the side. And we'll use RAM Resource Access Manager to share out the service network to those accounts and then to do the associations there. That's one approach. Now the other approach is where we go in the opposite direction. So again, same constructs, we have our service network, our services and resources and the spoke accounts, but here. We can use Resource Access Manager in the spoke accounts and share those into an account that has the service network and do the association there. So functionally what that means is we can take either that centralized approach or a more distributed approach and if we want to have multiple service networks, this allows us the flexibility to do that because we can associate our services and resources with multiple service networks. So moving right along, we'll add our resource gateway and our resource configuration to the architecture and we'll associate that with the service networks and now that will be discoverable just like our service was. So now we need to think about how are we actually going to control access to these. Now for services, I want to start with services in particular because this gives us the ability to use something called off policies. Now all policies are essentially IAM policy documents that will attach to either the service network. Or the service itself, so here I've got back up here we've got our service network with a broad policy applied to it and generally speaking we're going to apply broad policies at the service network because this is going to encompass everything that traverses through the service network. So in the example you can see on the screen here, the criteria is if I'm a member of this organization I can access this service network again deliberately very broad we're setting the outer boundaries of what we want to be considered acceptable. Now at the service level this is where it can be much more granular. I can do things for example like saying like in this policy I only am going to allow this particular caller access to this particular service if they're performing a get if I try to do a post or a put that won't be allowed. Now in order to ensure that I can do this kind of stuff where I'm adding this identity information and I'm relying on caller identities, I need to sign any requests that go into Lattice with SIGV4. Now if you're not familiar with SIB4, this is a super high level overview. There's a lot of resources that talk about it in more depth, but essentially this is a protocol we'll use for authenticating requests into AWS services and in this case Lattice in particular. Now what this will allow us to do is verify identity using a signature that we create from our valid AWS access key, so the secret key ID, the access key ID, all of that stuff that you would use for using the CLI or something like that. Those credentials will be used and we'll calculate a request signature using that which will protect our data in transit, so we have to make sure that that data has not been tampered with. And we'll also have a 15 minute window to prevent against replay attacks when we're using SIGB4 and our keys that we're using at the very beginning to start this whole process are continuously validated. So that's how we're getting that continuous authentication that we were trying to get to earlier. Now if we want to do something like that with this, we will require authentication here because in this case this particular policy we won't have that org ID information unless we're using SIGV4. Alternatively we could allow anonymous or unauthenticated requests like so and we can define that at the service network level whether we're saying we want AWSIAM to be required or we want to have, you know, anonymous requests allowed as well. So at this point, we've got everything built. How do our VPCs actually connect to this? Well, we've got two options here. The first option is called a service network association and this will behave. This will behave similarly to if you have something like a like a S3 gateway endpoint or a virtual private gateway associated with your VPC. It's essentially a local association to your VPC. It'll have a unique host name per service and per resource defined there. And then we'll have IP addresses when we go to resolve the DNS names that are coming from these services because Lattice will give us DNS names for each service and resource that we have. Those IP addresses will resolve to IPs from the lattice prefix lists for both services and resources. So I'll give you an example. I've got service IPs, those will come from that 169254171 address range. And then for resource IPs these will come again from another non-routable space 129, 224/17. So when I go to resolve DNS and these are part of a managed prefix list I should call out as well that you can use insecurity groups if you're doing things like security group referencing. So in this case, if my client BPC goes to resolve the MyService FQDN generated by Lattice, that'll resolve to a non-routable 169254 address and this is similar if you've ever used like the EC2 instance metadata or you know the NTP server. It's using that same construct there. Similarly for resources. This will resolve to unique IP address per resource. So here I've got a 129224 address and that will be unique per resource that I have in my service network. So this is an association. Our second option It's going to be a service network endpoint. Now these will operate like private link endpoints and they'll consume IP addresses within the VPC, and this will be available for the entirety of the service network. It's not a 1 to 1 mapping of an endpoint to a particular resource. So each service and resource will have a unique host name and IP address. And those IP addresses will now come from the client VPC again similar to what you would have with a private link endpoint. So if I do have this in client VPC at the bottom there, that site or block is 102. So when I go to resolve my VPC or my DNS, excuse me, for the service, I'll see it comes to an address from that VPC. And same for that resource as well. Now there is a third option here which I'm not gonna talk about in great depth where if you essentially want to bypass lattice altogether and just have resources be available in a 1 to 1 mapping more similar to what you would have with Privatein, that is an option. I'm not going to talk about it in depth today, but if you want to know more about that, we had a great reinvent session this past year that talks more about that in depth. So if you're curious about that, I would encourage you to check that out. And then we'll go ahead and move on. So at this point we have these two options which one do you choose? There's some specific use cases where you would want to use endpoints instead of the Service Network Association because with the Service Network Association I can only have one per VPC. So if I needed to access multiple service networks, I won't be able to do that with the Service Network Association. Now I mentioned this earlier, why would you want to have multiple service networks associated with the VPC? Well, you could have a scenario like if you're connecting to multiple third parties via Lattice, and they have separate service networks, but you need to consume them all from a single VPC. So instead what we can do is we could have a service network endpoint alongside our service network association and we can access multiple service networks that way. Alternatively we could take a different approach where we have multiple service network end points in a single VPC, and we're using DNS to connect into those separate service networks. Another common scenario that you might think about here would be a shared services approach where if you need to have a VPC that's reaching out to multiple different environments like let's say you have a service network for Dev, QA for prod, you can do this here with multiple service network endpoints. Now another use case would be connectivity from on premises. So remember that these were non-routable address spaces, but if we have connectivity from on-premises via our transit VI or via a private VI or maybe a site to site VPN, we can now reach service network end points because those will use IP addresses that are routable within the VPC instead of that non-routable space we saw before. So we could even throw a firewall endpoint in there if we're using a private VIF and a VPN connection to a virtual private gateway and if we want to sign a request from on premises we could use IAM rolls anywhere to ensure that we're doing that SIGV4 signing as well. And when we go to resolve DNS that will now resolve to those routable IPs which will be reachable over our hybrid connectivity both for services as well as resources. And again this is a single service network, but the same principle is going to apply with multiple service networks as well. If we wanted to access all of these from on premises we could do that with multiple endpoints in that VPC. Now the other consideration here would be cross region access to lattice services. Now in this case I've got a client VPC in Region A, and I've got a service network in Region B, and I want to access some stuff over there. So what I will do is I'll create a service network endpoint. And I'll need to have some kind of existing connectivity between Region A and Region B and that could be a cross-region peering connection, that could be transit gateways peered together, that could be a cloud when core network. Regardless, that'll be how my VPC can now connect to that endpoint. But this still requires us to have full layer 34 connectivity to that remote region. So alternatively, I could do this via service networks instead. So in Region A, I've got the same set up here, but in Region A I've now created an association from this VPC into a service network and in Region B I'll have my service network endpoint, same as before, but Region A, I'll now create a resource gateway. And I'll create a configuration associated with that and I'll target the DNS name of that end point in the remote region. Now I still need to have some kind of connectivity between my regions again peering transit gateway cloudan, etc. but. I don't have to have all of my VPCs in Region A with that full connectivity here if I just want to do this via service networks this essentially gives me a way to bridge these regional constructs together today for service networks. So just to summarize here, If you're gonna do service network associations, those are gonna be probably kind of what you'll start with, and they'll be for a single service network for VPC. And if you don't have any concerns with IP consumption. But if you need to access multiple service networks or access them from on-premises or outside of a given VPC, that's where you would want to use a service network end point. So we looked at something similar with verified access. What does the flow look like here? How do we think about this from a defensive perspective? Well, the first consideration for security is if there's no service network endpoint or no service network association, so I'll set them up here, but if I backtrack. Neither of these VPCs can access my service network, so automatically I have this connectivity aspect that if they're not attached to the service network, they won't be able to consume anything in that service network. So I'll add them back in. But we also have security groups. Each of these is going to have a security group that we can associate with it, and we can restrict the traffic allowed into our service network with the security groups, and we could even use security group referencing to scope that down even further. So in this example, my client security group is the only resource that's going to be allowed to send traffic into my service network and again I can do that on either of these security groups regardless of which connectivity method I'm choosing here. Now if I'm accessing services on the provider side, I'll have to allow inbound traffic from those VBC lattice prefixes. Like I'll connect here. Now I need to think about my off policies as well because remember for services we have our off policies too, so our off policy will be evaluated at the service network level as well as at the individual service level and only if both of those are allowed through will I pass my traffic through in this case I'm actually targeting an ALB, not the compute, remember you can do both and pass that through here. Now for BBC Resources this is gonna be a little bit different because remember our off policies are not going to be evaluated for. TCP slash VPC resources so we'll still have our security groups on the client side and on the provider side we'll need to allow outbound traffic from our resource gateway to our resources and in this case my target security group I'll need to allow inbound traffic again I can use security group referencing to bring traffic in from that resource gateway security group. So we'll pass through again into our service network. We're not evaluating against the off policies, and we're connecting in that way. So this starts to get a little bit away from that, but we're still zero trusted Jason. We're still using a lot of other constructs here and we'll still have some additional visibility in the logs that we can use to identify this. Let me show you what I mean. So here I've got my access logs and this will all be example of information that you can get for a lattice service. Now notice this is gonna include layer 7 information as well so it'll include things like the request path, the request method, you know, it'll include the service network ARN and the source VPCID. So we've got a lot of additional metadata beyond just a cider block that we would have had previously. And we can use these just like we did with verified access. We can use the information that's in these access logs to then go back and build our off policies either at the service level or at the service network level. So again, same concept applies here. And for VPC resources you'll notice this is a little bit smaller, a little bit less information, but again we're not operating at layer 7. We're not going to have visibility because this might not be an HDP HPS request, but we still have information like our service network ARN. We can see the VPC endpoint IDs that this path followed, as well as the source VPC ARN, as well as things like ports, protocols, etc. Now at this point, We're not going through our firewall. So how do we think about that? How do we reason about moving to a lattice when we're coming away from a firewall and the firewall was previously that point at which we allowed or blocked traffic? How do these constructs map to each other? There's a couple of things here. So remember I don't know 45 minutes ago at this point I mentioned that there was a distinction between access control and inspection that really comes down to what is your firewall doing, right? If you're just doing access control, well, we've seen that we have all policies for services. We can have those additional security groups in place. We have the VPC associations and end points that will control access to those service networks and we can even break out into multiple service networks. So if you're just doing access control. Lattice can still fit in there. If you're doing something like inspection though like where again you need that DPA inspection, you're breaking apart TLS you're inspecting it there, then lattice might not be the right fit and that's fine. And from a logging in visibility perspective we just saw we'll get a lot of that same information from our access logs as well as some additional metadata that we can use. Now the second question we want to ask here is what are our connectivity requirements, because if we think about our transit gateway or cloud win architecture, we previously had full layer 34 connectivity, and then we had to use our firewall to scope that down even further. We had to, we had wide open pipes and we had to constrain them. But now We have to think about is this a bidirectional process or do I need to take a provider consumer approach instead? I might have some VPCs that are providing services and resources but don't need to talk to anything else. I might have consumers that don't need to host any services. I can move away from this, you know, bi-directional process and think about it from more of a what are my actual connectivity requirements. And finally, I do need to go through the task of thinking about what my firewall rules mean. So I've got a firewall rule here, super simple. What do these ciders represent? Are they applications? Are they teams? Are they business units? I need to understand what this actually means and translate from cider block into human and identity so again I can map these to off policies to service networks, etc. So just to summarize here, When you're thinking about this, first remember that you can manage network associations and end points and that will control access to your service networks. Second, remember that you can associate your services and resources to one or more service networks if they need to be accessed through those different networks. Apply your off policies for services and to do that you'll need to set the off type to AWSIAM like we saw earlier and then you can use the appropriate condition keys in those policies so that could be the source VPC owner account, something very broad. At the service network layer. And more granular at the individual service or application level. And finally for visibility that's where access logs as well as things like cloud watch metrics will come into play. So again, how do we migrate? And we wanna take a guess? I'll give you a hint, it's all about DNS once again. So, Here Until we configure DNS, our flows are still gonna pass through the transit gateway and the firewall. So if I've got public DNS here and I'm using public DNS to resolve things like, you know, my AOB, I'll query at a.example.com, those will all still flow through my firewall through my transit gateway because I haven't changed anything. DNS is still controlling which path I take. Now this is public DNS where I'm querying a public hosted zone. The traffic flows to an AOB. Same thing is gonna apply with a private zone and with private DNS I'll have a private hosted zone associated with those DPCs, and they'll use that to resolve DNS and pass traffic to again those private ALB addresses. Now my goal here is I want to migrate client VPCB to VPC Lattice. There's two ways we can do this. Per VPC per domain or per VPC per service one of these is more granular than the other, so let's walk through them. So the first thing I'm gonna do is if I'm using public DNS, if I have a public hosted zone for example.com, and that's how I'm doing DNS today, I'm gonna create a new private hosted zone, for example, .com, and then I'll replicate the records for that public zone into the private zone. Now functionally this is going to give us a split view of DNS, but this is important to have here because we need to avoid impacting the VPCs that we don't want to migrate. We only want to focus on individual VPCs here. So once I do that in my private hosted zone, I'm gonna update that resource record for AAIexample.com to point to now my VPC lattice DNS name. Now I still haven't changed anything yet because this zone is not associated, so I can pre-stage as much of this as I want in advance and as someone who has suffered through migrations before, pre-staging everything as much as you can is super helpful. So I'll stage this in advance and now I'll associate this new private hosted zone with my client B VPC. So when client B goes now it goes to query at a.example.com. That traffic will now pass through our lattice service network. Whereas Clyde VPCA is still using that public hosted zone and is not affected, so I have this ability to change this per VPC if I want to do this. So that was public DNS. Private DNS is going to look pretty similarly. In this case, private hosted zone associated with DPCs A and B. I'll create another new private hosted zone again, for example.com. I'll replicate my records. I'll update that resource record just like I did before and then I'm going to first need to remove the existing private hosted zone association for example.com. And then I'll go ahead and associate that new private hosted zone with client VPCB. So as before, I'll query AAexample.com that resolves the lattice, and that's my path in. And just a quick call out here. I don't know if you all are familiar with Route 53 profiles, but if you're not, if you're going to be managing a lot of, you know, private hosted zones at scale, this is a great way to do that and can make your lattice migration process a lot easier. So that was the first option again there's some variation there depending on which DNS form we were using, but for the second one it's going to be functionally the same whether we're using public or private hosted zone in this case, for example.com as before. We'll create a new private hosted zone. And that we associate that new private hosted zone with VPCs, but the difference here is that this private hosted zone is for AA.example.com instead of just example.com, and we were replicating all of those records. We're being much more granular here. So because we've matched this service specific private hosted zone, once we do that association, Route 53 will now resolve the DNS name for AA.example.com using that more specific zone. And so that will give us the flexibility that we want so when we query A A as before, it's gonna go through lattice. Now this is more granular and the benefit here is that I don't need to remove that existing private hosted zone for example.com if I'm using private DNS. The downside is is that I do need to do this now per service or per resource that I want to migrate to so there could be some tradeoffs here but you can take either approach. So just to summarize what we've talked about. So first off, remember when we're thinking about lattice, we need to consider what our connectivity requirements actually are here. Do these VPCs need access to the service network? If they do, are they providers of resources? Are they consumers of resources? Do we need have need to have multiple service networks? We've got a lot of options here depending on what we want to do. Now remember, as before, we can apply these coarse grain service network policies and the narrow service policies. So what that means is thinking back to our example where we were firewall administrators earlier, now we can set the broad parameters of what's acceptable and then our application teams or our developers can set more granular permissions based on what they know about the applications. I don't need to know all of that. I've set the broad parameters and they can be more constrained with what they want to allow in. Use your access logs and use those access logs to design your off policies. Remember, use that information and work backwards to build your policies. You're going to use authentication, if you're going to incorporate some of the identity data, you will need to integrate SIGV4. And remember, service network endpoints, use them when you need to. Use them when it's appropriate to do so. And remember that lattice is not going to be a replacement for everything if you have that requirement for deep packet inspection or some of those more advanced firewall use cases that we alluded to earlier, keep those in place, nothing wrong with that. You can use these right alongside each other. So at this point, We're using verified access for our remote connectivity into our environment. As well as VPC lattice for service to service to service communication in East West traffic. Now at the same time, we haven't altered our egress flows out to the internet. We still have our traffic passing through to Direct Connect on premises. And similarly, if we have some of those east west flows that I talked about earlier where we need to maintain a firewall, those are unchanged as well. We're now complementing our existing security controls with zero trust where it can be applied. So moral of the story, things you should take away from this exercise. First off, zero trust is not a zero-sum game, OK? You can use zero trust solutions alongside your existing controls. It's not an all or nothing approach, and what that means is you can adopt and migrate at your own pace. You can adopt slowly for new services. You could start by migrating existing one. You could migrate and bring on new ones at the same time. There's a mixed bag here. You can do a lot of different things. You have flexibility with this approach. Remember that you're gonna have these two tiers of policy, so think about where you want to delineate those permissions. Who's going to control your central policies, who's going to control your application policies? Do they need to be closely aligned? Do you even want to have application policies or do you want all control at the service network level? You can do that. Finally, maintain your firewalls and VPNs for places where zero trust solutions don't fit again. Fit in there today. You can do that. There's nothing wrong with that. It makes sense to use these alongside each other where you need to. So as you're thinking about a new service, a new resource, you know, bringing something in for remote access, the question you should probably ask yourself is where can zero trust fit in? Is there a spot where zero trust will play a role, and should we be looking at that for this? So with that, I want to thank you all for your patience today with the slide mishaps and thank you so much for joining the session. Please do complete the session survey in the mobile app and I'll be around for questions right afterwards if anyone wants to chat. Thanks folks.
