# AWS re:Inforce 2025 - A practical guide to generative AI agent resilience (SEC323)

**Video Link:** [Watch on YouTube](https://www.youtube.com/watch?v=W7T5NtDuvnA)

## Video Information
- **Author:** AWS Events
- **Duration:** 21.3 minutes
- **Word Count:** 3,397 words
- **Publish Date:** 20250618

## Summary
This AWS re:Inforce 2025 presentation provides a comprehensive guide to building resilient generative AI agents. The speakers, Yuan Zhang (Gen AI specialist) and Jennifer Moran (senior resilience specialist), break down AI agents into their core components and introduce a systematic approach to ensuring they remain operational under various failure conditions. They define a generative AI agent as having four key characteristics: a reasoning engine for goal-oriented tasks, memory capabilities (both short and long-term), planning abilities to break down complex goals, and tool integration for external interactions.

The presentation introduces a 7-layer architecture framework for analyzing AI agent resilience: foundation models, agent orchestration, software layer, knowledge base, agent ecosystem, deployment infrastructure, and security/compliance with observability. Using AWS's resilience analysis framework, they demonstrate how to apply five key resilience properties (fault isolation, redundancy, sufficient capacity, timely output, and correct output) across these layers to identify and mitigate potential failure modes that could compromise agent performance.

## Key Points
• **AI Agent Definition**: Four core characteristics - reasoning engine, memory (short/long-term), planning capability, and tool integration for external APIs and databases
• **7-Layer Architecture Framework**: Foundation models, agent orchestration, software layer, knowledge base, agent ecosystem, deployment infrastructure, security/compliance/observability
• **Five Resilience Properties**: Fault isolation & shared fate, redundancy, sufficient capacity & excessive load management, timely output, and correct output validation
• **Fault Isolation Techniques**: Implement validation processes to prevent hallucinated outputs from cascading, use circuit breaker patterns for overwhelmed tools, and establish distributed tracing across all layers
• **Redundancy Strategies**: Leverage Amazon Bedrock's multi-region model deployment, implement automated failover processes with alternate tool routing, and monitor replication lag for knowledge bases
• **Capacity Management**: Implement automated token quota management, use intelligent request throttling with priority-based allocation, reduce search depth during peak loads, and maintain real-time capacity monitoring
• **Latency Optimization**: Define clear service level objectives, optimize agent orchestration to reduce rumination steps, implement streaming responses, and use asynchronous processing for independent tools
• **Output Validation**: Validate input/output against agent scope, leverage deterministic agent flows for predictable tasks, implement performance evaluation at every step, and introduce human-in-the-loop for high-stakes decisions
• **Operational Excellence**: Apply traditional DevOps practices with AI-specific modifications, including model management capabilities using MLOps and AI-specific observability metrics

## Technical Details
**AWS Services & Technologies:**
- Amazon Bedrock for managed AI agent deployment with multi-region model deployment and cross-region inference capabilities
- Bedrock Agents for managed service deployment where Amazon handles infrastructure while developers control agent logic
- Vector databases for knowledge base storage with embedding models for ingestion and query handling
- Distributed tracing across the 7-layer architecture

**Agent Development Frameworks:**
- ReAct (Reasoning and Acting) pattern for breaking down goal achievement
- Chain of thought reasoning for planning capabilities
- Async processing capabilities for parallel tool execution and sub-agent triggering
- Human-in-the-loop confirmation steps for high-stakes decisions

**Resilience Implementation Patterns:**
- Circuit breaker pattern for tool execution management
- Automated failover processes with alternate tool routing
- Token quota management and intelligent request throttling
- Priority-based resource allocation during peak loads
- Search depth reduction for knowledge base optimization
- Real-time capacity monitoring for proactive scaling

**MLOps & Observability:**
- Model management capabilities integration
- AI-specific metrics for traditional DevOps practices
- Performance evaluation at every agent step (reasoning engine, planning, tool selection, parameter fitting, information synthesis)
- Latency observability across reasoning steps and tool execution
- Replication lag monitoring with defined SLAs for knowledge bases

**Security & Compliance Controls:**
- Granular access control for different users to specific tools and database sections
- Agent safety controls to limit operational scope to intended resources and tasks
- Input/output validation against defined agent scope
- Deterministic agent flows for predictable task execution patterns

## Full Transcript

Hi, everyone. Do, do any of you, have any of you build agents? Yes. Oh good. Do you just wish your agent could keep running and never fail, no matter how many people are hitting it, no matter what random request your users are throwing at it? Yes, great. That's what we're gonna talk about today. We're gonna talk about how to make our Gen AI agent resilient. My name is Yuan Zhang. I'm a Gen AI specialist, principal architect here. And I'm Jennifer Moran. I am a senior resilience specialist SA here. OK. Here's our our agenda. We're gonna spend a little time to clarify what is a GAI agent. And then we're gonna break it apart, see what are the components and layers inside of the GAI agent that could cause resilience problems. And then we're gonna talk about different resilience controls and and with operational excellence. So what is a GAI agent? The official definition says it's an artificial it's an artificial intelligence agent, um, that is a software program that can interact with the environment, collect data, and use the data to perform self-determined tasks to meet the goals. It's very dry and then tell say a lot of things. Let's break it down. So there are 4 things. One is you need to have a reasoning engine. And the agents are goal-oriented. So it will achieve the goal, although the path is not predetermined. So in the, you use the reasoning engines such as react train of thought to break out how do we achieve that goal. Second, it has memories. It cannot just be a one-off question answering. You need to have a short term memory to know what's the conversation context. You can also configure a long-term memory to have the basic user information there to give the user to give the agent more context to complete the task. And you need to have the planning capability. So the goal could be a really big ask. Then the agents need to be able to break it down into smaller by chewable sizes, uh different steps, and need to order these steps um in the right order to achieve the goal. And the last, you need to use tools. With the tools, the agents are just a chatting companion. With the tool, it can get things done, right? And it can use these external APIs or databases for help. So that's the characteristics of agents. Now, pop quiz is Lama 3.3 an agent. Good. I see a lot of shake heads. That's no, it's just the language model, right? Uh, what about cloud? Is cloud the app the agent? No. 00, some of them, some of you say no, some of us say yes. I could argue that with the right configuration, it could be an agent. It has a reasoning capability, you can do the planning, it has the memory for sure and right now it has different tools you can configure it. So if it's configured right, then it could be an agent. OK. So there, and the agent to the user is a cute chatbot and we can throw in some tasks, but for the developers, we know there are a lot of things behind the scenes. Let's break them apart. Uh, we put the agents into these 7 layers. I know it's a lot, we try to consolidate it, but these 7 layers are platform agnostic, agent development framework agnostic. So whatever way you build agent, you can always map to these 7 layers and then analyze whether they're resilient or not. OK, first layer is the foundation models. That's where everything started. We have large language models. They provide the reasoning capability to provide the the planning capabilities. And then on top of that, we start to build the agent orchestration. These are the logic we built into the agent. They determine what are the sub agent to call, what are the tools we can use. OK. And then now we have agent the software layer, um. We have a, um, there's also a knowledge base. So we pull knowledge base out as a separate layer because there are two components in the knowledge base. One is the storage of this vector database and the other part is the embedding model that help us ingest and build knowledge database and handle the queries. So we need to consider both the storage resilience and the embedding model resilience. Next is the agent ecosystems. So this includes all the tools the agent can call these external APIs. It also includes other features such as prompt caching, prompt management. The next one is the deployment infrastructure with all these different components we can deploy. So where do we deploy these agents? We can use the old fashioned way. I got a bunch of Linux server. I can got some issue to instance, I just managed my own and I deploy my, my agents there. You can also choose to use managed service such as Bedrock agents. In this case, Amazon Bedrock is in charge, is responsible to keep your agents up and running while the development team need to control the agent's logic and how it's built. The 6th layer is security and compliance. The security side include the security of the agent, so who have access to the agent. Quite a lot of cases we need to consider a find a more granular access control. So certain users can only have access to certain tools, certain user can only have access to certain part of the database. It also includes the agent's safety and security. So when the agents operating, especially when you give a certain tools, it will have a lot of power. So how do we control the agent only operate on the resource on the task that we want them to operate on? And also the compliance side, we need to control the users only asking for questions and within the agents um response scope. Oh, the last layer is most important because of the complexity goes into the agent, we need to have really good observability to keep track of what is happening at every step. And to build high quality agents, we need evaluation at every step. So the answer is final one answer, but we need to evaluate which step that is making them most easy to to break and then we can improve on that. So those are the 7 layers, um. And then let's talk about the resilience controls over them. OK, so now we're gonna talk about the resilience controls so I have a question for the audience has anyone heard of the resilience analysis framework? Oh No. All right, this is fantastic because that's what we're gonna talk about over the next couple of slides. So let me tell you a little bit about it. So the idea is that the resilience analysis framework came about by the resilience, uh, teams at AWS working with our customers, and the idea is that all resilient workloads, no matter what they are, will have 5 key resilience properties. Those properties will have corresponding failure modes that will try to contradict that property. And the idea is by having these properties and these failure modes you can do a resilience modeling exercise where you can try to identify the failure modes that are trying to challenge that property and be able to put resilience controls in place to either prevent, detect, or mitigate against those failure modes. So that's what we're gonna talk about today. So I want to draw your attention to the screen where as you can see we have the 7 layers that you and just explained, and we're gonna walk through the different key resilience properties, the failure modes as they relate to the ones that are green. Now that's not to say that you wouldn't want to look at these resilience properties through all of these layers, but we only have 20 minutes, so we're gonna try to highlight some of them using the different, uh, the different seven layers. So the first one is fault isolation and shared fate. So fault isolation is being able to build boundaries in order to prevent a cascading failure. And the idea of shared fate is that sometimes within those boundaries there are or crossing those boundaries are common dependencies and the reason that it's a shared fate failure mode is a lot of times you don't know that there's a dependency between those layers or across those boundaries. So if we think about this from a uh Gen AI workload we can think about from the reasoning layer putting in either physical or logical boundaries, right? So think about your foundation model that is uh providing a hallucinated output. And you don't want to have that cascade to your next layer right? you wanna be able to contain that output through validation processes and not have it call the next layer or the tool execution because the output is not good. We can also think of a circuit breaker circuit breaker pattern which is also something that we can implement in our orchestration layer where we know or we can put in this pattern to understand if one of our tool executions is being over. Overwhelm right it's not responsive. There's too many calls going to it it's not able to handle that request so what we can do is instead of calling that when we know it's gonna fail, we can put a circuit breaker in there and trip the circuit, meaning don't call that tool execution until it's able to uh respond and uh come back from being overwhelmed. We also want to think about distributed tracing so you see all these layers right? and we wanna make sure that we have that distributed tracing across each of these layers because what's gonna happen is we wanna make sure that we understand what's happening um both at the layer and between the layers and we wanna see if there are unintended boundaries getting crossed right? so again things that you may not think are a dependency between the layers but actually are. So our next property is redundancy, and this can be redundant components, this can be redundant paths and the idea is that this is going to eliminate single points of failure if you have a single point of failure and that component or path fails, you could potentially have a systemwide outage so we want to put redundancy in place to mitigate that. And the way that we can do that from the foundation model perspective is if you are using Amazon Bedrock, Amazon Bedrock does have a multi-region model deployment, so it does offer in some models a cross region inference where the redundancy is handled by the service for you so it's gonna take care of if one path is not available for whatever reason it's gonna choose that alternate path for you. We can also talk about um automated uh failover process and alternate tool routing so we talked about circuit breaker last time uh in the previous slide this kind of goes hand in hand with that, right? We wanna know that if the circuit breaker is tripping, we have an automated failover process that can use an alternate tool that's still gonna provide the same characteristics of that original tool but again you have that alternate routing that redundant path that you can use. We also want to make sure that we have replication lag monitoring for our knowledge bases, so having a replication lag between your primary and your secondary uh knowledge base is gonna be important, and you wanna make sure that you have SLAs identified for that replication and you wanna be able to understand if you are breaching those SLAs because you want that knowledge base whether it's the primary or the secondary to be accessible to the tools that needs it. And finally monitoring all paths right so if you can monitor both the different components, uh, redundant components and the different paths, you're going to be able to identify when there could be a potential issue happening right? so you can get a leading indicator to say oh there might be an issue with this let me go and see, uh, make sure that I can take care of it before your users have a bad experience. Third is gonna be sufficient capacity and excessive load, so sufficient capacity is when our workload is able to handle regular traffic and unexpected surges, but excessive load can actually compromise even the most well provisioned systems. So what we wanna do with excessive load is we wanna make sure that we are managing our quota management right? and we wanna do this in an automated fashion. So we wanna make sure that we understand our token quotas because what we don't want to happen is we don't want a single complex conversation exhausting all the resources for all of the other uh users so we need to make sure that we are on top of that token quota. Also intelligent request throttling so what we can do is we wanna use uh resource uh priority resource based allocation and we wanna make sure that the most important calls or requests are the ones that get through and what we can do during peak load is the ones that are less critical can sort of take a backseat to the ones that really matter. Reducing search depth, so in our knowledge base, having a high peak at times you can reduce that search depth right? so making sure that you're still having those uh critical conversations can uh come back from that knowledge base but again just reducing that search step so you um can manage that load. And real time capacity monitoring, right, making sure that again we're actively monitoring those tokens, we're actively monitoring, uh, model inference times in order to be able to proactively scale, uh, when we need to again because what we don't want to happen is our users to have a bad experience. And then I'm going to pass it back to you and and she's going to do the next two. OK. OK, timely output. Who has a user that expects your GI agent can produce an answer like that immediately. Right, they all do. Well, this is a typical trade-off between cost, quality, and latency. I've never met any user will say, I will give up on quality. Uh, just give me answer fast, right? cause quality is always the first priority right now we're still at the stage to win win the trust from humans. So given that they expect high quality, then the agent need time to think. So given these constraints, how can we make the agent have lower latency? Well, the first thing to do is define the service level objectives, just old fashioned expectation management. Have the conversation with the user upfront. Is this, does, do you have to have the answer immediately or can you wait for like 5 minutes? Quite a lot of cases, some old um tasks will take hours or many days for human to complete. Now it's gonna take 10 minutes for an agent. If the conversation happens upfront, it's probably OK. Otherwise, they're gonna staring at a spinning wheel for 10 minutes. It's a really bad experience. And the next one is optimized agent orchestration. So when we build the agent because of the non-determinous nature of this GI applications, it's gonna go through a lot of different iterations. It's gonna do its own planning, it's gonna think things through steps and then ruminate on the steps and think about whether I made the right decision. So if we can optimize that, analyze what uh the agent has been through, is it because of some of the instruction I gave to the agent, make it take certain actions that could largely reduce the time is spent on ruminating these steps. Next is the latency observability, of course, have very have transparency over what's taking time at every step. It could be the reasoning steps. It could, it might be the tools just taking a long time to get in the do what they need to do. Also, if all of these are tried, then that's, it still needs to take time and the user expect results immediately, we can start to stream out the, the answers instead of having to finish the finish generating all the texts and then send a big blob. And there's also some of the agent development framework provide async processing capabilities. So you can trigger the tools asynchronously when they're not interdependent. You can also trigger multiple sub-agents asynchronously, so it's save some time. And the next property is correct output. And this we're not talking about the overall accuracy of um the large language model accuracy. This we're talking about when we built an agent and we define this as a scope, we need to make sure that the answer within this scope. So the first thing to do is to invalidate the input and output. So is the user's ask within my agent's ability to answer it. If not, let's stop it right there. Also, we can leverage deterministic agent flows. So when we have the agent, it can do a lot of different things. We can rely on all the non-determinant nature. But if we know certain tasks need to be taken or certain sub-agents could be triggered when user set certain things, we could build the rules into the agents. In that way, it takes out the nondeterminis nature of the agent and largely enhance the the accuracy. And performance evaluation is so important. Because of the different steps that agent need to take, we need to have a transparency on what is doing every step and evaluate it at every level when we want to improve accuracy. Is it the raising engine doing the right job upfront? Did they create the right plan to execute my task. If it's the right plan, is it picking out the right tools to or the right sub agent to do the task. If it's the right tool, did they pick out the right parameters for these tools to fit in to execute on the queries. If the tool is doing the right job, did they return it back to the agent and did the agent use that information to generate your final answer? So have the transparency over these steps is super helpful. And last one is when. When the certain task is uh is a high stake, we can introduce human the loop. So if you're using BA agent, there's the confirmation step you can introduce it's within the agent. Human can confirm before it can carry on certain tasks. If you're using some open source framework, you can pull out to a human node and then there you can define what action to take when human approve or disapprove certain actions. OK OK, so finishing up, uh, operational excellence as it relates to uh generative AI. So really the idea here is take your traditional DevOps practices and think about how you would incorporate AI specific uh things through their lens. So for example thinking about strong engineering. Teams that have modeled management capabilities using ML ops again we talked about observability uh adding in those AI specific metrics so all of this is going to be important and again it's your traditional dev ops practices but just sort of tweaking to add those AI specific elements. So we covered a lot and our key takeaways for you are we challenge you to go back and take your workloads, use the 7 layers that we identified, map your workload to those layers, do the resilience modeling exercise with the 5 key resilience properties and failure modes, and think about your operational excellence and how you can put an AI lens on it. So that's it. Uh, if you have any questions, we're actually gonna be on the side of the stage for you, so thank you very much.
